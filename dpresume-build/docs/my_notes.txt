required skills - fullstack lead developer, architect
expert in modern software architecture concepts,]
expert in python language backend development
expert in js, react+redux+ag-grid, css, html5 - front end development
databases sql, plsql, nosql
knowlege of devOps, toolsets, agile/ceremonies/kanban/sprints/multifunctional small teams, pair programming, TDD approaches, cloud etc
packaging build and deployment

linux environment, docker, jenkin, cross platform builds, CI/CD, chef, ansible, puppet, building & consuming RESTFul APIs, microservice architecture
design patterns and software architecture patterns
written & verbal communications
analytical problem solving skills
modern QA practices
self starter, work closely, collaboratively
change, release, incident processes (ARM, Snow)

agile team capacity planner, available load, story points to deliver
one resource can max deliver 9-12 story points per sprint
agile ceremonies: planning, estimation, scrum, review, retrospective
artifacts: product increment, backlog, scrum backlog
KANBAN: todo|in progress|blocked|ready to be accepted|descoped|Done
DoD: definiation of Done
DoR: Definition of Readiness
glossary: acceptance criteria, architect, burn down/up charts, backlog grooming, daily scrum, Emergence(team will evolve), epic, estimation, fibbonacci (story points: 1,2,3,5,8,13), impediment, pig, planning, pocker, process, backlog, product owner, relative estimation, release, retro, scrum master(bridge between team and po, teach PO, improve dev team, engineering processes, tracks progress), role, self organisation, spike, sprint, sprint goal, task, story time, task list, task board, team, member, thumb vote, time boxing, velocity, what xp practices, empricism(impact and adapt)
what have you done yesterday, what you will do today, impediments
iterative and incremental development

DevOps Tool Set
code quality tools knowledge: Crucibel, SonarQube
Gradle:
    build tool like maven and ant but far more better as gradle scripts can be written in modern programing languages also groovie based DSL scripts
    dependency management, incremental builds, faster and reliable
git:
    code management tool
Jenkin (open source) like ARM in RBS, bamboo paid by atlassian:
    CI/CD: Continuous Integration / Continuous Deployment, sever that allows you to automate different stages of delivery pipeline
    huge pluggin ecosystem
    integrates with docker and puppet also
    prepare build, test build, deploy to pre prod and finally to prod
Docker:
    build, manage, secure apps, anywhere, os and platform independent
    container platform, you can use docker images rather than virtual images
    makes distributed development possible
    easy cloud migration
Kubernetes:
    solves scalability issue with docker container if it grows
Puppet:
    software discovery, configuration management
Chef:
    infra, build, and deploy
Raygun:
    error monitoring tools, performance monitoring, linked to code and traces back to line of error in code

two server side apis/systems can interact with each other with use of agent/broker, pub-sub modes or queue like Argon in RBS
send messages/files and receive messages
same thing happens in distributed and clustering environment
this sort of updates to slave nodes happenes in db replication services which pushes partial updates to nodes to make them in sync

REST APIs: Representation State Transfer
JSON (Javascript Object Notation) by Roy Fielding, since data is not tied to method, resource, REST can take advantages for having diff multiple calls, return different data sets, formats and change strcuture. The freedom and flexibility
its difficult to maintain states in REST and import to note that what makes REST a RESTFul services
5 of below to follow for a RESTful api
1. client server model: separate implementation, separation of concerns and duties
2. stateless: 2 APIs/URIs in application are independent of each other, the data or params for a service/api to work has been fed completely and independently.  Dont rely on server states to build objects. for better performance, store the states on the client rather than on server
3. Cache: faster, better performance apps, cache on client side, donig so reduces number of api calls and memory uses on servers
4. Uniform Interface: decouple client and server for independent evolution, standard means of communication, CRUD(Create Read Update Delete) implementation & JSON for scale
5. Layered System: apply MVC design pattern (model: data, view: output, controller: actions and events), this approach is used for security reasons also as different solves different security and accessibility concerns, encapsulation of data and add/deletion of components at runtime without making app down or crash
GET: read, PUT: update, POST: insert, Patch: partial update, Delete: delete

communication modes: synchronous, asynchronous

PORTS:
0-1023: common protocols and services
1024-49151: registered ICANN: servers
49152-65353: dynamic private

20/21: FTP - TCP (Transmission Control Protocol: control the packets)
22: SSH - TCP/UDP (User Datagram protocol: doesnt track like DNS, media stream, online multiplayer games, TFTP: trivial file transfer)
23: Telnet - TCP
25: smtp -TCP
50/51: IPSec
53: DNS - TCP/UDP
67/68: DHCP - TCP/UDP
69: TFTP - UDP
80/8080: http - TCP
110: POP3 - TCP
119: NNTP - TCP
123: NTP - TCP
135-139: Net BIOS - TCP/UDP
143: IMAP4
161/162: SNMP
443: https
389: LDAP (Active Directory)
3389: RDP (Remote Desktop Protocol)

Modern QA practices:
Breakfree from classical roles & responsibilty
choose your release criteria carefully
prioritise bug fixes based on users
admit 2 tier approach to test automation
    faster test cases on every commit
    complete regression overnight run in every sprint
stay close to relevent environment
form a dedicated security testing team
form a dedicated performance testing team
run a regression life cycle
simulate customer accounts as of production
perform sanity tests on production

Software Architecture patterns
general reusable solutions, solves scalability concerns, helps better design discussion and thoughts around
1. layered pattern:
2. client server pattern:
3. master slave / pub-sub / observer patterns:
4. pipe & filter pattern:
5. Broker pattern:
6. peer to peer pattern:
7. Event bus pattern:
8. MVC pattern:
9. Black Board pattern:
10. interpreter pattern:


ES6/ES2015 and other javascript concepts:
let & const: block scoped,**,default and expanding params, call(arg known similar to positional args in python)/apply(arg not known, similar to kwargs in python), Array.find(), Array.findIndex(), Array.includes()
Arrow Function: ()=>{}, short expression for writting function, great saviour for this & _that overhead, solves the this related context problems and undefine error is prevented,

Pure functions are those whose return value depends solely on the values of their arguments.

hoisting: vars anywhere in program is hoisted on top of code at runtime and this error of var defining doesnt occur, ie. you can define a var later in code and use it before elsewhere
"use strict" is used to prevent hoisting and js gives some code errors in design time which otherwise it wont give

Array.push(),pop(), shift(),unshift()

clousure: variable ref to a function which is return by another function, ie the state of function is saved and can be used later in program or passed along

array and objects behave same way in js array is also like object with default keys as integer indexes but diff is that you can define keys in object yourself. see same loop for objects and arrays

js always passes primtive objects by values like string var but when object/array is passed, its by references. however, changing the property of an object is which is primitives changes the underlying object since master object is by reference, hence its childs are also by refrences

Primitives (copied by value): null,undefined,Number,String,Boolean
Objects (copied by reference): Object,Array,Function

primitive js types are immutable

basic class:
https://javascript.info/class
https://www.digitalocean.com/community/tutorials/understanding-classes-in-javascript
class MyClass {
  prop = value; // field
  constructor(...) { // constructor
    // ...
  }
  method(...) {} // method
  get something(...) {} // getter method
  set something(...) {} // setter method
  [Symbol.iterator]() {} // method with computed name/symbol name
  // ...
}

class User {
  constructor(name) {
    this.name = name;
  }
  get name() {
      return this._name;
    }

    set name(value) {
      if (value.length < 4) {
        alert("Name is too short.");
        return;
      }
      this._name = value;
    }
  sayHi() {
    alert(this.name);
  }
}
let user = new User("John");
console.log(user.name,"is perfactly valid")
user.sayHi();
alert(typeof User);
alert(User === User.prototype.constructor);
alert(User.prototype.sayHi);
alert(Object.getOwnPropertyNames(User.prototype));

// rewriting class User in pure functions
// 1. Create constructor function
function User(name) {
  this.name = name;
}
// any function prototype has constructor property by default, so we don't need to create it
// 2. Add the method to prototype
User.prototype.sayHi = function() {
  alert(this.name);
};
let user = new User("John");
user.sayHi();

class expression may/may not have a name
let User = class {
  sayHi() {
    alert("Hello");
  }
};
new User().sayHi();

function makeClass(phrase) {
  // declare a class and return it
  return class {
    sayHi() {
      alert(phrase);
    };
  };
}
let User = makeClass("Hello");
new User().sayHi(); // Hello

computed properties
function f() { return "sayHi"; }

class User {
  [f()]() {
    alert("Hello");
  }

}
new User().sayHi();


Tornado:
Async web framework, non block i/o, scalable and distibuted, fault tolerant, high load, non blocking IO Loops, callbacks, tasks and timeouts, 
simple TCP server, Application Class need to be overridden, request handler, generators-async code and callbacks
can handle 1000s of paralle connections
define a requets handler, implement set, define an aplication, tell application to start, start ioloop
Application: collection of request handlers, implement listen, starts http server, sets application request callback, handle user request,
**kwargs={a:1,b:2} keyword args-length of params is not defined and is not known
*args=positional args -length of params is defined and is known
APP: parse the url, decide which handler to use, creates an instance of it and executes the code
each connection gets one instance
RequestHandler: calls the handler method, checks XSRF(Cross Site Request Forgery) cookie, closes the connection
IOLoop: core of the tornado, similar to Node, its also asynchronous and non blocking, useable standalong with WSGI, used for server and client, single instance per process
it execute forever, callbacks asap, timeout, handle events (something that changes the state of socket), 
Thread Local: stack context, keeps track of socket connections, handles association between sockets and classes
AddTimeout: pushesh the timeout to the heap of timeouts & wraps it in a stack context
websockets: similar to asynchronous, the connection is left open, after writting, waits for more, continuously sees IOStreaming, to application, its looks like request handler, acdepts the connections and decides the versoin of protocol and initiate websocket.protocol class
twisted mysql api for db connection tornado is considered the best
it also has templating mechanism
loader=tornado.template.loader('\..\..\folder path')
self.write(loader.load('index.html').generate('title'))
also self.write(json.dumps(base,jsonutil.default))
Tornado designed to be stateless and don't have session support out of the box.
Use secure cookies to store sensitive information like user_id. Use standard cookies to store not critical information.
For storing large objects - use standard scheme - MySQL + memcache.
Note that EVERY user will have a session, even those not logged in. When a user logs in, you'll want to attach their status as logged in (and their username/user_id, etc) to their session
The key issue with sessions is not where to store them, is to how to expire them intelligently. Regardless of where sessions are stored, as long as the number of stored sessions is reasonable (i.e. only active sessions plus some surplus are stored), all this data is going to fit in RAM and be served fast
pip install torndsession: Torndsession is a session extension for Tornado web framework. Torndsession support application memory, file, redis or memcached to save session data for request, and it’s easy to extend for developer

tornado session management - its stateless and hence internal management of sessions and you can use secure_cookies:
Cookies and secure cookies, Cookies are not secure and can easily be modified by clients.
set_secure_cookie and get_secure_cookie
You can set cookies in the user’s browser with the set_cookie method:
class MainHandler(tornado.web.RequestHandler):
    def get(self):
        if not self.get_cookie("mycookie"):
            self.set_cookie("mycookie", "myvalue")
            self.write("Your cookie was not set yet!")
        else:
            self.write("Your cookie was set!")
application = tornado.web.Application([
    (r"/", MainHandler),
], cookie_secret="__TODO:_GENERATE_YOUR_OWN_RANDOM_VALUE_HERE__")


class MainHandler(tornado.web.RequestHandler):
    def get(self):
        if not self.get_secure_cookie("mycookie"):
            self.set_secure_cookie("mycookie", "myvalue")
            self.write("Your cookie was not set yet!")
        else:
            self.write("Your cookie was set!")

class BaseHandler(tornado.web.RequestHandler):
    def get_current_user(self):
        return self.get_secure_cookie("user")

class MainHandler(BaseHandler):
    def get(self):
        if not self.current_user:
            self.redirect("/login")
            return
        name = tornado.escape.xhtml_escape(self.current_user)
        self.write("Hello, " + name)

class LoginHandler(BaseHandler):
    def get(self):
        self.write('<html><body><form action="/login" method="post">'
                   'Name: <input type="text" name="name">'
                   '<input type="submit" value="Sign in">'
                   '</form></body></html>')

    def post(self):
        self.set_secure_cookie("user", self.get_argument("name"))
        self.redirect("/")

application = tornado.web.Application([
    (r"/", MainHandler),
    (r"/login", LoginHandler),
], cookie_secret="__TODO:_GENERATE_YOUR_OWN_RANDOM_VALUE_HERE__")

class MainHandler(BaseHandler):
    @tornado.web.authenticated
    def get(self):
        name = tornado.escape.xhtml_escape(self.current_user)
        self.write("Hello, " + name)

settings = {
    "cookie_secret": "__TODO:_GENERATE_YOUR_OWN_RANDOM_VALUE_HERE__",
    "login_url": "/login",
    "xsrf_cookies": True,
}
application = tornado.web.Application([
    (r"/", MainHandler),
    (r"/login", LoginHandler),
], **settings)

UIModule xsrf_form_html()
<form action="/new_message" method="post">
  {% module xsrf_form_html() %}
  <input type="text" name="message"/>
  <input type="submit" value="Post"/>
</form>

function getCookie(name) {
    var r = document.cookie.match("\\b" + name + "=([^;]*)\\b");
    return r ? r[1] : undefined;
}

jQuery.postJSON = function(url, args, callback) {
    args._xsrf = getCookie("_xsrf");
    $.ajax({url: url, data: $.param(args), dataType: "text", type: "POST",
        success: function(response) {
        callback(eval("(" + response + ")"));
    }});
};

DNS rebinding is an attack that can bypass the same-origin policy and allow external sites to access resources on private networks. 
# GOOD: same as previous example using tornado.routing.
app = Application([
    (HostMatches(r'(localhost|127\.0\.0\.1)'),
        [('/foo', FooHandler)]),
    ])

Third party authentication
The tornado.auth module implements the authentication and authorization protocols for a number of the most popular sites on the web, including Google/Gmail, Facebook, Twitter, and FriendFeed. 
class GoogleOAuth2LoginHandler(tornado.web.RequestHandler,
                               tornado.auth.GoogleOAuth2Mixin):
    async def get(self):
        if self.get_argument('code', False):
            user = await self.get_authenticated_user(
                redirect_uri='http://your.site.com/auth/google',
                code=self.get_argument('code'))
            # Save the user with e.g. set_secure_cookie
        else:
            await self.authorize_redirect(
                redirect_uri='http://your.site.com/auth/google',
                client_id=self.settings['google_oauth']['key'],
                scope=['profile', 'email'],
                response_type='code',
                extra_params={'approval_prompt': 'auto'})






React:
order of life cycle events: 
  Mounting: constructor()->static getDerivedStateFromProps()->render()->componentDidMount()
  Updating: static getDerivedStateFromProps()->shouldComponentUpdate(nextProps, nextState)->render()->getSnapshotBeforeUpdate()->componentDidUpdate(prevProps, prevState, snapshot)
  Unmounting: componentWillUnmount()
  Error: during render, static getDerivedStateFromError()->componentDidCatch(). to avoid system error being displayed, write an ErrorBoundary class with fallback ui, implement static getDerivedStateFromError(error), componentDidCatch(error, info) methods
  setState(updater[, callback]), component.forceUpdate(callback)
  render() will not be invoked if shouldComponentUpdate() returns false.
  componentDidUpdate() will not be invoked if shouldComponentUpdate() returns false.
  Calling forceUpdate() will cause render() to be called on the component, skipping shouldComponentUpdate(). This will trigger the normal lifecycle methods for child components, including the shouldComponentUpdate() method of each child

what all ways we can rerender components: An update can be caused by changes to props or state.
hooks:
advantages and disadvantages of react over framework:
controlled and uncontrolled components:

named export (can be multiple per file) and default export (only one per file)
export const MyComponent = () => {} //named export
export const MyComponent2 = () => {}
import {MyComponent,MyComponent2} from 'mycomponents file'

export default const MyComponent = () => {}
import MyComponent from './compfile'

axios api for data fetch
aggrid for tabular data
es6-promise
html-react-parser
redux
lodash - js api for manipuaing objects and arrays
react, react-dom,react-scripts, create-react-app <app name>, npm start, npm run build, PostBuild:''
babel-core for reading and recognising JSX syntax (html inside of js body)

map higher order function: 
  newArr=arr.map((elm,id)=>return elm) or arr.map(elm=>elm*2)
  newArr=obj.map(ox=> ox.id+"->"+ox.data)
  return a new array of same length of original array on which it runs

reduce higher order function: 
  var mostExpPilot=pilots.reduce((accumulator,pilot)=>accumulator+pilot.years,0), 0 is the inital value of accumulator
  returns a signle value of the iteration and it passes result of previous run as callback

filter higher order function: 
  filter out some elements based on some condition
  var rebels=arr.filter(pilot=>pilot.fact==='Rebels')

Chained Up:
const totalJediScore=persons.filter(person=>person.isForcedUser).map(jedi=>jedi.pilotScore+pilot.shootingScore).reduce(acc,score=>acc+score,0)

promises: q, bluebird, deffered.js, vow, ES6 way (async over function and await over actual call)
older way of promise is the resolve and reject callbacks
var wait100=new Promise((resolve, reject)=>{
  setTimeout(()=>resolve,1000)
}).then(()=>{console.log('x')})

desctruct syntax:
arr=[1,2,3,4,5,6] 
[one,two,...three]=arr
one=1, two=2, three=[3,4,5,6]

obj={a:1,b:2,c:3,d:4}
{a,b,c}=obj, gets the fields from object and store values in it

Pass the context
var logUpperCase=function(){
  this.string=this.string.upperCase()
  return ()=>console.log(this.string)
}
//closure call
logUpperCase.call({string:'my name is dp'})

module & class
export class a extends b{
  construcotr(option, data){
    super(args)
    this.name=""
    this.age=10
  }
}
eg in file1: module.exports={
  fn=()=>{console.log('func call from file1')}
}
in file2:
var file1=require('./file1.js')
file1.fn()
this is named export: import {x,y} from 'module1'
this is annonymous export: import z from 'module1'
import * as x from 'module1'

simplest react component is to write js function return jsx
this is function based components, cannot use setState mehtods, these are stateless components
hence if you want to use states in functional component uplift your component to parent visibility and pass down states as props
react16.8 uses hooks, that means now you can use lifeCycle events in functional components
function Welcome(props) {
  return <h1>Hello, {props.name}</h1>;
}

class based component is ES6 syntax follow react lifecycle events like componentDidMount() and ComponentDidUpdate() etc as they all come down from React.Component class
can use state and setState mehtods
class Welcome extends React.Component {
  render() {
    return <h1>Hello, {this.props.name}</h1>;
  }
}


Redux:
central state management library/framework which works on concept of action dispatchers/reducers to maintain states and centrally make it available by provider & connect, values comes via props
connect the component to redux import {connect} from 'react-redux'
mapStateToProps=(state)=>{
  counter: state.counter
}
connect function also injects the dispatch events into components as props
so store.dispatch({type:'INCREMENT'}) in main app is equivalent to this.props.dispatch({type:'xxxx'}), you can check this in react dev tools
export default connect({mapStateToProps})(<componentClass>) //connect is higher order function, when we call it, it returns a function with component being exported
create the store,go to main app component and create the store, import {creatStore} fropm 'redux' and top most line after import should be
const store=creatStoreate(reducerFunction) //reducer function for initial state of store
provide store to the app
import {Provider} from 'react-redux'
const app=()=>{
  <Provider store={store}><component /></Provider>
}
createStore requires a reducer function for initial state to store
function reducerFunction(state, actions){, its a pure function //takes 2 args (previous state, action object from dispatcher), make sure reducer never returns null
  //check actions.type==='INCREMENT'
  //check actions.type==='DECREMENT'
  return {counter:1}
}
WrapUp Actions: state has function called dispatchers and changing states is via dispatching events
store.dispatch({type:'INCREMENT'}) //passes action to reducers thats being assigned in store
store.dispatch({type:'DECREMENT'})

three principles that Redux follow
Single source of truth:
State is read-only:
Changes are made with pure functions:

advantages of Redux: Predictability of outcome, Maintainability, Server-side rendering, Developer tools, Community and ecosystem, Ease of testing, Organization

components of Redux
Action – It’s an object that describes what happened.
Reducer –  It is a place to determine how the state will change.
Store – State/ Object tree of the entire application is saved in the Store.
View – Simply displays the data provided by the Store.

Explain Flux, similar to redux
Flux is an architectural pattern which enforces the uni-directional data flow. It controls derived data and enables communication between multiple 
components using a central Store which has authority for all data. Any update in data throughout the application must occur here only. Flux provides 
stability to the application and reduces run-time errors.


BigO:
time to paint a fense of height, weight, p layers is O(w*h*p). any time taken to execute any program or algo
space complexity is paralleled to time complexity, given size of n array has O(n) space requirement or number of bytes it needs for storage in memory
sorting
if memory constraints are very tight you can use Quick Sort and generally its fastest, whose worst-time compelxity is O(n^{2}) but average case complexity is O(nlogn).
Merge Sort is the fastest stable sorting algorithm with worst-case complexity of O(nlogn), but it requires extra space. 

for 2d array O(n^2), n*n matrix
addition is better than multiplication
additon: for(int a:arrA){ print(a) } for(int b:arrB){ print(b) }, it has O(A+B)
multiplication: for(int a:arrA){ for(int b:arrB){ print(a,b) }}, it has O(A*B)
if the algorithm is in the form of "do this then this", its addition type of algo, then just add the runtimes
if the algorithm is in the form of "do this for each of this", its multiplicaton type of algo, then just multiply the runtimes

Amortised time: eg ArrayList, when array is full, create double the list by copying N elements, this is done once in a while so wrt to cost over the program
we arfe ok to do it, time to this is amortised
as we insert element we double the capacity,eg 1,2,4,8,16,32,64... COPY
we make copies with every insertions, if left to right is like double and right to left is 1/2 of x, then this pattern is of Logrithmic complexity and runtime is O(logN)
eg binary search, with every search, search list is reduced to half.
Left 2 Right:(N=16, N=8, N=4, N=2, N=1), right 2 left(N=1, N=2, N=4, N=8, N=16)
N=2^k, what is k, this is what exactly log expresses, 16=2^4, k=4 or log2 16=4 => log2 N=k or 2^k=N
this is same reason why we have runtime complexity as O(logN) in balanced binary tree

Recursive Algorithms
int f(int n){
  if (n<=1){
    return 1
  }
  return f(n-1)+f(n-2)
}
we are double it each time we loop over it, hence its O(n*n), call is getting doubled, sum of power of 2
2^0+2^1+2^2+2^3+2^4+......+2^n
so for recursive calls, often but not always is O(branches ^ n), O((2^n+1) - 1) nodes will be there
the space complexity of this would be O(N)


Design Patterns
Creational design patterns
These design patterns are all about class instantiation. This pattern can be further divided into class-creation patterns and object-creational patterns. While class-creation patterns use inheritance effectively in the instantiation process, object-creation patterns use delegation effectively to get the job done.
Example of Abstract Factory
Abstract Factory: Creates an instance of several families of classes
Builder: Separates object construction from its representation
Factory Method: Creates an instance of several derived classes
Object Pool: Avoid expensive acquisition and release of resources by recycling objects that are no longer in use
Prototype: A fully initialized instance to be copied or cloned
Singleton: A class of which only a single instance can exist

Structural design patterns
These design patterns are all about Class and Object composition. Structural class-creation patterns use inheritance to compose interfaces. Structural object-patterns define ways to compose objects to obtain new functionality. 
Adapter: Match interfaces of different classes
Bridge: Separates an object’s interface from its implementation
Composite: A tree structure of simple and composite objects
Decorator: Add responsibilities to objects dynamically
Facade: A single class that represents an entire subsystem
Flyweight: A fine-grained instance used for efficient sharing
Private Class Data: Restricts accessor/mutator access
Proxy: An object representing another object

Behavioral design patterns
These design patterns are all about Class's objects communication. Behavioral patterns are those patterns that are most specifically concerned with communication between objects.
Chain of responsibility: A way of passing a request between a chain of objects
Command: Encapsulate a command request as an object
Interpreter: A way to include language elements in a program
Iterator: Sequentially access the elements of a collection
Mediator: Defines simplified communication between classes
Memento: Capture and restore an object's internal state
Null Object: Designed to act as a default value of an object
Observer: A way of notifying change to a number of classes
State: Alter an object's behavior when its state changes
Strategy: Encapsulates an algorithm inside a class
Template method: Defer the exact steps of an algorithm to a subclass
Visitor: Defines a new operation to a class without change



python:
practice some programs: https://dbader.org
difference between python2 and 3:
  Python 3.0 (most favoured python implementation) was released in 2008. Its newest version, 3.6, was released in 2016, and version 3.7 is currently in development.
  among plenty of differences but 5 of these being most prevelent
  legacy->future, library (not forward compatible)->todays libraries are strictly 3 onwards, Strings are ASCII->Strings are uncode, rounds calci->correct calci, print->print()

To make your project be single-source Python 2/3 compatible, the basic steps are:
  Only worry about supporting Python 2.7
  Make sure you have good test coverage (coverage.py can help; pip install coverage)
  Learn the differences between Python 2 & 3
  Use Futurize (or Modernize) to update your code (e.g. pip install future)
  Use Pylint to help make sure you don’t regress on your Python 3 support (pip install pylint)
  Use caniusepython3 to find out which of your dependencies are blocking your use of Python 3 (pip install caniusepython3)
  Once your dependencies are no longer blocking you, use continuous integration to make sure you stay compatible with Python 2 & 3 (tox can help test against multiple versions of Python; pip install tox)
  Consider using optional static type checking to make sure your type usage works in both Python 2 & 3 (e.g. use mypy to check your typing under both Python 2 & Python 3).

Unicode strings can store foreign language letters, Roman letters and numerals, symbols, emojis, etc., offering you more choices
Each newer version of Python continues to get faster runtime. Meanwhile, nobody’s currently working to make Python 2.7 work faster. Community support is better with Python 3.

range and xrange:
    range returns a Python list object (big list, big memory needs, space) and xrange returns an xrange object (generators, on demand, saves memory)
    both takes 3 args, start, stop, step

del: delete variables and cleans memory
packages: import sounds.effects.echo /  from sounds.effects import echo, logical separation of modules
file handling:
    f=open("path",r/w/a), while true: line=f.readline() f.close
    for line in open(path): print line
    with open(path) as f: for line in f: print line
try...except....finally
dictionary: a={}, a=dict()
sets: a=set()
lists: a=[] a=list()
generators: yield x, produces iterators, produces sequences of values instead of single value, iterator = ('Hello' for i in range(3))
generator expressions:
    Generator expressions are a high-performance, memory–efficient generalization of list comprehensions and generators
    e.g.  iterators in a single line of code, iterator = ('Hello' for i in range(3)), next(iterator)...
    genexpr = (expression for item in collection)
    def generator():
        for item in collection:
            yield expression
    even_squares = lambda n: (x * x for x in range(n) if x % 2 == 0)

store in env vars:
    export API_KEY="your-api-key"
    import os
    api_key = os.getenv("API_KEY")

comprehensions: [i for i in [1,2,3,4,5]], [i for i in range(10)], [(i,f) for i in nums for f in fruit], [(i,f) for i in nums for f in fruit if f[0] == "P" if i%2 == 1]
collections:
    https://docs.python.org/2/library/collections.html
  Counter: The Counter() function in collections module takes an iterable or a mapping as the argument and returns a Dictionary.  counter.elements() as list
  defaultdict:  The defaultdict works exactly like a python dictionary, except for it does not throw KeyError when you try to access a non-existent key. nums=defaultdict(int) #int is passed as the default_factory
  OrderedDict: OrderedDict is a dictionary where keys maintain the order in which they are inserted, which means if you change the value of a key later, it will not change the position of the key.
  deque: The deque is a list optimized for inserting and removing items.
  ChainMap: ChainMap is used to combine several dictionaries or mappings. It returns a list of dictionaries.
  namedtuple: The namedtuple() returns a tuple with names for each position in the tuple. One of the biggest problems with ordinary tuples is that you have to remember the index of each field of a tuple object. This is obviously difficult

pickling: Pickle module accepts any Python object and converts it into a string representation and dumps it into a file by using dump function, this process is called pickling
Python memory is managed by Python private heap space. it also has built in garbage collector
PyChecker/pylint is a static analysis tool that detects the bugs in Python source code and warns about the style and complexity of the bug
A Python decorator is a specific change that we make in Python syntax to alter functions easily.
Everything in Python is an object and all variables hold references to the objects.
pass is no-operation Python statement
slicing: A mechanism to select a range of items from sequence types like list, tuple, strings etc.
copying copy.copy () or copy.deepcopy()
share vars and object across: To share global variables across modules within a single program, create a special module. Import the config module in all modules of your application. The module will be available as a global variable across modules.
make a Python Script executable on Unix, you need to do two things, Script file's mode must be executable and the first line must begin with # ( #!/usr/local/bin/python)
delete files in python os.remove (filename) or os.unlink(filename)
namespace, In Python, every name introduced has a place where it lives and can be hooked for
lambda is a single expression anonymous function often used as inline function.
A lambda form in python does not have statements as it is used to make new function object and then return them at runtime.
ORM in python: 
  not suitable for larger application having volumnous data and which needs speed to run
  Facilitates implementing domain model pattern
  django uses ORM features where we define modules and export them dierctly to databases, the django framework maintains these ORM modals and data inside them
  sqlAlchemy is very famous ORB library for working with DBs
  Rails ORM using the Active Record architectural pattern
Explain what is Dogpile effect? How can you prevent this effect?
Dogpile effect is referred to the event when cache expires, and websites are hit by the multiple requests made by the client at the same time. This effect can be prevented by using semaphore lock. In this system when value expires, first process acquires the lock and starts generating new value.

django:
    https://data-flair.training/blogs/django-interview-questions/


flask session management: 
  Unlike a Cookie, Session data is stored on server. Session is the time interval when a client logs into a server and logs out of it.
  A session with each client is assigned a Session ID, , a Flask application needs a defined SECRET_KEY.

  from flask import Flask, session, redirect, url_for, escape, request
  app = Flask(__name__)
  app.secret_key = 'any random string’

  Session[‘username’] = ’admin’
  session.pop('username', None)
  @app.route('/')
  def index():
    if request.method == 'POST':
        session['username'] = request.form['username']
        return redirect(url_for('index'))
     return '''

   if 'username' in session:
      username = session['username']
         return 'Logged in as ' + username + '<br>' + \
         "<b><a href = '/logout'>click here to log out</a></b>"
   return "You are not logged in <br><a href = '/login'></b>" + \
      "click here to log in</b></a>"

@app.route('/logout')
def logout():
   # remove the username from the session if it is there
   session.pop('username', None)
   return redirect(url_for('index'))



Oracle:
What is PL SQL
Differentiate between % ROWTYPE and TYPE RECORD.
    % ROWTYPE is used when a query returns an entire row of a table or view.
    TYPE RECORD, on the other hand, is used when a query returns column of different tables or views.
    Eg. TYPE r_emp is RECORD (sno smp.smpno%type,sname smp sname %type)
    e_rec smp %ROWTYPE
    Cursor c1 is select smpno,dept from smp;
    e_rec c1 %ROWTYPE
Show code of a cursor for loop
    Eg. FOR smp_rec IN C1 LOOP
    totalsal=totalsal+smp_recsal;
    ENDLOOP;
Explain the uses of database trigger: autocode that runs on INSERT, UPdate and Delete
    used in
        1) Audit data modifications.
        2) Log events transparently.
        3) Enforce complex business rules.
        4) Maintain replica tables
        5) Derive column values
        6) Implement Complex security authorizations
What are the two types of exceptions: user_defined and predefined.
Show some predefined exceptions: DUP_VAL_ON_INDEX, ZERO_DIVIDE, NO_DATA_FOUND, TOO_MANY_ROWS, CURSOR_ALREADY_OPEN, INVALID_NUMBER, INVALID_CURSOR, PROGRAM_ERROR, TIMEOUT _ON_RESOURCE, STORAGE_ERROR, LOGON_DENIED, VALUE_ERROR
Explain Raise_application_error: It is a procedure of package DBMS_STANDARD that allows issuing of user_defined error messages from database trigger or stored sub-program.
Show how functions and procedures are called in a PL SQL block.
    Function: total:=calculate_sal('b644')
    Procedure: calculate_bonus('b644');
Explain two virtual tables available at the time of database trigger execution.
    THEN.column_name and NOW.column_name.
    For INSERT related triggers, NOW.column_name values are available only.
    For DELETE related triggers, THEN.column_name values are available only.
    For UPDATE related triggers, both Table columns are available.
What are the rules to be applied to NULLs whilst doing comparisons?
1) NULL is never TRUE or FALSE
2) NULL cannot be equal or unequal to other values
3) If a value in an expression is NULL, then the expression itself evaluates to NULL except for concatenation operator (||)
How is a process of PL SQL compiled?
    Compilation process includes syntax check, bind and p-code generation processes.
    Syntax checking checks the PL SQL codes for compilation errors. When all errors are corrected, a storage address is assigned to the variables
    that hold data. It is called Binding. P-code is a list of instructions for the PL SQL engine. P-code is stored in the database for named blocks and
    is used the next time it is executed.
Differentiate between Syntax and runtime errors.
    A syntax error can be easily detected by a PL/SQL compiler. For eg, incorrect spelling.
    A runtime error is handled with the help of exception-handling section in an PL/SQL block. For eg, SELECT INTO statement, which does not return any rows.
Explain Commit, Rollback and Savepoint.
    For a COMMIT statement, the following is true:
        Other users can see the data changes made by the transaction.
        The locks acquired by the transaction are released.
        The work done by the transaction becomes permanent.
    A ROLLBACK statement gets issued when the transaction ends, and the following is true.
        The work done in a transition is undone as if it was never issued.
        All locks acquired by transaction are released.
        It undoes all the work done by the user in a transaction.
   With SAVEPOINT, only part of transaction can be undone.
Define Implicit and Explicit Cursors.
    A cursor is implicit by default. The user cannot control or process the information in this cursor.
    If a query returns multiple rows of data, the program defines an explicit cursor. This allows the application to process each row sequentially as the cursor returns it.
Explain mutating table error.
    It occurs when a trigger tries to update a row that it is currently using.
    It is fixed by using views or temporary tables, so database selects one and updates the other. eg read from view and update table
When is a declare statement required?
    DECLARE statement is used by PL SQL anonymous blocks such as with stand alone, non-stored procedures. If it is used, it must come first in a stand alone file.
How many triggers can be applied to a table?: max 12
What is the importance of SQLCODE and SQLERRM?: SQLCODE-error number SQLERRM: message for the last error.
If a cursor is open, how can we find in a PL SQL Block?: the %ISOPEN cursor status variable can be used.
Show the two PL/SQL cursor exceptions.: Cursor_Already_Open, Invaid_cursor
What operators deal with NULL?: NVL converts NULL to another specified value, var:=NVL(var2,'Hi'); IS NULL and IS NOT NULL
Does SQL*Plus also have a PL/SQL Engine?: No, Thus, all PL/SQL code is sent directly to database engine
What packages are available to PL SQL developers?: DBMS_ series of packages, such as, DBMS_PIPE, DBMS_DDL, DBMS_LOCK, DBMS_ALERT, DBMS_OUTPUT, DBMS_JOB, DBMS_UTILITY, DBMS_SQL, DBMS_TRANSACTION, UTL_FILE.
Explain 3 basic parts of a trigger.: A triggering statement or event, A restriction, An action
What are character functions?: INITCAP, UPPER, SUBSTR, LOWER and LENGTH
Show the cursor attributes of PL/SQL.
    %ISOPEN : Checks if the cursor is open or not
    %ROWCOUNT : The number of rows that are updated, deleted or fetched.
    %FOUND : Checks if the cursor has fetched any row. It is true if rows are fetched
    %NOT FOUND : Checks if the cursor has fetched any row. It is True if rows are not fetched.
What is an Intersect?: Intersect is the product of two tables and it lists only matching rows
What are sequences?: Sequences are used to generate sequence numbers without an overhead of locking. Its drawback is that the sequence number is lost if the transaction is rolled back
What are the uses of SYSDATE and USER keywords?: SYSDATE: pseudo column server system date. USER: pseudo column current user logged onto the session. used for mainly logging
How does ROWID help in running a query faster?
    ROWID is the logical address of a row, it is not a physical column. It composes of data block number, file number and row number in the data block. Thus, I/O time gets minimized retrieving the row, and results in a faster query.
What are database links used for?
    Database links are created in order to form communication between various databases, or different environments like test, development and production. The database links are read-only to access other information as well.
What does fetching a cursor do?: Fetching a cursor reads Result Set row by row.
What does closing a cursor do?: Closing a cursor clears the private SQL area as well as de-allocates memory
Explain the uses of Control File.: It is a binary file. It records the structure of the database. It includes locations of several log files, names and timestamps. They can be stored in different locations to help in retrieval of information if one file gets corrupted.
Explain Consistency: Consistency shows that data will not be reflected to other users until the data is commit, so that consistency is maintained.
Differ between Anonymous blocks and sub-programs.: Anonymous blocks are unnamed blocks that are not stored anywhere whilst sub-programs are compiled and stored in database. They are compiled at runtime.
Differ between DECODE and CASE.
    DECODE and CASE statements are very similar, but CASE is extended version of DECODE. DECODE does not allow Decision making statements in its place.
    select decode(totalsal=12000,'high',10000,'medium') as decode_tesr from smp where smpno in (10,12,14,16); This statement returns an error.
    CASE is directly used in PL SQL, but DECODE is used in PL SQL through SQL only.
Explain autonomous transaction.
    An autonomous transaction is an independent transaction of the main or parent transaction. It is not nested if it is started by another transaction.
    There are several situations to use autonomous transactions like event logging and auditing.
What are the uses of MERGE?
    MERGE is used to combine multiple DML statements into one.
    Syntax : merge into tablename
            using(query)
            on(join condition)
            when not matched then
            [insert/update/delete] command
            when matched then
            [insert/update/delete] command
Can 2 queries be executed simultaneously in a Distributed Database System?
    Yes, they can be executed simultaneously. One query is always independent of the second query in a distributed database system based on the 2 phase commit.
What is out parameter used for eventhough return statement can also be used in pl/sql?
    Out parameters (similar to byref) allows more than one value in the calling program. Out parameter is not recommended in functions. Procedures can be used instead of functions if multiple values are required. Thus, these procedures are used to execute Out parameters.
How would you convert date into Julian date format?: select to_char(to_date('29-Mar-2013','dd-mon-yyyy'),'J') as julian from dual;
Explain SPOOL
    Spool command can print the output of sql statements in a file.
    spool/tmp/sql_outtxt
    select smp_name, smp_id from smp where dept='accounts';
    spool off;
Mention what PL/SQL package consists of?
    A PL/SQL package consists of
        PL/SQL table and record TYPE statements
        Procedures and Functions
        Cursors
        Variables ( tables, scalars, records, etc.) and constants
        Exception names and pragmas for relating an error number with an exception
        Cursors
Mention what are the benefits of PL/SQL packages?
    Enforced Information Hiding: It offers the liberty to choose whether to keep data private or public
    Top-down design: You can design the interface to the code hidden in the package before you actually implemented the modules themselves
    Object persistence: Objects declared in a package specification behaves like a global data for all PL/SQL objects in the application. You can modify the package in one module and then reference those changes to another module
    Object oriented design: The package gives developers strong hold over how the modules and data structures inside the package can be used
    Guaranteeing transaction integrity: It provides a level of transaction integrity
    Performance improvement: The RDBMS automatically tracks the validity of all program objects stored in the database and enhance the performance of packages
Mention what are different methods to trace the PL/SQL code?
    DBMS_APPLICATION_INFO
    DBMS_TRACE
    DBMS_SESSION and DBMS_MONITOR
    trcsess and tkproof utilities
Explain how you can copy a file to file content and file to PL/SQL table in advance PL/SQL?: fcopy procedure, file2pstab
Mention what is the function that is used to transfer a PL/SQL table log to a database table?: PROCEDURE ps2db
What is the difference between a Heap table and a Clustered table? How can we identify if the table is a heap table
    A Heap table is a table in which, the data rows are not stored in any particular order,  no clustered index.
    A clustered table is a table that has a predefined clustered index on one column or multiple columns of the table that defines the storing order of the rows within the data pages and the order of the pages within the table, based on the clustered index key.
    sys.partitions, sys.indexes
Online Transaction Processing (OLTP) databases, workloads are used for transactional systems
Online Analytical Processing (OLAP) database workloads are used for data warehousing systems (dimensions & measures)
Difference between varchar and varchar2 data types?: 2000 bytes, occupies space for null / 4000 bytes doesnt occupy space for null
What is RAW datatype: stores binary data with max size 32767
What are nested tables?:   Nested table is a data type in Oracle which is used to support columns containing multi valued attributes. It also hold entire sub table
What is COALESCE function: check null and returns next not null, if all null, then return null, Coalesce(value1, value2,value3,…)
What is BLOB datatype?: A BLOB data type is a varying length binary string which is used to store two gigabytes memory. Length should be specified in Bytes for BLOB
What is the difference between TRANSLATE and REPLACE?: Translate is used for character by character substitution and Replace is used substitute a single character with a word
What is a sub query and what are the different types of subqueries?
    Sub Query is also called as Nested Query or Inner Query which is used to get data from multiple tables. A sub query is added in the where clause of the main query.
    Correlated sub query: A Correlated sub query cannot be as independent query but can reference column in a table listed in the from list of the outer query.
    Non-Correlated subquery: This can be evaluated as if it were an independent query. Results of the sub query are submitted to the main query or parent query.
What is cross join?: Cross join is defined as the Cartesian product of records from the tables present in the join. Cross join will produce result which combines each row from the first table with the each row from the second table.
privileges: GRANT user1 TO user2 WITH MANAGER OPTION;[/sql]
VArray: columns containing multivalued attributes, hold bounded array of values
details of a table: desc tablename
SET operators: Union, Union All, Intersect and Minus
delete duplicate rows in a table?: Duplicate rows in the table can be deleted by using ROWID
Can we store pictures in the database and if so, how it can be done?: Yes, we can store pictures in the database by Long Raw Data type
What is hash cluster?: Hash Cluster is a technique used to store the table for faster retrieval. Apply hash value on the table to retrieve the rows from the table
constraints: null, not null, check, default
parameter mode: IN, OUT, IN OUT
$ORACLE_BASE and $ORACLE_HOME: base dir and beneath base
last record added: Select * from (select * from employees order by rownum desc) where rownum<2;
How to display employee records who gets more salary than the average salary in the department
    Select * from employee where salary>(select avg(salary) from dept, employee where dept.deptno = employee.deptno;
Indexes
It is a database object used to sort the values of a column for retrieving data fastly from a table. By default the rows in a table are identified with their row id’s
When ever you create an index on a table, the system gives index id’s which are stored in the index. If a table is having index then the system retrieves the rows basing on the index id’s else it retrieves value using row id’s
Types of Index
There are two types of indexes
1. Simple Index
2. Composite Index
There are two ways to create an index. They are
Creating index on a single column
Creating index on multiple columns
Simple Index
Any index created on a single table is called a simple index
Syntax : create index <index name> on <table name>(column);
Creating Index On A Single Column
When you create an index on a single column,
Create Table temporary(sno number(3), sname varchar2(10));
NOTE :
Index name must be unique in the database and it is user choice. Since index is a data base object, system allocates some memory in the database. The index stores all index id’s allocated for a column.
Note : we can not create any index on views.

SQL> select * from temporary;
SNO NAME
--------- ----------
110 Nithya
111 Saloni
112 Mahesh
113 Priya
114 Prasad
115 Aruna

creating an index
create index idx_temp on temporary(sno);
Note : We can not see the Details of Index file
select * from idx_temp;
ERROR at line 1:
ORA-00942: table or view does not exist
Creating Unique indexes on a Single column
Syntax :
Create unique index <index name> on <table name> (<column>)

Note : Unique index can be created on any table, except it does not contain any duplicate values on the column for which you are creating a unique index

Example :
create unique index idx_temp on sample1(sno);
insert into sample1 values(&sno,’&sname’);
enter any value for sno: 1
enter any value for sname : Mahesh
enter any value for sno: 2
enter any value for sname : Prasad
enter any value for sno: 1
enter any value for sname : Nithya
Error :
Ora-00001: unique constraint (scott.idx_temp) violated
Note :
Once if u create any index on a particular column, then that column can not be re indexed. To re index on that column, first remove that existing index using drop index command and then create new index on that column
If you create an index on an existing index, it will show an error message
create unique index idx_temp1 on temporary(sno)
ERROR at line 1:
ORA-01408: such column list already indexed

Dropping an Index
Drop Index : This Command Is Used To Drop Any Index
Syntax : Drop Index <Index Name>
Example : drop index idx_temp;

Composite index
An index created on 2 or more columns in a table is called composite index.
Syntax :
Create index <index name> on
<table name> (<column1>, <column2>,-------)

first create a table
create table bank(accno number(3),accna varchar2(10),ddno number(6),
amt number(6));

next Create the index on that table
create index idx_bank on bank (accno,ddno);

next Insert some values into that table
insert into bank values (100,'Prasad',11111,2343)
insert into bank values (102,'Mahesh',22222,4334)
insert into bank values (102,'Nithya',22222,4334)
select * from bank;
ACCNO ACCNA DDNO AMT
--------- ---------- --------- -------------
100 Prasad 11111 2343
102 Mahesh 22222 4334
102 Nithya 22222 4334
100 Saloni 11111 3433

Composite unique index
We can create unique index on two or more columns also using composite unique index
Syntax :
Create unique index <index name> on <table name> (<column1>,<column2>,-------)
create unique index idx_bank1 on bank1(accno,ddno);
insert into bank values (100,'Priya',11111,3445)
insert into bank values (102,'Mahi',22222,4334)
insert into bank values (102,'Sound',22222,4334)
ERROR at line 1:
ORA-00001: unique constraint (SCOTT.IDX_BANK1) violated
Note :
1. There is a problem in the above index, the index will give an error only when both columns value are given duplicate values. If any one is different the index will accept those values.
2. Once a column is indexed already, it is not possible to reindex that column to any other index file
Drop index
This command is used to drop any index
Syntax : Drop index <index name>
Ex : Drop Index Idx_Sample;
There are two built in objects, which gives information about the indexs that are in the data base.
user_indexes : contains all user defined index’s information
Example :
select * from user_indexes;
select index_name, table_name, table_owner from user_indexes;
All_indexes : contains all user defined index’s and pre defined index’s information
Example :
select * from all_indexes;
select index_name, table_name, table_owner from all_indexes;
