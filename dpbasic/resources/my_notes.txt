<pre>
required skills - fullstack lead developer, architect
expert in modern software architecture concepts,]
expert in python language backend development
expert in js, react+redux+ag-grid, css, html5 - front end development
databases sql, plsql, nosql
knowlege of devOps, toolsets, agile/ceremonies/kanban/sprints/multifunctional small teams, pair programming, TDD approaches, cloud etc
packaging build and deployment

linux environment, docker, jenkin, cross platform builds, CI/CD, chef, ansible, puppet, building & consuming RESTFul APIs, microservice architecture
design patterns and software architecture patterns
written & verbal communications
analytical problem solving skills
modern QA practices
self starter, work closely, collaboratively
change, release, incident processes (ARM, Snow)

agile team capacity planner, available load, story points to deliver
one resource can max deliver 9-12 story points per sprint
agile ceremonies: planning, estimation, scrum, review, retrospective
artifacts: product increment, backlog, scrum backlog
KANBAN: todo|in progress|blocked|ready to be accepted|descoped|Done
DoD: definiation of Done
DoR: Definition of Readiness
glossary: acceptance criteria, architect, burn down/up charts, backlog grooming, daily scrum, Emergence(team will evolve), epic, estimation, fibbonacci (story points: 1,2,3,5,8,13), impediment, pig, planning, pocker, process, backlog, product owner, relative estimation, release, retro, scrum master(bridge between team and po, teach PO, improve dev team, engineering processes, tracks progress), role, self organisation, spike, sprint, sprint goal, task, story time, task list, task board, team, member, thumb vote, time boxing, velocity, what xp practices, empricism(impact and adapt)
what have you done yesterday, what you will do today, impediments
iterative and incremental development

DevOps Tool Set
code quality tools knowledge: Crucibel, SonarQube
Gradle:
    build tool like maven and ant but far more better as gradle scripts can be written in modern programing languages also groovie based DSL scripts
    dependency management, incremental builds, faster and reliable
git:
    code management tool
Jenkin (open source) like ARM in RBS, bamboo paid by atlassian:
    CI/CD: Continuous Integration / Continuous Deployment, sever that allows you to automate different stages of delivery pipeline
    huge pluggin ecosystem
    integrates with docker and puppet also
    prepare build, test build, deploy to pre prod and finally to prod
Docker:
    build, manage, secure apps, anywhere, os and platform independent
    container platform, you can use docker images rather than virtual images
    makes distributed development possible
    easy cloud migration
Kubernetes:
    solves scalability issue with docker container if it grows
Puppet:
    software discovery, configuration management
Chef:
    infra, build, and deploy
Raygun:
    error monitoring tools, performance monitoring, linked to code and traces back to line of error in code

two server side apis/systems can interact with each other with use of agent/broker, pub-sub modes or queue like Argon in RBS
send messages/files and receive messages
same thing happens in distributed and clustering environment
this sort of updates to slave nodes happenes in db replication services which pushes partial updates to nodes to make them in sync

REST APIs: Representation State Transfer
JSON (Javascript Object Notation) by Roy Fielding, since data is not tied to method, resource, REST can take advantages for having diff multiple calls, return different data sets, formats and change strcuture. The freedom and flexibility
its difficult to maintain states in REST and import to note that what makes REST a RESTFul services
5 of below to follow for a RESTful api
1. client server model: separate implementation, separation of concerns and duties
2. stateless: 2 APIs/URIs in application are independent of each other, the data or params for a service/api to work has been fed completely and independently.  Dont rely on server states to build objects. for better performance, store the states on the client rather than on server
3. Cache: faster, better performance apps, cache on client side, donig so reduces number of api calls and memory uses on servers
4. Uniform Interface: decouple client and server for independent evolution, standard means of communication, CRUD(Create Read Update Delete) implementation & JSON for scale
5. Layered System: apply MVC design pattern (model: data, view: output, controller: actions and events), this approach is used for security reasons also as different solves different security and accessibility concerns, encapsulation of data and add/deletion of components at runtime without making app down or crash
GET: read, PUT: update, POST: insert, Patch: partial update, Delete: delete

communication modes: synchronous, asynchronous

PORTS:
0-1023: common protocols and services
1024-49151: registered ICANN: servers
49152-65353: dynamic private

20/21: FTP - TCP (Transmission Control Protocol: control the packets)
22: SSH - TCP/UDP (User Datagram protocol: doesnt track like DNS, media stream, online multiplayer games, TFTP: trivial file transfer)
23: Telnet - TCP
25: smtp -TCP
50/51: IPSec
53: DNS - TCP/UDP
67/68: DHCP - TCP/UDP
69: TFTP - UDP
80/8080: http - TCP
110: POP3 - TCP
119: NNTP - TCP
123: NTP - TCP
135-139: Net BIOS - TCP/UDP
143: IMAP4
161/162: SNMP
443: https
389: LDAP (Active Directory)
3389: RDP (Remote Desktop Protocol)

Modern QA practices:
Breakfree from classical roles & responsibilty
choose your release criteria carefully
prioritise bug fixes based on users
admit 2 tier approach to test automation
    faster test cases on every commit
    complete regression overnight run in every sprint
stay close to relevent environment
form a dedicated security testing team
form a dedicated performance testing team
run a regression life cycle
simulate customer accounts as of production
perform sanity tests on production

Software Architecture patterns
general reusable solutions, solves scalability concerns, helps better design discussion and thoughts around
1. layered pattern:
2. client server pattern:
3. master slave / pub-sub / observer patterns:
4. pipe & filter pattern:
5. Broker pattern:
6. peer to peer pattern:
7. Event bus pattern:
8. MVC pattern:
9. Black Board pattern:
10. interpreter pattern:


ES6/ES2015 and other javascript concepts:
let & const: block scoped,**,default and expanding params, call(arg known similar to positional args in python)/apply(arg not known, similar to kwargs in python), Array.find(), Array.findIndex(), Array.includes()
Arrow Function: ()=>{}, short expression for writting function, great saviour for this & _that overhead, solves the this related context problems and undefine error is prevented,

Pure functions are those whose return value depends solely on the values of their arguments.

hoisting: vars anywhere in program is hoisted on top of code at runtime and this error of var defining doesnt occur, ie. you can define a var later in code and use it before elsewhere
"use strict" is used to prevent hoisting and js gives some code errors in design time which otherwise it wont give

Array.push(),pop(), shift(),unshift()

clousure: variable ref to a function which is return by another function, ie the state of function is saved and can be used later in program or passed along

array and objects behave same way in js array is also like object with default keys as integer indexes but diff is that you can define keys in object yourself. see same loop for objects and arrays

js always passes primtive objects by values like string var but when object/array is passed, its by references. however, changing the property of an object is which is primitives changes the underlying object since master object is by reference, hence its childs are also by refrences

Primitives (copied by value): null,undefined,Number,String,Boolean
Objects (copied by reference): Object,Array,Function

primitive js types are immutable

basic class:
https://javascript.info/class
https://www.digitalocean.com/community/tutorials/understanding-classes-in-javascript
class MyClass {
  prop = value; // field
  constructor(...) { // constructor
    // ...
  }
  method(...) {} // method
  get something(...) {} // getter method
  set something(...) {} // setter method
  [Symbol.iterator]() {} // method with computed name/symbol name
  // ...
}

class User {
  constructor(name) {
    this.name = name;
  }
  get name() {
      return this._name;
    }

    set name(value) {
      if (value.length < 4) {
        alert("Name is too short.");
        return;
      }
      this._name = value;
    }
  sayHi() {
    alert(this.name);
  }
}
let user = new User("John");
console.log(user.name,"is perfactly valid")
user.sayHi();
alert(typeof User);
alert(User === User.prototype.constructor);
alert(User.prototype.sayHi);
alert(Object.getOwnPropertyNames(User.prototype));

// rewriting class User in pure functions
// 1. Create constructor function
function User(name) {
  this.name = name;
}
// any function prototype has constructor property by default, so we don't need to create it
// 2. Add the method to prototype
User.prototype.sayHi = function() {
  alert(this.name);
};
let user = new User("John");
user.sayHi();

class expression may/may not have a name
let User = class {
  sayHi() {
    alert("Hello");
  }
};
new User().sayHi();

function makeClass(phrase) {
  // declare a class and return it
  return class {
    sayHi() {
      alert(phrase);
    };
  };
}
let User = makeClass("Hello");
new User().sayHi(); // Hello

computed properties
function f() { return "sayHi"; }

class User {
  [f()]() {
    alert("Hello");
  }

}
new User().sayHi();


Tornado:
Async web framework, non block i/o, scalable and distibuted, fault tolerant, high load, non blocking IO Loops, callbacks, tasks and timeouts, 
simple TCP server, Application Class need to be overridden, request handler, generators-async code and callbacks
can handle 1000s of paralle connections
define a requets handler, implement set, define an aplication, tell application to start, start ioloop
Application: collection of request handlers, implement listen, starts http server, sets application request callback, handle user request,
**kwargs={a:1,b:2} keyword args-length of params is not defined and is not known
*args=positional args -length of params is defined and is known
APP: parse the url, decide which handler to use, creates an instance of it and executes the code
each connection gets one instance
RequestHandler: calls the handler method, checks XSRF(Cross Site Request Forgery) cookie, closes the connection
IOLoop: core of the tornado, similar to Node, its also asynchronous and non blocking, useable standalong with WSGI, used for server and client, single instance per process
it execute forever, callbacks asap, timeout, handle events (something that changes the state of socket), 
Thread Local: stack context, keeps track of socket connections, handles association between sockets and classes
AddTimeout: pushesh the timeout to the heap of timeouts & wraps it in a stack context
websockets: similar to asynchronous, the connection is left open, after writting, waits for more, continuously sees IOStreaming, to application, its looks like request handler, acdepts the connections and decides the versoin of protocol and initiate websocket.protocol class
twisted mysql api for db connection tornado is considered the best
it also has templating mechanism
loader=tornado.template.loader('\..\..\folder path')
self.write(loader.load('index.html').generate('title'))
also self.write(json.dumps(base,jsonutil.default))
Tornado designed to be stateless and don't have session support out of the box.
Use secure cookies to store sensitive information like user_id. Use standard cookies to store not critical information.
For storing large objects - use standard scheme - MySQL + memcache.
Note that EVERY user will have a session, even those not logged in. When a user logs in, you'll want to attach their status as logged in (and their username/user_id, etc) to their session
The key issue with sessions is not where to store them, is to how to expire them intelligently. Regardless of where sessions are stored, as long as the number of stored sessions is reasonable (i.e. only active sessions plus some surplus are stored), all this data is going to fit in RAM and be served fast
pip install torndsession: Torndsession is a session extension for Tornado web framework. Torndsession support application memory, file, redis or memcached to save session data for request, and it’s easy to extend for developer

tornado session management - its stateless and hence internal management of sessions and you can use secure_cookies:
Cookies and secure cookies, Cookies are not secure and can easily be modified by clients.
set_secure_cookie and get_secure_cookie
You can set cookies in the user’s browser with the set_cookie method:
class MainHandler(tornado.web.RequestHandler):
    def get(self):
        if not self.get_cookie("mycookie"):
            self.set_cookie("mycookie", "myvalue")
            self.write("Your cookie was not set yet!")
        else:
            self.write("Your cookie was set!")
application = tornado.web.Application([
    (r"/", MainHandler),
], cookie_secret="__TODO:_GENERATE_YOUR_OWN_RANDOM_VALUE_HERE__")


class MainHandler(tornado.web.RequestHandler):
    def get(self):
        if not self.get_secure_cookie("mycookie"):
            self.set_secure_cookie("mycookie", "myvalue")
            self.write("Your cookie was not set yet!")
        else:
            self.write("Your cookie was set!")

class BaseHandler(tornado.web.RequestHandler):
    def get_current_user(self):
        return self.get_secure_cookie("user")

class MainHandler(BaseHandler):
    def get(self):
        if not self.current_user:
            self.redirect("/login")
            return
        name = tornado.escape.xhtml_escape(self.current_user)
        self.write("Hello, " + name)

class LoginHandler(BaseHandler):
    def get(self):
        self.write('<html><body><form action="/login" method="post">'
                   'Name: <input type="text" name="name">'
                   '<input type="submit" value="Sign in">'
                   '</form></body></html>')

    def post(self):
        self.set_secure_cookie("user", self.get_argument("name"))
        self.redirect("/")

application = tornado.web.Application([
    (r"/", MainHandler),
    (r"/login", LoginHandler),
], cookie_secret="__TODO:_GENERATE_YOUR_OWN_RANDOM_VALUE_HERE__")

class MainHandler(BaseHandler):
    @tornado.web.authenticated
    def get(self):
        name = tornado.escape.xhtml_escape(self.current_user)
        self.write("Hello, " + name)

settings = {
    "cookie_secret": "__TODO:_GENERATE_YOUR_OWN_RANDOM_VALUE_HERE__",
    "login_url": "/login",
    "xsrf_cookies": True,
}
application = tornado.web.Application([
    (r"/", MainHandler),
    (r"/login", LoginHandler),
], **settings)

UIModule xsrf_form_html()
<form action="/new_message" method="post">
  {% module xsrf_form_html() %}
  <input type="text" name="message"/>
  <input type="submit" value="Post"/>
</form>

function getCookie(name) {
    var r = document.cookie.match("\\b" + name + "=([^;]*)\\b");
    return r ? r[1] : undefined;
}

jQuery.postJSON = function(url, args, callback) {
    args._xsrf = getCookie("_xsrf");
    $.ajax({url: url, data: $.param(args), dataType: "text", type: "POST",
        success: function(response) {
        callback(eval("(" + response + ")"));
    }});
};

DNS rebinding is an attack that can bypass the same-origin policy and allow external sites to access resources on private networks. 
# GOOD: same as previous example using tornado.routing.
app = Application([
    (HostMatches(r'(localhost|127\.0\.0\.1)'),
        [('/foo', FooHandler)]),
    ])

Third party authentication
The tornado.auth module implements the authentication and authorization protocols for a number of the most popular sites on the web, including Google/Gmail, Facebook, Twitter, and FriendFeed. 
class GoogleOAuth2LoginHandler(tornado.web.RequestHandler,
                               tornado.auth.GoogleOAuth2Mixin):
    async def get(self):
        if self.get_argument('code', False):
            user = await self.get_authenticated_user(
                redirect_uri='http://your.site.com/auth/google',
                code=self.get_argument('code'))
            # Save the user with e.g. set_secure_cookie
        else:
            await self.authorize_redirect(
                redirect_uri='http://your.site.com/auth/google',
                client_id=self.settings['google_oauth']['key'],
                scope=['profile', 'email'],
                response_type='code',
                extra_params={'approval_prompt': 'auto'})


React:
design principal of react: https://reactjs.org/docs/design-principles.html
order of life cycle events: 
  Mounting: constructor()->static getDerivedStateFromProps()->render()->componentDidMount()
  Updating: static getDerivedStateFromProps()->shouldComponentUpdate(nextProps, nextState)->render()->getSnapshotBeforeUpdate()->componentDidUpdate(prevProps, prevState, snapshot)
  Unmounting: componentWillUnmount()
  Error: during render, static getDerivedStateFromError()->componentDidCatch(). to avoid system error being displayed, write an ErrorBoundary class with fallback ui, implement static getDerivedStateFromError(error), componentDidCatch(error, info) methods
  setState(updater[, callback]), component.forceUpdate(callback)
  render() will not be invoked if shouldComponentUpdate() returns false.
  componentDidUpdate() will not be invoked if shouldComponentUpdate() returns false.
  Calling forceUpdate() will cause render() to be called on the component, skipping shouldComponentUpdate(). This will trigger the normal lifecycle methods for child components, including the shouldComponentUpdate() method of each child

what all ways we can rerender components: An update can be caused by changes to state. Props are readonly
hooks >  v16.8.0 Hooks let you use more of React’s features without classes:
  import React, { useState } from 'react';
  function Example() {
    // Declare a new state variable, which we'll call "count"
    const [count, setCount] = useState(0);

    return (
      <div>
        <p>You clicked {count} times</p>
        <button onClick={() => setCount(count + 1)}>
          Click me
        </button>
      </div>
    );
  }
advantages and disadvantages of react over framework:
controlled and uncontrolled components: In a controlled component, form data is handled by a React component. The alternative is uncontrolled components, where form data is handled by the DOM itself. To write an uncontrolled component, instead of writing an event handler for every state update, you can use a ref to get form values from the DOM.

named export (can be multiple per file) and default export (only one per file)
export const MyComponent = () => {} //named export
export const MyComponent2 = () => {}
import {MyComponent,MyComponent2} from 'mycomponents file'

export default const MyComponent = () => {}
import MyComponent from './compfile'

axios api for data fetch
aggrid for tabular data
es6-promise
html-react-parser
redux
lodash - js api for manipuaing objects and arrays
react, react-dom,react-scripts, create-react-app <app name>, npm start, npm run build, PostBuild:''
babel-core for reading and recognising JSX syntax (html inside of js body)

map higher order function: 
  newArr=arr.map((elm,id)=>return elm) or arr.map(elm=>elm*2)
  newArr=obj.map(ox=> ox.id+"->"+ox.data)
  return a new array of same length of original array on which it runs

reduce higher order function: 
  var mostExpPilot=pilots.reduce((accumulator,pilot)=>accumulator+pilot.years,0), 0 is the inital value of accumulator
  returns a signle value of the iteration and it passes result of previous run as callback

filter higher order function: 
  filter out some elements based on some condition
  var rebels=arr.filter(pilot=>pilot.fact==='Rebels')

Chained Up:
const totalJediScore=persons.filter(person=>person.isForcedUser).map(jedi=>jedi.pilotScore+pilot.shootingScore).reduce(acc,score=>acc+score,0)

promises: q, bluebird, deffered.js, vow, ES6 way (async over function and await over actual call)
older way of promise is the resolve and reject callbacks
var wait100=new Promise((resolve, reject)=>{
  setTimeout(()=>resolve,1000)
}).then(()=>{console.log('x')})

desctruct syntax:
arr=[1,2,3,4,5,6] 
[one,two,...three]=arr
one=1, two=2, three=[3,4,5,6]

obj={a:1,b:2,c:3,d:4}
{a,b,c}=obj, gets the fields from object and store values in it

Pass the context
var logUpperCase=function(){
  this.string=this.string.upperCase()
  return ()=>console.log(this.string)
}
//closure call
logUpperCase.call({string:'my name is dp'})

module & class
export class a extends b{
  construcotr(option, data){
    super(args)
    this.name=""
    this.age=10
  }
}
eg in file1: module.exports={
  fn=()=>{console.log('func call from file1')}
}
in file2:
var file1=require('./file1.js')
file1.fn()
this is named export: import {x,y} from 'module1'
this is annonymous export: import z from 'module1'
import * as x from 'module1'

simplest react component is to write js function return jsx
this is function based components, cannot use setState mehtods, these are stateless components
hence if you want to use states in functional component uplift your component to parent visibility and pass down states as props
react16.8 uses hooks, that means now you can use lifeCycle events in functional components
function Welcome(props) {
  return <h1>Hello, {props.name}</h1>;
}

class based component is ES6 syntax follow react lifecycle events like componentDidMount() and ComponentDidUpdate() etc as they all come down from React.Component class
can use state and setState mehtods
class Welcome extends React.Component {
  render() {
    return <h1>Hello, {this.props.name}</h1>;
  }
}


Redux:
central state management library/framework which works on concept of action dispatchers/reducers to maintain states and centrally make it available by provider & connect, values comes via props
connect the component to redux import {connect} from 'react-redux'
mapStateToProps=(state)=>{
  counter: state.counter
}
connect function also injects the dispatch events into components as props
so store.dispatch({type:'INCREMENT'}) in main app is equivalent to this.props.dispatch({type:'xxxx'}), you can check this in react dev tools
export default connect({mapStateToProps})(<componentClass>) //connect is higher order function, when we call it, it returns a function with component being exported
create the store,go to main app component and create the store, import {creatStore} fropm 'redux' and top most line after import should be
const store=creatStoreate(reducerFunction) //reducer function for initial state of store
provide store to the app
import {Provider} from 'react-redux'
const app=()=>{
  <Provider store={store}><component /></Provider>
}
createStore requires a reducer function for initial state to store
function reducerFunction(state, actions){, its a pure function //takes 2 args (previous state, action object from dispatcher), make sure reducer never returns null
  //check actions.type==='INCREMENT'
  //check actions.type==='DECREMENT'
  return {counter:1}
}
WrapUp Actions: state has function called dispatchers and changing states is via dispatching events
store.dispatch({type:'INCREMENT'}) //passes action to reducers thats being assigned in store
store.dispatch({type:'DECREMENT'})

three principles that Redux follow
Single source of truth:
State is read-only:
Changes are made with pure functions:

advantages of Redux: Predictability of outcome, Maintainability, Server-side rendering, Developer tools, Community and ecosystem, Ease of testing, Organization

components of Redux
Action – It’s an object that describes what happened.
Reducer –  It is a place to determine how the state will change.
Store – State/ Object tree of the entire application is saved in the Store.
View – Simply displays the data provided by the Store.

Explain Flux, similar to redux
Flux is an architectural pattern which enforces the uni-directional data flow. It controls derived data and enables communication between multiple 
components using a central Store which has authority for all data. Any update in data throughout the application must occur here only. Flux provides 
stability to the application and reduces run-time errors.


BigO:
time to paint a fense of height, weight, p layers is O(w*h*p). any time taken to execute any program or algo
space complexity is paralleled to time complexity, given size of n array has O(n) space requirement or number of bytes it needs for storage in memory
sorting
if memory constraints are very tight you can use Quick Sort and generally its fastest, whose worst-time compelxity is O(n^{2}) but average case complexity is O(nlogn).
Merge Sort is the fastest stable sorting algorithm with worst-case complexity of O(nlogn), but it requires extra space. 

for 2d array O(n^2), n*n matrix
addition is better than multiplication
additon: for(int a:arrA){ print(a) } for(int b:arrB){ print(b) }, it has O(A+B)
multiplication: for(int a:arrA){ for(int b:arrB){ print(a,b) }}, it has O(A*B)
if the algorithm is in the form of "do this then this", its addition type of algo, then just add the runtimes
if the algorithm is in the form of "do this for each of this", its multiplicaton type of algo, then just multiply the runtimes

Amortised time: eg ArrayList, when array is full, create double the list by copying N elements, this is done once in a while so wrt to cost over the program
we arfe ok to do it, time to this is amortised
as we insert element we double the capacity,eg 1,2,4,8,16,32,64... COPY
we make copies with every insertions, if left to right is like double and right to left is 1/2 of x, then this pattern is of Logrithmic complexity and runtime is O(logN)
eg binary search, with every search, search list is reduced to half.
Left 2 Right:(N=16, N=8, N=4, N=2, N=1), right 2 left(N=1, N=2, N=4, N=8, N=16)
N=2^k, what is k, this is what exactly log expresses, 16=2^4, k=4 or log2 16=4 => log2 N=k or 2^k=N
this is same reason why we have runtime complexity as O(logN) in balanced binary tree

Recursive Algorithms
int f(int n){
  if (n<=1){
    return 1
  }
  return f(n-1)+f(n-2)
}
we are double it each time we loop over it, hence its O(n*n), call is getting doubled, sum of power of 2
2^0+2^1+2^2+2^3+2^4+......+2^n
so for recursive calls, often but not always is O(branches ^ n), O((2^n+1) - 1) nodes will be there
the space complexity of this would be O(N)


Design Patterns
Creational design patterns
These design patterns are all about class instantiation. This pattern can be further divided into class-creation patterns and object-creational patterns. While class-creation patterns use inheritance effectively in the instantiation process, object-creation patterns use delegation effectively to get the job done.
Example of Abstract Factory
Abstract Factory: Creates an instance of several families of classes
Builder: Separates object construction from its representation
Factory Method: Creates an instance of several derived classes
Object Pool: Avoid expensive acquisition and release of resources by recycling objects that are no longer in use
Prototype: A fully initialized instance to be copied or cloned
Singleton: A class of which only a single instance can exist

Structural design patterns
These design patterns are all about Class and Object composition. Structural class-creation patterns use inheritance to compose interfaces. Structural object-patterns define ways to compose objects to obtain new functionality. 
Adapter: Match interfaces of different classes
Bridge: Separates an object’s interface from its implementation
Composite: A tree structure of simple and composite objects
Decorator: Add responsibilities to objects dynamically
Facade: A single class that represents an entire subsystem
Flyweight: A fine-grained instance used for efficient sharing
Private Class Data: Restricts accessor/mutator access
Proxy: An object representing another object

Behavioral design patterns
These design patterns are all about Class's objects communication. Behavioral patterns are those patterns that are most specifically concerned with communication between objects.
Chain of responsibility: A way of passing a request between a chain of objects
Command: Encapsulate a command request as an object
Interpreter: A way to include language elements in a program
Iterator: Sequentially access the elements of a collection
Mediator: Defines simplified communication between classes
Memento: Capture and restore an object's internal state
Null Object: Designed to act as a default value of an object
Observer: A way of notifying change to a number of classes
State: Alter an object's behavior when its state changes
Strategy: Encapsulates an algorithm inside a class
Template method: Defer the exact steps of an algorithm to a subclass
Visitor: Defines a new operation to a class without change



python:
practice some programs: https://dbader.org
difference between python2 and 3:
  Python 3.0 (most favoured python implementation) was released in 2008. Its newest version, 3.6, was released in 2016, and version 3.7 is currently in development.
  among plenty of differences but 5 of these being most prevelent
  legacy->future, library (not forward compatible)->todays libraries are strictly 3 onwards, Strings are ASCII->Strings are uncode, rounds calci->correct calci, print->print()

To make your project be single-source Python 2/3 compatible, the basic steps are:
  Only worry about supporting Python 2.7
  Make sure you have good test coverage (coverage.py can help; pip install coverage)
  Learn the differences between Python 2 & 3
  Use Futurize (or Modernize) to update your code (e.g. pip install future)
  Use Pylint to help make sure you don’t regress on your Python 3 support (pip install pylint)
  Use caniusepython3 to find out which of your dependencies are blocking your use of Python 3 (pip install caniusepython3)
  Once your dependencies are no longer blocking you, use continuous integration to make sure you stay compatible with Python 2 & 3 (tox can help test against multiple versions of Python; pip install tox)
  Consider using optional static type checking to make sure your type usage works in both Python 2 & 3 (e.g. use mypy to check your typing under both Python 2 & Python 3).

Unicode strings can store foreign language letters, Roman letters and numerals, symbols, emojis, etc., offering you more choices
Each newer version of Python continues to get faster runtime. Meanwhile, nobody’s currently working to make Python 2.7 work faster. Community support is better with Python 3.

range and xrange:
    range returns a Python list object (big list, big memory needs, space) and xrange returns an xrange object (generators, on demand, saves memory)
    both takes 3 args, start, stop, step

del: delete variables and cleans memory
packages: import sounds.effects.echo /  from sounds.effects import echo, logical separation of modules
file handling:
    f=open("path",r/w/a), while true: line=f.readline() f.close
    for line in open(path): print line
    with open(path) as f: for line in f: print line
try...except....finally
dictionary: a={}, a=dict()
sets: a=set()
lists: a=[] a=list()
generators: yield x, produces iterators, produces sequences of values instead of single value, iterator = ('Hello' for i in range(3))
generator expressions:
    Generator expressions are a high-performance, memory–efficient generalization of list comprehensions and generators
    e.g.  iterators in a single line of code, iterator = ('Hello' for i in range(3)), next(iterator)...
    genexpr = (expression for item in collection)
    def generator():
        for item in collection:
            yield expression
    even_squares = lambda n: (x * x for x in range(n) if x % 2 == 0)

store in env vars:
    export API_KEY="your-api-key"
    import os
    api_key = os.getenv("API_KEY")

comprehensions: [i for i in [1,2,3,4,5]], [i for i in range(10)], [(i,f) for i in nums for f in fruit], [(i,f) for i in nums for f in fruit if f[0] == "P" if i%2 == 1]
collections:
    https://docs.python.org/2/library/collections.html
  Counter: The Counter() function in collections module takes an iterable or a mapping as the argument and returns a Dictionary.  counter.elements() as list
  defaultdict:  The defaultdict works exactly like a python dictionary, except for it does not throw KeyError when you try to access a non-existent key. nums=defaultdict(int) #int is passed as the default_factory
  OrderedDict: OrderedDict is a dictionary where keys maintain the order in which they are inserted, which means if you change the value of a key later, it will not change the position of the key.
  deque: The deque is a list optimized for inserting and removing items.
  ChainMap: ChainMap is used to combine several dictionaries or mappings. It returns a list of dictionaries.
  namedtuple: The namedtuple() returns a tuple with names for each position in the tuple. One of the biggest problems with ordinary tuples is that you have to remember the index of each field of a tuple object. This is obviously difficult

pickling: Pickle module accepts any Python object and converts it into a string representation and dumps it into a file by using dump function, this process is called pickling
Python memory is managed by Python private heap space. it also has built in garbage collector
PyChecker/pylint is a static analysis tool that detects the bugs in Python source code and warns about the style and complexity of the bug
A Python decorator is a specific change that we make in Python syntax to alter functions easily.
Everything in Python is an object and all variables hold references to the objects.
pass is no-operation Python statement
slicing: A mechanism to select a range of items from sequence types like list, tuple, strings etc.
copying copy.copy () or copy.deepcopy()
share vars and object across: To share global variables across modules within a single program, create a special module. Import the config module in all modules of your application. The module will be available as a global variable across modules.
make a Python Script executable on Unix, you need to do two things, Script file's mode must be executable and the first line must begin with # ( #!/usr/local/bin/python)
delete files in python os.remove (filename) or os.unlink(filename)
namespace, In Python, every name introduced has a place where it lives and can be hooked for
lambda is a single expression anonymous function often used as inline function.
A lambda form in python does not have statements as it is used to make new function object and then return them at runtime.
ORM in python: 
  not suitable for larger application having volumnous data and which needs speed to run
  Facilitates implementing domain model pattern
  django uses ORM features where we define modules and export them dierctly to databases, the django framework maintains these ORM modals and data inside them
  sqlAlchemy is very famous ORB library for working with DBs
  Rails ORM using the Active Record architectural pattern
Explain what is Dogpile effect? How can you prevent this effect?
Dogpile effect is referred to the event when cache expires, and websites are hit by the multiple requests made by the client at the same time. This effect can be prevented by using semaphore lock. In this system when value expires, first process acquires the lock and starts generating new value.

django:
    https://data-flair.training/blogs/django-interview-questions/


flask session management: 
  Unlike a Cookie, Session data is stored on server. Session is the time interval when a client logs into a server and logs out of it.
  A session with each client is assigned a Session ID, , a Flask application needs a defined SECRET_KEY.

  from flask import Flask, session, redirect, url_for, escape, request
  app = Flask(__name__)
  app.secret_key = 'any random string’

  Session[‘username’] = ’admin’
  session.pop('username', None)
  @app.route('/')
  def index():
    if request.method == 'POST':
        session['username'] = request.form['username']
        return redirect(url_for('index'))
     return '''

   if 'username' in session:
      username = session['username']
         return 'Logged in as ' + username + '<br>' + \
         "<b><a href = '/logout'>click here to log out</a></b>"
   return "You are not logged in <br><a href = '/login'></b>" + \
      "click here to log in</b></a>"

@app.route('/logout')
def logout():
   # remove the username from the session if it is there
   session.pop('username', None)
   return redirect(url_for('index'))



Oracle:
What is PL SQL
Differentiate between % ROWTYPE and TYPE RECORD.
    % ROWTYPE is used when a query returns an entire row of a table or view.
    TYPE RECORD, on the other hand, is used when a query returns column of different tables or views.
    Eg. TYPE r_emp is RECORD (sno smp.smpno%type,sname smp sname %type)
    e_rec smp %ROWTYPE
    Cursor c1 is select smpno,dept from smp;
    e_rec c1 %ROWTYPE
Show code of a cursor for loop
    Eg. FOR smp_rec IN C1 LOOP
    totalsal=totalsal+smp_recsal;
    ENDLOOP;
Explain the uses of database trigger: autocode that runs on INSERT, UPdate and Delete
    used in
        1) Audit data modifications.
        2) Log events transparently.
        3) Enforce complex business rules.
        4) Maintain replica tables
        5) Derive column values
        6) Implement Complex security authorizations
What are the two types of exceptions: user_defined and predefined.
Show some predefined exceptions: DUP_VAL_ON_INDEX, ZERO_DIVIDE, NO_DATA_FOUND, TOO_MANY_ROWS, CURSOR_ALREADY_OPEN, INVALID_NUMBER, INVALID_CURSOR, PROGRAM_ERROR, TIMEOUT _ON_RESOURCE, STORAGE_ERROR, LOGON_DENIED, VALUE_ERROR
Explain Raise_application_error: It is a procedure of package DBMS_STANDARD that allows issuing of user_defined error messages from database trigger or stored sub-program.
Show how functions and procedures are called in a PL SQL block.
    Function: total:=calculate_sal('b644')
    Procedure: calculate_bonus('b644');
Explain two virtual tables available at the time of database trigger execution.
    THEN.column_name and NOW.column_name.
    For INSERT related triggers, NOW.column_name values are available only.
    For DELETE related triggers, THEN.column_name values are available only.
    For UPDATE related triggers, both Table columns are available.
What are the rules to be applied to NULLs whilst doing comparisons?
1) NULL is never TRUE or FALSE
2) NULL cannot be equal or unequal to other values
3) If a value in an expression is NULL, then the expression itself evaluates to NULL except for concatenation operator (||)
How is a process of PL SQL compiled?
    Compilation process includes syntax check, bind and p-code generation processes.
    Syntax checking checks the PL SQL codes for compilation errors. When all errors are corrected, a storage address is assigned to the variables
    that hold data. It is called Binding. P-code is a list of instructions for the PL SQL engine. P-code is stored in the database for named blocks and
    is used the next time it is executed.
Differentiate between Syntax and runtime errors.
    A syntax error can be easily detected by a PL/SQL compiler. For eg, incorrect spelling.
    A runtime error is handled with the help of exception-handling section in an PL/SQL block. For eg, SELECT INTO statement, which does not return any rows.
Explain Commit, Rollback and Savepoint.
    For a COMMIT statement, the following is true:
        Other users can see the data changes made by the transaction.
        The locks acquired by the transaction are released.
        The work done by the transaction becomes permanent.
    A ROLLBACK statement gets issued when the transaction ends, and the following is true.
        The work done in a transition is undone as if it was never issued.
        All locks acquired by transaction are released.
        It undoes all the work done by the user in a transaction.
   With SAVEPOINT, only part of transaction can be undone.
Define Implicit and Explicit Cursors.
    A cursor is implicit by default. The user cannot control or process the information in this cursor.
    If a query returns multiple rows of data, the program defines an explicit cursor. This allows the application to process each row sequentially as the cursor returns it.
Explain mutating table error.
    It occurs when a trigger tries to update a row that it is currently using.
    It is fixed by using views or temporary tables, so database selects one and updates the other. eg read from view and update table
When is a declare statement required?
    DECLARE statement is used by PL SQL anonymous blocks such as with stand alone, non-stored procedures. If it is used, it must come first in a stand alone file.
How many triggers can be applied to a table?: max 12
What is the importance of SQLCODE and SQLERRM?: SQLCODE-error number SQLERRM: message for the last error.
If a cursor is open, how can we find in a PL SQL Block?: the %ISOPEN cursor status variable can be used.
Show the two PL/SQL cursor exceptions.: Cursor_Already_Open, Invaid_cursor
What operators deal with NULL?: NVL converts NULL to another specified value, var:=NVL(var2,'Hi'); IS NULL and IS NOT NULL
Does SQL*Plus also have a PL/SQL Engine?: No, Thus, all PL/SQL code is sent directly to database engine
What packages are available to PL SQL developers?: DBMS_ series of packages, such as, DBMS_PIPE, DBMS_DDL, DBMS_LOCK, DBMS_ALERT, DBMS_OUTPUT, DBMS_JOB, DBMS_UTILITY, DBMS_SQL, DBMS_TRANSACTION, UTL_FILE.
Explain 3 basic parts of a trigger.: A triggering statement or event, A restriction, An action
What are character functions?: INITCAP, UPPER, SUBSTR, LOWER and LENGTH
Show the cursor attributes of PL/SQL.
    %ISOPEN : Checks if the cursor is open or not
    %ROWCOUNT : The number of rows that are updated, deleted or fetched.
    %FOUND : Checks if the cursor has fetched any row. It is true if rows are fetched
    %NOT FOUND : Checks if the cursor has fetched any row. It is True if rows are not fetched.
What is an Intersect?: Intersect is the product of two tables and it lists only matching rows
What are sequences?: Sequences are used to generate sequence numbers without an overhead of locking. Its drawback is that the sequence number is lost if the transaction is rolled back
What are the uses of SYSDATE and USER keywords?: SYSDATE: pseudo column server system date. USER: pseudo column current user logged onto the session. used for mainly logging
How does ROWID help in running a query faster?
    ROWID is the logical address of a row, it is not a physical column. It composes of data block number, file number and row number in the data block. Thus, I/O time gets minimized retrieving the row, and results in a faster query.
What are database links used for?
    Database links are created in order to form communication between various databases, or different environments like test, development and production. The database links are read-only to access other information as well.
What does fetching a cursor do?: Fetching a cursor reads Result Set row by row.
What does closing a cursor do?: Closing a cursor clears the private SQL area as well as de-allocates memory
Explain the uses of Control File.: It is a binary file. It records the structure of the database. It includes locations of several log files, names and timestamps. They can be stored in different locations to help in retrieval of information if one file gets corrupted.
Explain Consistency: Consistency shows that data will not be reflected to other users until the data is commit, so that consistency is maintained.
Differ between Anonymous blocks and sub-programs.: Anonymous blocks are unnamed blocks that are not stored anywhere whilst sub-programs are compiled and stored in database. They are compiled at runtime.
Differ between DECODE and CASE.
    DECODE and CASE statements are very similar, but CASE is extended version of DECODE. DECODE does not allow Decision making statements in its place.
    select decode(totalsal=12000,'high',10000,'medium') as decode_tesr from smp where smpno in (10,12,14,16); This statement returns an error.
    CASE is directly used in PL SQL, but DECODE is used in PL SQL through SQL only.
Explain autonomous transaction.
    An autonomous transaction is an independent transaction of the main or parent transaction. It is not nested if it is started by another transaction.
    There are several situations to use autonomous transactions like event logging and auditing.
What are the uses of MERGE?
    MERGE is used to combine multiple DML statements into one.
    Syntax : merge into tablename
            using(query)
            on(join condition)
            when not matched then
            [insert/update/delete] command
            when matched then
            [insert/update/delete] command
Can 2 queries be executed simultaneously in a Distributed Database System?
    Yes, they can be executed simultaneously. One query is always independent of the second query in a distributed database system based on the 2 phase commit.
What is out parameter used for eventhough return statement can also be used in pl/sql?
    Out parameters (similar to byref) allows more than one value in the calling program. Out parameter is not recommended in functions. Procedures can be used instead of functions if multiple values are required. Thus, these procedures are used to execute Out parameters.
How would you convert date into Julian date format?: select to_char(to_date('29-Mar-2013','dd-mon-yyyy'),'J') as julian from dual;
Explain SPOOL
    Spool command can print the output of sql statements in a file.
    spool/tmp/sql_outtxt
    select smp_name, smp_id from smp where dept='accounts';
    spool off;
Mention what PL/SQL package consists of?
    A PL/SQL package consists of
        PL/SQL table and record TYPE statements
        Procedures and Functions
        Cursors
        Variables ( tables, scalars, records, etc.) and constants
        Exception names and pragmas for relating an error number with an exception
        Cursors
Mention what are the benefits of PL/SQL packages?
    Enforced Information Hiding: It offers the liberty to choose whether to keep data private or public
    Top-down design: You can design the interface to the code hidden in the package before you actually implemented the modules themselves
    Object persistence: Objects declared in a package specification behaves like a global data for all PL/SQL objects in the application. You can modify the package in one module and then reference those changes to another module
    Object oriented design: The package gives developers strong hold over how the modules and data structures inside the package can be used
    Guaranteeing transaction integrity: It provides a level of transaction integrity
    Performance improvement: The RDBMS automatically tracks the validity of all program objects stored in the database and enhance the performance of packages
Mention what are different methods to trace the PL/SQL code?
    DBMS_APPLICATION_INFO
    DBMS_TRACE
    DBMS_SESSION and DBMS_MONITOR
    trcsess and tkproof utilities
Explain how you can copy a file to file content and file to PL/SQL table in advance PL/SQL?: fcopy procedure, file2pstab
Mention what is the function that is used to transfer a PL/SQL table log to a database table?: PROCEDURE ps2db
What is the difference between a Heap table and a Clustered table? How can we identify if the table is a heap table
    A Heap table is a table in which, the data rows are not stored in any particular order,  no clustered index.
    A clustered table is a table that has a predefined clustered index on one column or multiple columns of the table that defines the storing order of the rows within the data pages and the order of the pages within the table, based on the clustered index key.
    sys.partitions, sys.indexes
Online Transaction Processing (OLTP) databases, workloads are used for transactional systems
Online Analytical Processing (OLAP) database workloads are used for data warehousing systems (dimensions & measures)
Difference between varchar and varchar2 data types?: 2000 bytes, occupies space for null / 4000 bytes doesnt occupy space for null
What is RAW datatype: stores binary data with max size 32767
What are nested tables?:   Nested table is a data type in Oracle which is used to support columns containing multi valued attributes. It also hold entire sub table
What is COALESCE function: check null and returns next not null, if all null, then return null, Coalesce(value1, value2,value3,…)
What is BLOB datatype?: A BLOB data type is a varying length binary string which is used to store two gigabytes memory. Length should be specified in Bytes for BLOB
What is the difference between TRANSLATE and REPLACE?: Translate is used for character by character substitution and Replace is used substitute a single character with a word
What is a sub query and what are the different types of subqueries?
    Sub Query is also called as Nested Query or Inner Query which is used to get data from multiple tables. A sub query is added in the where clause of the main query.
    Correlated sub query: A Correlated sub query cannot be as independent query but can reference column in a table listed in the from list of the outer query.
    Non-Correlated subquery: This can be evaluated as if it were an independent query. Results of the sub query are submitted to the main query or parent query.
What is cross join?: Cross join is defined as the Cartesian product of records from the tables present in the join. Cross join will produce result which combines each row from the first table with the each row from the second table.
privileges: GRANT user1 TO user2 WITH MANAGER OPTION;[/sql]
VArray: columns containing multivalued attributes, hold bounded array of values
details of a table: desc tablename
SET operators: Union, Union All, Intersect and Minus
delete duplicate rows in a table?: Duplicate rows in the table can be deleted by using ROWID
Can we store pictures in the database and if so, how it can be done?: Yes, we can store pictures in the database by Long Raw Data type
What is hash cluster?: Hash Cluster is a technique used to store the table for faster retrieval. Apply hash value on the table to retrieve the rows from the table
constraints: null, not null, check, default
parameter mode: IN, OUT, IN OUT
$ORACLE_BASE and $ORACLE_HOME: base dir and beneath base
last record added: Select * from (select * from employees order by rownum desc) where rownum<2;
How to display employee records who gets more salary than the average salary in the department
    Select * from employee where salary>(select avg(salary) from dept, employee where dept.deptno = employee.deptno;
Indexes
It is a database object used to sort the values of a column for retrieving data fastly from a table. By default the rows in a table are identified with their row id’s
When ever you create an index on a table, the system gives index id’s which are stored in the index. If a table is having index then the system retrieves the rows basing on the index id’s else it retrieves value using row id’s
Types of Index
There are two types of indexes
1. Simple Index
2. Composite Index
There are two ways to create an index. They are
Creating index on a single column
Creating index on multiple columns
Simple Index
Any index created on a single table is called a simple index
Syntax : create index <index name> on <table name>(column);
Creating Index On A Single Column
When you create an index on a single column,
Create Table temporary(sno number(3), sname varchar2(10));
NOTE :
Index name must be unique in the database and it is user choice. Since index is a data base object, system allocates some memory in the database. The index stores all index id’s allocated for a column.
Note : we can not create any index on views.

SQL> select * from temporary;
SNO NAME
--------- ----------
110 Nithya
111 Saloni
112 Mahesh
113 Priya
114 Prasad
115 Aruna

creating an index
create index idx_temp on temporary(sno);
Note : We can not see the Details of Index file
select * from idx_temp;
ERROR at line 1:
ORA-00942: table or view does not exist
Creating Unique indexes on a Single column
Syntax :
Create unique index <index name> on <table name> (<column>)

Note : Unique index can be created on any table, except it does not contain any duplicate values on the column for which you are creating a unique index

Example :
create unique index idx_temp on sample1(sno);
insert into sample1 values(&sno,’&sname’);
enter any value for sno: 1
enter any value for sname : Mahesh
enter any value for sno: 2
enter any value for sname : Prasad
enter any value for sno: 1
enter any value for sname : Nithya
Error :
Ora-00001: unique constraint (scott.idx_temp) violated
Note :
Once if u create any index on a particular column, then that column can not be re indexed. To re index on that column, first remove that existing index using drop index command and then create new index on that column
If you create an index on an existing index, it will show an error message
create unique index idx_temp1 on temporary(sno)
ERROR at line 1:
ORA-01408: such column list already indexed

Dropping an Index
Drop Index : This Command Is Used To Drop Any Index
Syntax : Drop Index <Index Name>
Example : drop index idx_temp;

Composite index
An index created on 2 or more columns in a table is called composite index.
Syntax :
Create index <index name> on
<table name> (<column1>, <column2>,-------)

first create a table
create table bank(accno number(3),accna varchar2(10),ddno number(6),
amt number(6));

next Create the index on that table
create index idx_bank on bank (accno,ddno);

next Insert some values into that table
insert into bank values (100,'Prasad',11111,2343)
insert into bank values (102,'Mahesh',22222,4334)
insert into bank values (102,'Nithya',22222,4334)
select * from bank;
ACCNO ACCNA DDNO AMT
--------- ---------- --------- -------------
100 Prasad 11111 2343
102 Mahesh 22222 4334
102 Nithya 22222 4334
100 Saloni 11111 3433

Composite unique index
We can create unique index on two or more columns also using composite unique index
Syntax :
Create unique index <index name> on <table name> (<column1>,<column2>,-------)
create unique index idx_bank1 on bank1(accno,ddno);
insert into bank values (100,'Priya',11111,3445)
insert into bank values (102,'Mahi',22222,4334)
insert into bank values (102,'Sound',22222,4334)
ERROR at line 1:
ORA-00001: unique constraint (SCOTT.IDX_BANK1) violated
Note :
1. There is a problem in the above index, the index will give an error only when both columns value are given duplicate values. If any one is different the index will accept those values.
2. Once a column is indexed already, it is not possible to reindex that column to any other index file
Drop index
This command is used to drop any index
Syntax : Drop index <index name>
Ex : Drop Index Idx_Sample;
There are two built in objects, which gives information about the indexs that are in the data base.
user_indexes : contains all user defined index’s information
Example :
select * from user_indexes;
select index_name, table_name, table_owner from user_indexes;
All_indexes : contains all user defined index’s and pre defined index’s information
Example :
select * from all_indexes;
select index_name, table_name, table_owner from all_indexes;

mongo:
https://docs.mongodb.com/manual/reference/glossary
->document database designed for ease of development and scaling
->A record in MongoDB is a document, which is a data structure composed of field and value  airs. MongoDB documents are similar to JSON objects. The values of fields may include other  ocuments, arrays, and arrays of documents. such as
->{
            "key1":"value",
            "address":{
                        "street":""
                        "city":""
                        "state":""
                        "zip":""
                        "country":""
            },
            "mobile":["2840284234","2134123123"]
}
->Embedded documents and arrays reduce need for expensive joins. MongoDB provides high performance data persistence
Dynamic schema supports fluent polymorphism.
->Support for embedded data models reduces I/O activity on database system.
->Indexes support faster queries and can include keys from embedded documents and arrays.
->rich query language support for CRUD(create read update delete), data aggregation and geospatial queries
->its highly available nosql db, its replica set utility makes it auto failover and data redundancy
->A replica set is a group of MongoDB servers that maintain the same data set, providing redundancy and increasing data availability.
->its horizontally scalable, sharding process distributes data across cluster of machines
from 3.4 ver onwards, it supports ZONE of datasets based on shard key. In balanced cluster, mongo reads/writes data covered by zone only
->In sharded clusters, you can create zones of sharded data based on the shard key. You can associate each zone with one or more shards in the cluster. A shard can associate with any number of zones. In a balanced cluster, MongoDB migrates chunks covered by a zone only to those shards associated with the zone
->         ->Isolate a specific subset of data on a specific set of shards.
            ->Ensure that the most relevant data reside on shards that are geographically closest to the application servers.
            ->Route data to shards based on the hardware / performance of the shard hardware.
            ->Each zone covers one or more ranges of shard key values for a collection. Each range a zone covers is always inclusive of its lower boundary and exclusive of its upper boundary. (updateZoneKeyRange, sh.updateZoneKeyRange and sh.addTagRange
            ->Starting in MongoDB 4.0.2, dropping a collection deletes its associated zone/tag ranges.
->it supports multiple storage engine including WiredTiger/In-Memory/MMAPv1(deprecated in 4.0)
->it also has various connectors available that we can use to talk to it using programming languages such as python/perl/java/c++/c#/php etc
->The balancer attempts to evenly distribute a sharded collection’s chunks across all shards in the cluster.
->shard key: The field MongoDB uses to distribute documents among members of a sharded cluster
->mongos - sharding rounting process to split data into sharded / clustred nodes
->The shard key determines the distribution (ideal shard key allows mongodb to evenly distributes chunks in shards) of the collection’s documents among the cluster’s shards. The shard key is either an indexed field or indexed compound fields that exists in every document in the collection. MongoDB partitions data in the collection using ranges of shard key values. Each range defines a non-overlapping range of shard key values and is associated with a chunk. Once you shard a collection, the shard key and the shard key values are immutable (cannot be changed later). sh.shardCollection( <database>.<collection, key )
->choosing a shard key: The choice of shard key affects the creation and distribution of the chunks across the available shards. This affects the overall efficiency and performance of operations within the sharded cluster.
->hashed / ranged shard keys:
From the mongo shell connected to the mongos, use the sh.shardCollection() method to shard a collection.
sh.enableSharding("<database>")
you must create a Hashed Indexes on the shard key using the db.collection.createIndex() method before using shardCollection()
sh.shardCollection("<database>.<collection>", { <shard key> : "hashed" } )
Using a hashed shard key to shard a collection results in a more random distribution of data
Hashed indexes use a hashing function to compute the hash of the value of the index field. [1] The hashing function collapses embedded documents and computes the hash for the entire value (MongoDB supports hashed indexes of any single field) but does not support multi-key (i.e. arrays) indexes.
MongoDB automatically computes the hashes when resolving queries using hashed indexes. Applications do not need to compute hashes.
convertShardKeyToHashed()
db.collection.createIndex( { _id: "hashed" } ) //#_id is index key
MongoDB hashed indexes truncate floating point numbers to 64-bit integers before hashing and cannot be larger than 2^53
->You cannot specify a unique constraint on a hashed index
->For a ranged sharded collection, only the (index on shard _id, compound index) indexes can be unique
->The cardinality of a shard key determines the maximum number of chunks the balancer can create. This can reduce or remove the effectiveness of horizontal scaling in the cluster. if cardinality is 4, then mongo cannot have more than 4 chunks of data. this constraints the numbers of effective shards in the cluster of 4 as well. adding additional shards wouldnt help here
-> If X has low cardinality, the distribution of inserts may not be effective and it wouldnt scale as incoming writes would onlt routes to subsets of shards, and high cardinality also wouldnt also gurantee a balanced shard, though it does better facilitate scaling. the frequency and rate of change of shard key also contributes to data distribution
->When possible, use a logical DNS hostname instead of an ip address, particularly when configuring replica set members or sharded cluster members. The use of logical DNS hostnames avoids configuration changes due to ip address changes.
->Starting in MongoDB 3.6, MongoDB binaries, mongod and mongos, bind to localhost by default. If the net.ipv6 configuration file setting or the --ipv6 command line option is set for the binary, the binary additionally binds to the localhost IPv6 address.
->A shard key cannot exceed 512 bytes.
->WiredTiger storage engine is the default storage engine used to create the data files in the --dbpath or storage.dbPath. WiredTiger uses document-level concurrency control for write operations. As a result, multiple clients can modify different documents of a collection at the same time.  WiredTiger uses optimistic concurrency control uses intent locks at global, db and collection level. when 2 writes conflict, mongo makes one success and retries the other
->Some global operations, typically short lived operations involving multiple databases, still require a global “instance-wide” lock. Some other operations, such as dropping a collection, still require an exclusive database lock.
->WiredTiger uses MultiVersion Concurrency Control (MVCC). At the start of an operation, WiredTiger provides a point-in-time snapshot of the data to the operation. A snapshot presents a consistent view of the in-memory data.
->mongo db checkpoints snapsshots of data every 60 seconds or 2GB (older than 3.6) whichever occurs earlier
->Starting in MongoDB 4.0, you cannot specify --nojournal option or storage.journal.enabled: false for replica set members that use the WiredTiger storage engine.
->Journaling is use to recover the data from snapshots.
->Journaling: WiredTiger uses a write-ahead log (i.e. journal) in combination with checkpoints to ensure data durability. its uses snappy compression tool. use the storage.wiredTiger.engineConfig.journalCompressor setting
->To adjust the size of the WiredTiger internal cache, see storage.wiredTiger.engineConfig.cacheSizeGB and --wiredTigerCacheSizeGB.
->A shard key index can be an ascending index on the shard key, a compound index that start with the shard key and specify ascending order for the shard key, or a hashed index.
->A shard key index cannot be an index that specifies a multikey index, a text index or a geospatial index on the shard key fields.
->Shard Key is Immutable
If you must change a shard key:
            Dump all data from MongoDB into an external format.
            Drop the original sharded collection.
            Configure sharding using the new shard key.
            Pre-split the shard key range to ensure initial even distribution.
            Restore the dumped data into MongoDB.
->For clusters with high insert volumes, a shard keys with monotonically increasing and decreasing keys can affect insert throughput. If your shard key is the _id field, be aware that the default values of the _id fields are ObjectIds which have generally increasing values
->When inserting documents with monotonically increasing shard keys, all inserts belong to the same chunk on a single shard and this way migration to distribute data more evenly
->intent lock: A lock on a resource that indicates that the holder of the lock will read (intent shared) or write (intent exclusive) the resource using concurrency control at a finer granularity than that of the resource with the intent lock. Intent locks allow concurrent readers and writers of a resource
->
cmd=1
host = db.serverStatus().host;
prompt=function() {return db+" @ "+host+"->"+(cmd++)+": "}
show collections->db.getCollectionNames()
db.myCollection.find().pretty()
db.serverStatus().uptime
db.stats().objects
db.dropDatabase()
DBQuery.shellBatchSize = 10;
db.collection.help(), db.collection.find().help()\
db.collection.save
db.collection.find().toArray
show users->db.getUsers()
cursor = db.collection.find();
while ( cursor.hasNext() ) {
   printjson( cursor.next() );
}
mongo localhost:27017/test myjsfile.js
load("scripts/myjstest.js")
db.collection.insertOne()        Insert a new document into the collection.
db.collection.insertMany()      Insert multiple new documents into the collection.
db.collection.updateOne()      Update a single existing document in the collection.
db.collection.updateMany()    Update multiple existing documents in the collection.
db.collection.save()    Insert either a new document or update an existing document in the collection.
db.collection.deleteOne()       Delete a single document from the collection.
db.collection.deleteMany()     Delete documents from the collection.
db.collection.drop()     Drops or removes completely the collection.
db.collection.createIndex()     Create a new index on the collection if the index does not exist; otherwise, the operation has no effect.
db.getSiblingDB()        Return a reference to another database using this same connection without explicitly switching the current database. This allows for cross database queries.

The best indexes for your application must take a number of factors into account, including the kinds of queries you expect, the ratio of reads to writes, and the amount of free memory on your system.

When developing your indexing strategy you should have a deep understanding of your application’s queries. Before you build indexes, map out the types of queries you will run so that you can build indexes that reference those fields. Indexes come with a performance cost, but are more than worth the cost for frequent queries on large data sets. Consider the relative frequency of each query in the application and whether the query justifies an index.
Generally, MongoDB only uses one index to fulfill most queries. However, each clause of an $or query may use a different index, and starting in 2.6, MongoDB can use an intersection of multiple indexes.
to Support Your Queries, Sort Query Results, Ensure Indexes Fit in RAM, Create Queries that Ensure Selectivity

Explain what is MongoDB?
            Mongo-DB is a document database which provides high performance, high availability and easy scalability
What is “Namespace” in MongoDB?
            MongoDB stores BSON (Binary Interchange and Structure Object Notation) objects in the collection. The concatenation of the collection name and database name is called a namespace.
What is sharding in MongoDB?
            The procedure of storing data records across multiple machines is referred as Sharding. It is a MongoDB approach to meet the demands of data growth. It is the horizontal partition of data in a database or search engine. Each partition is referred as shard or database shard.
How can you see the connection used by Mongos?
            To see the connection used by Mongos use db_adminCommand (“connPoolStats”)
Explain what is a replica set?
            A replica set is a group of mongo instances that host the same data set. In replica set, one node is primary, and another is secondary. From primary to the secondary node all data replicates.
How replication works in MongoDB?
            Across multiple servers, the process of synchronizing data is known as replication. It provides redundancy and increase data availability with multiple copies of data on different database server. Replication helps in protecting the database from the loss of a single server.
While creating Schema in MongoDB what are the points need to be taken in consideration?
            Design your schema according to user requirements
            Combine objects into one document if you use them together. Otherwise, separate them
            Do joins while write, and not when it is on read
            For most frequent use cases optimize your schema
            Do complex aggregation in the schema
What is the syntax to create a collection and to drop a collection in MongoDB?
            Syntax to create collection in MongoDB is db.createCollection(name,options)
            Syntax to drop collection in MongoDB is db.collection.drop()
Explain what is the role of profiler in MongoDB?
            MongoDB database profiler shows performance characteristics of each operation against the database. You can find queries using the profiler that are slower than they should be
Explain can you move old files in the moveChunk directory?
            Yes, it is possible to move old files in the moveChunk directory, during normal shard balancing operations these files are made as backups and can be deleted once the operations are done.
To do safe backups what is the feature in MongoDB that you can use?
            Journaling is the feature in MongoDB that you can use to do safe backups.
Mention what is Objecld composed of? 12 bytes of BSON type consists of 4bytes Timestamp, 3bytes Client machine ID, 2 byte Client process ID,3 byte incremented counter
Mention what is the command syntax for inserting a document?
            database.collection.insert (document)
Mention how you can inspect the source code of a function?
            invoke function without parenthesis
What is the command syntax that tells you whether you are on the master server or not? And how many master does MongoDB allow?
            Db.isMaster(), only 1 master
Explain what are indexes in MongoDB?
            Indexes are special structures in MongoDB, which stores a small portion of the data set in an easy to traverse form. Ordered by the value of the field specified in the index, the index stores the value of a specific field or set of field
Mention what is the basic syntax to use index in MongoDB?
            The basic syntax to use in MongoDB is >db.COLLECTION_NAME.ensureIndex ( {KEY:1} ). In here the key is the the name of the COLUMN (or KEY:VALUE pair) which is present in the documents
Explain what is GridFS in MongoDB?
            For storing and retrieving large files such as images, video files and audio files GridFS is used. By default, it uses two files fs.files and fs.chunks to store the file’s metadata and the chunks.
Compare SQL databases and MongoDB at a high level.
            SQL databases store data in form of tables, rows, columns and records. This data is stored in a pre-defined data model which is not very much flexible for today's real-world highly growing applications. MongoDB in contrast uses a flexible structure which can be easily modified and extended.
Does MongoDB support foreign key constraints?
            No. MongoDB does not support such relationships.
Does MongoDB support ACID transaction management and locking functionalities?
            No. MongoDB does not support default multi-document ACID transactions. However, MongoDB provides atomic operation on a single document.
How can you achieve primary key - foreign key relationships in MongoDB?
            By default MongoDB does not support such primary key - foreign key relationships. However, we can achieve this concept by embedding one document inside another. Foe e.g. an address document can be embedded inside customer document.
Does MongoDB need a lot of RAM?
            No. MongoDB can be run even on a small amount of RAM. MongoDB dynamically allocates and de-allocates RAM based on the requirements of other processes.
Does MongoDB pushes the writes to disk immediately or lazily?
            MongoDB pushes the data to disk lazily. It updates the immediately written to the journal but writing the data from journal to disk happens lazily.
If you remove a document from database, does MongoDB remove it from disk?
            Yes. Removing a document from database removes it from disk too.
Mention the command to insert a document in a database called school and collection called persons.
            use school;
            db.persons.insert( { name: "kadhir", dept: "CSE" } )
How many indexes does MongoDB create by default for a new collection?
            By default, MongoDB created the _id collection for every collection
Can you create an index on an array field in MongoDB? If yes, what happens in this case?
            Yes. An array field can be indexed in MongoDB. In this case, MongoDB would index each value of the array.
What is a covered query in MongoDB?
            A covered query is the one in which:
            fields used in the query are part of an index used in the query, and
            the fields returned in the results are in the same index
Why is a covered query important?
            Since all the fields are covered in the index itself, MongoDB can match the query condition as well as return the result fields using the same index without looking inside the documents. Since indexes are stored in RAM or sequentially located on disk, such access is a lot faster.
What happens if an index does not fit into RAM?
            If the indexes do not fit into RAM, MongoDB reads data from disk which is relatively very much slower than reading from RAM.
Mention the command to list all the indexes on a particular collection.
            db.collection.getIndexes()
At what interval does MongoDB write updates to the disk?
            By default configuration, MongoDB writes updates to the disk every 60 seconds. However, this is configurable with the commitIntervalMs and syncPeriodSecs options.
How can you achieve transaction and locking in MongoDB?
            use the nesting of documents, also called embedded documents. MongoDB supports atomic operations within a single document
What is Aggregation in MongoDB?
            Aggregations operations process data records and return computed results. Aggregation operations group values from multiple documents together, and can perform a variety of operations on the grouped data to return a single result. MongoDB provides three ways to perform aggregation: the aggregation pipeline, the map-reduce function, and single purpose aggregation methods and commands.
What are Primary and Secondary Replica sets?
            Primary and master nodes are the nodes that can accept writes. MongoDB's replication is 'single-master:' only one node can accept write operations at a time.
            Secondary and slave nodes are read-only nodes that replicate from the primary.
By default, MongoDB writes and reads data from both primary and secondary replica sets. True or False.
            False. MongoDB writes data only to the primary replica set.
Why are MongoDB data files large in size?
            MongoDB preallocates data files to reserve space and avoid file system fragmentation when you setup the server.
When should we embed one document within another in MongoDB?
            You should consider embedding documents for:
            'contains' relationships between entities
            One-to-many relationships
            Performance reasons
Why MongoDB is not preferred over a 32-bit system?
            When running a 32-bit build of MongoDB, the total storage size for the server, including data and indexes, is 2 gigabytes. For this reason, do not deploy MongoDB to production on 32-bit machines.
            If you're running a 64-bit build of MongoDB, there's virtually no limit to storage size.
What is a Storage Engine in MongoDB
            A storage engine is the part of a database that is responsible for managing how data is stored on disk. For example, one storage engine might offer better performance for read-heavy workloads, and another might support a higher-throughput for write operations.
Which are the two storage engines used by MongoDB?
            MongoDB uses MMAPv1 and WiredTiger.
What is the role of a profiler in MongoDB? Where does the writes all the data?
            The database profiler collects fine grained data about MongoDB write operations, cursors, database commands on a running mongod instance. You can enable profiling on a per-database or per-instance basis.
            The database profiler writes all the data it collects to the system.profile collection, which is a capped collection.
How does Journaling work in MongoDB?
            When running with journaling, MongoDB stores and applies write operations in memory and in the on-disk journal before the changes are present in the data files on disk. Writes to the journal are atomic, ensuring the consistency of the on-disk journal files. With journaling enabled, MongoDB creates a journal subdirectory within the directory defined by dbPath, which is /data/db by default.
Can you configure the cache size for MMAPv1? How?
            No. MMAPv1 does not allow configuring the cache size
Can you configure the cache size for WiredTiger? How?
            For the WiredTiger storage engine, you can specify the maximum size of the cache that WiredTiger will use for all data. This can be done using storage.wiredTiger.engineConfig.cacheSizeGB option.
How does MongoDB provide concurrency?
            MongoDB uses reader-writer locks that allow concurrent readers shared access to a resource, such as a database or collection, but give exclusive access to a single write operation.
How can you isolate your cursors from intervening with the write operations?
            You can use the snapshot() method on a cursor to isolate the operation for a very specific case. snapshot() traverses the index on the _id field and guarantees that the query will return each document no more than once.
Can one MongoDB operation lock more than one databases? If yes, how?
            Yes. Operations like copyDatabase(), repairDatabase(), etc. can lock more than onne databases involved.
Which command can be used to provide various information on the query plans used by a MongoDB query?
            The explain() command can be used for this information. The possible modes are: 'queryPlanner', 'executionStats', and 'allPlansExecution'.


AWS:
https://www.aws.training/Certification
https://www.certmetrics.com/amazon/
https://wsr.pearsonvue.com/testtaker/registration/Dashboard.htm?conversationId=1715272
study: https://aws.amazon.com/certification/certified-solutions-architect-associate/
https://aws.amazon.com/certification/certification-prep/
https://docs.aws.amazon.com/
services:
languages support on aws: c++,go, java, python, ruby, node, dotnet, javascript, php
EC2: Elastic Compute Cloud:  resizeable computing capacity
Batch: run batch computing workload, need large amount of computing resources, does all heavy lifting of resources
ECR: Elastic Computer Registry, docker containers in aws
EKS: Elastic Kubernetes Service runs kubernets clustrs
Beanstalk: quickly build and deploys apps on cloud without need to worry about infra, ou simply upload your application, and AWS Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring
lambda: you can run code without even deploying to a particular server, you write small functions and aws manages running code and resouces it uses. pay only for compute time your function runs.your lambda code can interact with other aws resources or call from any other language, web, mobile apis, providing greaty flexibility to manage code and not anything else
lightsail: Amazon Lightsail is the easiest way to get started with AWS for developers who just need virtual private servers. Lightsail includes everything you need to launch your project quickly – a virtual machine, SSD-based storage, data transfer, DNS management, and a static IP – for a low, predictable price.
ParallelCluster: tool that helps you to deploy and manage high performance computing (HPC) clusters
SAM: Serverless Application Modal, open-source framework that enables you to build serverless applications on AWS. It provides you with a template specification to define your serverless application, and a command line interface (CLI) tool.
serverless application repository: repository for serverless applications. It enables teams, organizations, and individual developers to find, deploy, publish, share, store, and easily assemble serverless architectures
S3 Simple Storage Service: Amazon Simple Storage Service (Amazon S3) is storage for the internet. You can use Amazon S3 to store and retrieve any amount of data at any time, from anywhere on the web. You can accomplish these tasks using the simple and intuitive web interface of the AWS Management Console.
EFS Elastic File System: Amazon EFS provides file storage for your Amazon EC2 instances. With Amazon EFS, you can create a file system, mount the file system on your EC2 instances, and then read and write data from your EC2 instances to and from your file system.
FSx: Amazon FSx provides fully managed third-party file systems with the native compatibility and feature sets for workloads such as Microsoft Windows–based storage, high-performance computing, machine learning, and electronic design automation. Amazon FSx supports two file system types: Lustre and Windows File Server.
Glacier: Amazon Simple Storage Service Glacier (Amazon S3 Glacier) is a storage service optimized for infrequently used data, or "cold data." The service provides durable and extremely low-cost storage with security features for data archiving and backup. With Amazon S3 Glacier, you can store your data cost effectively for months, years, or even decades. Amazon S3 Glacier enables you to offload the administrative burdens of operating and scaling storage to AWS, so you don't have to worry about capacity planning, hardware provisioning, data replication, hardware failure detection and recovery, or time-consuming hardware migrations.
snowball: AWS Snowball is a service for customers who want to transport terabytes or petabytes of data to and from AWS, or who want to access the storage and compute power of the AWS Cloud locally and cost effectively in places where connecting to the internet might not be an option
Storage Gateway: AWS Storage Gateway is a service that connects an on-premises software appliance with cloud-based storage to provide seamless and secure integration between your on-premises IT environment and the AWS storage infrastructure in the cloud.
RDS Relational Database Service: Amazon Relational Database Service (Amazon RDS) is a web service that makes it easier to set up, operate, and scale a relational database in the cloud. It provides cost-efficient, resizable capacity for an industry-standard relational database and manages common database administration tasks.
DocumentDB: Amazon DocumentDB (with MongoDB compatibility) is a fast, reliable, and fully managed database service that makes it easy for you to set up, operate, and scale MongoDB-compatible databases
DynamoDB: Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. You can use Amazon DynamoDB to create a database table that can store and retrieve any amount of data, and serve any level of request traffic. Amazon DynamoDB automatically spreads the data and traffic for the table over a sufficient number of servers to handle the request capacity specified by the customer and the amount of data stored, while maintaining consistent and fast performance
ElastiCache: Amazon ElastiCache makes it easy to set up, manage, and scale distributed in-memory cache environments in the AWS Cloud. It provides a high performance, resizable, and cost-effective in-memory cache, while removing complexity associated with deploying and managing a distributed cache environment. ElastiCache works with both the Redis and Memcached engines; to see which works best for you,
Neptune: fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Neptune is a purpose-built, high-performance graph database engine that is optimized for storing billions of relationships and querying the graph with milliseconds latency. Neptune supports the popular graph query languages Apache TinkerPop Gremlin and W3C’s SPARQL, allowing you to build queries that efficiently navigate highly connected datasets. Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security
Redshift: is a fast, fully managed, petabyte-scale data warehouse service that makes it simple and cost-effective to efficiently analyze all your data using your existing business intelligence tools. It is optimized for datasets ranging from a few hundred gigabytes to a petabyte or more and costs less than $1,000 per terabyte per year, a tenth the cost of most traditional data warehousing solutions.
cloud9: IDE on cloud
CodeBuild: aws fully managed build service for your code, compiles, runs tests, and produces artifacts which are ready to deploy
codeCommit: git like repo on cloud, managed by aws
codeDeploy:  automate the deployment of applications to instances and to update the applications as required.
CodePipeline: continuous delivery service that enables you to model, visualize, and automate the steps required to release your software.
CodeStar: CodeStar lets you quickly develop, build, and deploy applications on AWS
xray: to analyze the behavior of their distributed applications by providing request tracing, exception collection, and profiling capabilities
IAM: Identity Access Management: web service for securely controlling access to AWS services. With IAM, you can centrally manage users, security credentials such as access keys, and permissions that control which AWS resources users and applications can access.
Artifact: web service that enables you to download AWS security and compliance documents such as ISO certifications and SOC reports
congnito: service that you can use to create unique identities for your users, authenticate these identities with identity providers, and save mobile user data in the AWS Cloud.
Firewall Manager:  simplifies your AWS WAF administration and maintenance tasks across multiple accounts and resources. With AWS Firewall Manager, you set up your firewall rules just once. The service automatically applies your rules across your accounts and resources, even as you add new resources
Cloud Directory: is a cloud-native directory that can store hundreds of millions of application-specific objects with multiple relationships and schemas. Use Cloud Directory when you need a cloud-scale directory to share and control access to hierarchical data between your applications. With Cloud Directory, you can organize application data into multiple hierarchies to support many organizational pivots and relationships across directory information
GuardDuty: continuous security monitoring service. Amazon GuardDuty can help to identify unexpected and potentially unauthorized or malicious activity in your AWS environment
Inspector: Inspector is a security vulnerability assessment service that helps improve the security and compliance of your AWS resources. Amazon Inspector automatically assesses resources for vulnerabilities or deviations from best practices, and then produces a detailed list of security findings prioritized by level of severity. Amazon Inspector includes a knowledge base of hundreds of rules mapped to common security standards and vulnerability definitions that are regularly updated by AWS security researchers
Macie: security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. Amazon Macie recognizes sensitive data such as personally identifiable information (PII) or intellectual property, and provides you with dashboards and alerts that give visibility into how this data is being accessed or moved
RAM: Resource Access Manager enables you to share your resources with any AWS account or organization in AWS Organizations. Customers who operate multiple accounts can create resources centrally and use AWS RAM to share them with all of their accounts to reduce operational overhead. AWS RAM is available at no additional charge
Secrets Manager helps you to securely encrypt, store, and retrieve credentials for your databases and other services. Instead of hardcoding credentials in your apps, you can make calls to Secrets Manager to retrieve your credentials whenever needed. Secrets Manager helps you protect access to your IT resources and data by enabling you to rotate and manage access to your secrets
Security Hub provides you with a comprehensive view of the security state of your AWS resources. Security Hub collects security data from across AWS accounts and services, and helps you analyze your security trends to identify and prioritize the security issues across your AWS environment
Shield: AWS Shield Standard is automatically included at no extra cost beyond what you already pay for AWS WAF and your other AWS services. For added protection against DDoS attacks, AWS offers AWS Shield Advanced. AWS Shield Advanced provides expanded DDoS attack protection for your Amazon EC2 instances, Elastic Load Balancing load balancers, Amazon CloudFront distributions, and Amazon Route 53 hosted zones
SSO-Single Sign On: cloud-based service that simplifies managing SSO access to AWS accounts and business applications. You can control SSO access and user permissions across all your AWS accounts in AWS Organizations. You can also administer access to popular business applications and custom applications that support Security Assertion Markup Language (SAML) 2.0. In addition, AWS SSO offers a user portal where your users can find all their assigned AWS accounts, business applications, and custom applications in one place.
WAF: Web Application Firewall that lets you monitor web requests that are forwarded to Amazon CloudFront distributions or an Application Load Balancer. You can also use AWS WAF to block or allow requests based on conditions that you specify, such as the IP addresses that requests originate from or values in the requests
CloudHSM offers secure cryptographic key storage for customers by providing managed hardware security modules in the AWS Cloud.
AWS Key Management Service (KMS) is an encryption and key management service scaled for the cloud. KMS keys and functionality are used by other AWS services, and you can use them to protect data in your own applications that use AWS
Crypto Tools libraries are designed to help everyone do cryptography right, even without special expertise. Our client-side encryption libraries help you to protect your sensitive data at its source using secure cryptographic algorithms, envelope encryption, and signing
AWS Certificate Manager (ACM) makes it easy to provision, manage, and deploy SSL/TLS certificates on AWS managed resources.
Auto Scaling, you can quickly set up dynamic and predictive scaling for your scalable AWS resources. It uses Amazon EC2 Auto Scaling to scale your EC2 instances and Application Auto Scaling to scale resources from other services
CloudFormation: enables you to create and provision AWS infrastructure deployments predictably and repeatedly. It helps you leverage AWS products such as Amazon EC2, Amazon Elastic Block Store, Amazon SNS, Elastic Load Balancing, and Auto Scaling to build highly reliable, highly scalable, cost-effective applications in the cloud without worrying about creating and configuring the underlying AWS infrastructure. AWS CloudFormation enables you to use a template file to create and delete a collection of resources together as a single unit (a stack)
CloudTrail: you can monitor your AWS deployments in the cloud by getting a history of AWS API calls for your account, including API calls made via the AWS Management Console, the AWS SDKs, the command line tools, and higher-level AWS services. You can also identify which users and accounts called AWS APIs for services that support CloudTrail, the source IP address the calls were made from, and when the calls occurred. You can integrate CloudTrail into applications using the API, automate trail creation for your organization, check the status of your trails, and control how administrators turn CloudTrail logging on and off
CloudWatch: reliable, scalable, and flexible monitoring solution that you can start using within minutes. You no longer need to set up, manage, and scale your own monitoring systems and infrastructure
Control Tower: service that enables you to enforce and manage governance rules for security, operations, and compliance at scale across all your organizations and accounts in the AWS Cloud
OpsWorks: provides a simple and flexible way to create and manage stacks and applications. With AWS OpsWorks, you can provision AWS resources, manage their configuration, deploy applications to those resources, and monitor their health
Service Catalog: enables IT administrators to create, manage, and distribute portfolios of approved products to end users, who can then access the products they need in a personalized portal. Typical products include servers, databases, websites, or applications that are deployed using AWS resources (for example, an Amazon EC2 instance or an Amazon RDS database). You can control which users have access to specific products to enforce compliance with organizational business standards, manage product lifecycles, and help users find and launch products with confidence
Systems Manager: to organize, monitor, and automate management tasks on your AWS resources
API Gateway enables you to create and deploy your own REST and WebSocket APIs at any scale. You can create robust, secure, and scalable APIs that access AWS or other web services, as well as data that’s stored in the AWS Cloud. You can create APIs to use in your own client applications, or you can make your APIs available to third-party app developers
App Mesh makes it easy to monitor and control microservices running on AWS
Cloud Map lets you name and discover your cloud resources.
CloudFront, aws CDN speeds up distribution of your static and dynamic web content, such as .html, .css, .php, image, and media files. When users request your content, CloudFront delivers it through a worldwide network of edge locations that provide low latency and high performance
Direct Connect links your internal network to an AWS Direct Connect location over a standard 1 gigabit or 10 gigabit Ethernet fiber-optic cable. One end of the cable is connected to your router, the other to an AWS Direct Connect router. With this connection in place, you can create virtual interfaces directly to the AWS cloud and Amazon Virtual Private Cloud, bypassing Internet service providers in your network path
Elastic Load Balancing automatically distributes your incoming application traffic across multiple targets, such as EC2 instances. It monitors the health of registered targets and routes traffic only to the healthy targets. Elastic Load Balancing supports three types of load balancers: Application Load Balancers, Network Load Balancers, and Classic Load Balancers
Global Accelerator is a network layer service in which you create accelerators to improve availability and performance for internet applications used by a global audience.
Route 53 is a highly available and scalable Domain Name System (DNS) web service
Amazon Virtual Private Cloud (Amazon VPC) enables you to launch Amazon Web Services (AWS) resources into a virtual network that you've defined. This virtual network closely resembles a traditional network that you'd operate in your own data center, with the benefits of using the scalable infrastructure of AWS
AWS Virtual Private Network (AWS VPN) lets you establish a secure and private tunnel from your network or device to the AWS Cloud. You can extend your existing on-premises network into a VPC, or connect to other AWS resources from a client. AWS VPN offers two types of private connectivity that feature the high availability and robust security necessary for your data.
Amazon Elastic Transcoder lets you convert media files that you have stored in Amazon S3 into media files in the formats required by consumer playback devices. For example, you can convert large, high-quality digital media files into formats that users can play back on mobile devices, tablets, web browsers, and connected televisions.
AWS Elemental MediaConnect is a reliable, secure, and flexible transport service for live video. Using AWS Elemental MediaConnect, broadcasters and content owners can cost-effectively send high-value live content into the cloud, securely transmit it to partners for distribution, and replicate it to multiple destinations around the globe.
AWS Elemental MediaConvert is a service that formats and compresses offline video content for delivery to televisions or connected devices. High-quality video transcoding makes it possible to create on-demand video assets for virtually any device
AWS Elemental MediaLive is a video service that allows easy and reliable creation of live outputs for broadcast and streaming delivery
AWS Elemental MediaPackage is a just-in-time video packaging and origination service that delivers highly secure, scalable, and reliable video streams to a wide variety of playback devices. AWS Elemental MediaPackage enriches audience experience with live, video on demand (VOD), live-to-VOD, and catch-up TV features.
AWS Elemental MediaStore is a video origination and storage service that offers the high performance, predictable low latency, and immediate consistency required for live origination. With AWS Elemental MediaStore, you can manage video assets as objects in containers to build dependable, cloud-based media workflows
AWS Elemental MediaTailor is a personalization and monetization service that allows scalable server-side ad insertion. The service enables you to serve targeted ads to viewers while maintaining broadcast quality in over-the-top (OTT) video applications. The service also enables you to track ad views for accurate ad reporting.
AWS Amplify enables developers to develop and deploy cloud-powered mobile and web apps. The Amplify Framework is a comprehensive set of SDKs, libraries, tools, and documentation for client app development. The Amplify Console provides a continuous delivery and hosting service for web applications
AWS AppSync is an enterprise level, fully managed GraphQL service with real-time data synchronization and offline programming features
AWS Device Farm is an app testing service that enables you to test your iOS, Android and Fire OS apps on real, physical phones and tablets that are hosted by AWS. The service allows you to upload your own tests or use built-in, script-free compatibility tests.
AWS SDK for Unity / .NET and Xamarin: .net & unity game ver > 4.0 
Pinpoint: helps you engage your customers by sending them email, SMS and voice messages, and push notifications. You can use Amazon Pinpoint to send targeted messages (such as promotions and retention campaigns), as well as transactional messages (such as order confirmations and password reset messages).
AWS Application Discovery Service helps systems integrators quickly and reliably plan application migration projects by automatically identifying applications running in on-premises data centers, their associated dependencies, and their performance profile.
AWS Database Migration Service is a web service you can use to migrate data from your database that is on-premises, on an Amazon Relational Database Service (Amazon RDS) DB instance, or in a database on an Amazon Elastic Compute Cloud (Amazon EC2) instance to a database on an AWS service. These services can include a database on Amazon RDS or a database on an Amazon EC2 instance. You can also migrate a database from an AWS service to an on-premises database. You can migrate data between heterogeneous or homogenous database engines
AWS DataSync is a data-transfer service that simplifies, automates, and accelerates moving and replicating data between on-premises storage systems and AWS storage services over the internet or AWS Direct Connect
AWS Migration Hub provides a single location to track migration tasks across multiple AWS tools and partner solutions. With Migration Hub, you can choose the AWS and partner migration tools that best fit your needs while providing visibility into the status of your migration projects. Migration Hub also provides key metrics and progress information for individual applications, regardless of which tools are used to migrate them.
The AWS Schema Conversion Tool makes heterogeneous database migrations easy by automatically converting the source database schema and a majority of the custom code to a format compatible with the target database. The custom code that the tool converts includes views, stored procedures, and functions. Any code that the tool cannot convert automatically is clearly marked so that you can convert it yourself
Federal Information Processing Standards (FIPS)
Federal Risk and Authorization Management Program (FedRAMP)
AWS Snowball is a service for customers who want to transport terabytes or petabytes of data to and from AWS, or who want to access the storage and compute power of the AWS Cloud locally and cost effectively in places where connecting to the internet might not be an option.
AWS Transfer for SFTP is a secure transfer service that stores your data in Amazon S3 and simplifies the migration of Secure File Transfer Protocol (SFTP) workflows to AWS
Amazon WorkSpaces offers an easy way to provide a cloud-based desktop experience to your end users. Select from a choice of bundles that offer a range of different amounts of CPU, memory, storage, and a choice of applications. Users can connect from a PC, Mac desktop computer, iPad, Kindle, or Android tablet.
Amazon AppStream 2.0 is a fully managed, secure application streaming service that lets you stream desktop applications to users without rewriting applications. AppStream 2.0 provides users with instant access to the applications that they need with a responsive, fluid user experience on the device of their choice.
Amazon WorkDocs is a fully managed, secure enterprise storage and sharing service with strong administrative controls and feedback capabilities that improve user productivity.
Amazon WorkLink is a fully managed, cloud-based service that enables secure, one-click access to internal websites and web apps from mobile devices.
Amazon WorkSpaces Application Manager (Amazon WAM) offers a fast, flexible, and secure way for you to deploy and manage applications for Amazon WorkSpaces.
NICE Desktop Cloud Visualization is a remote visualization technology that enables users to securely connect to graphic-intensive 3D applications hosted on a remote, high-performance server. With NICE DCV, you can make a server's high-performance graphics processing capabilities available to multiple remote users by creating secure client sessions.
Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to setup or manage, and you pay only for the queries you run. To get started, simply point to your data in S3, define the schema, and start querying using standard SQL.
Amazon CloudSearch is a fully managed service in the cloud that makes it easy to set up, manage, and scale a search solution for your website. Amazon CloudSearch enables you to search large collections of data such as web pages, document files, forum posts, or product information. With Amazon CloudSearch, you can quickly add search capabilities to your website without having to become a search expert or worry about hardware provisioning, setup, and maintenance. As your volume of data and traffic fluctuates, Amazon CloudSearch automatically scales to meet your needs.
AWS Data Pipeline is a web service that you can use to automate the movement and transformation of data. With AWS Data Pipeline, you can define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks.
Amazon EMR (Elastic Map Reduce) is a web service that makes it easy to process large amounts of data efficiently. Amazon EMR uses Hadoop processing combined with several AWS products to do such tasks as web indexing, data mining, log file analysis, machine learning, scientific simulation, and data warehousing.
AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores. AWS Glue consists of a central data repository known as the AWS Glue Data Catalog, an ETL engine that automatically generates Python code, and a flexible scheduler that handles dependency resolution, job monitoring, and retries. AWS Glue is serverless, so there's no infrastructure to set up or manage. Use the AWS Glue console to discover your data, transform it, and make it available for search and querying. You can also use the AWS Glue API operations to interface with AWS Glue
Amazon Kinesis makes it easy to collect, process, and analyze video and data streams in real time
Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managed service that makes it easy for you to build and run applications that use Apache Kafka to process streaming data.
Amazon QuickSight is a fast business analytics service to build visualizations, perform ad hoc analysis, and quickly get business insights from your data. Amazon QuickSight seamlessly discovers AWS data sources, enables organizations to scale to hundreds of thousands of users, and delivers fast and responsive query performance by using a robust in-memory engine (SPICE).
Amazon Redshift is a fast, fully managed, petabyte-scale data warehouse service that makes it simple and cost-effective to efficiently analyze all your data using your existing business intelligence tools. It is optimized for datasets ranging from a few hundred gigabytes to a petabyte or more and costs less than $1,000 per terabyte per year, a tenth the cost of most traditional data warehousing solutions.
Amazon EventBridge is a serverless event bus service that makes it easy to connect your applications with data from a variety of sources. EventBridge delivers a stream of real-time data from your own applications, software-as-a-service (SaaS) applications, and AWS services and routes that data to targets such as AWS Lambda. You can set up routing rules to determine where to send your data to build application architectures that react in real time to all of your data sources. EventBridge enables you to build event-driven architectures that are loosely coupled and distributed.
Amazon MQ - similar to RBS Argon, is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Amazon MQ provides interoperability with your existing applications and services. Amazon MQ works with your existing applications and services without the need to manage, operate, or maintain your own messaging system.
Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS moves data between distributed application components and helps you decouple these components.
AWS Step Functions makes it easy to coordinate the components of distributed applications as a series of steps in a visual workflow. You can quickly build and run state machines to execute the steps of your application in a reliable and scalable fashion.
Amazon Simple Workflow Service (Amazon SWF) makes it easy to build applications that coordinate work across distributed components. In Amazon SWF, a task represents a logical unit of work that is performed by a component of your application. Coordinating tasks across the application involves managing intertask dependencies, scheduling, and concurrency in accordance with the logical flow of the application. Amazon SWF gives you full control over implementing tasks and coordinating them without worrying about underlying complexities such as tracking their progress and maintaining their state.
Alexa for Business gives you the tools you need to manage Alexa devices, enroll users, and assign skills. You can build your own voice skills using the Alexa Skills Kit and the Alexa for Business API. You can also make them available as private skills for your organization.
Amazon Chime is a secure, real-time, unified communications service that transforms meetings by making them more efficient and easier to conduct.
Amazon WorkMail is a managed email and calendaring service that offers strong security controls and support for existing desktop and mobile clients.
AWS Ground Station is a fully managed service that enables you to control satellite communications, process satellite data, and scale your satellite operations. With AWS Ground Station, you don't have to build or manage your own ground station infrastructure.
AWS RoboMaker is a service that makes it easy to develop, simulate, and deploy intelligent robotics applications at scale.
Amazon Managed Blockchain is a fully managed service that makes it easy to create and manage scalable blockchain networks using popular open source frameworks. Currently, Managed Blockchain supports the Hyperledger Fabric open source framework.
Amazon GameLift is a fully managed service for deploying, operating, and scaling session-based multiplayer game servers in the cloud. Built on the AWS computing environment, GameLift lets you scale high performance game servers up and down to meet player demand.
Lumberyard is a free AAA game engine deeply integrated with AWS and Twitch—with full source. Lumberyard provides a growing set of tools to help you create the highest quality games, engage massive communities of fans, and connect games to the vast compute and storage of the cloud. Participate in the forums or learn about new changes on our blog.
AWS IoT enables secure, bi-directional communication between Internet-connected things (such as sensors, actuators, embedded devices, or smart appliances) and the AWS Cloud over MQTT and HTTP.
Amazon FreeRTOS is an operating system that makes microcontroller-based edge devices easy to program, deploy, secure, and maintain. Amazon FreeRTOS extends the FreeRTOS kernel, a popular open source operating system for microcontrollers, with software libraries that can be used to securely connect devices (locally and to the cloud) and update them remotely.
AWS IoT Analytics provides advanced data analysis for AWS IoT. You can collect large amounts of device data, process messages, and store them. You can then query the data and run sophisticated analytics to make accurate decisions in your IoT applications and machine learning use cases.
AWS IoT Device Defender is an AWS IoT security service that allows you to audit the configuration of your devices, monitor your connected devices to detect abnormal behavior, and to mitigate security risks. It gives you the ability to enforce consistent security policies across your AWS IoT device fleet and respond quickly when devices are compromised.
AWS IoT Device Management is a cloud-based device management service that makes it easy for customers to securely manage IoT devices throughout their lifecycle. Customers can use AWS IoT Device Management to onboard device information and configuration, organize their device inventory, monitor their fleet of devices, and remotely manage devices deployed across many locations. This remote management includes over-the-air (OTA) updates to device software.
AWS IoT Events allows you to monitor your equipment or device fleets for failures or changes in operation, and to trigger actions when such events occur. It enables you to build complex equipment monitoring applications on the AWS Cloud, accessible through the AWS IoT Events console or APIs. AWS IoT Events continuously watches IoT sensor data from devices and applications to identify significant events and take action when they occur.
AWS IoT Greengrass seamlessly extends AWS onto physical devices so they can act locally on the data they generate, while still using the cloud for management, analytics, and durable storage. AWS IoT Greengrass ensures your devices can respond quickly to local events and operate with intermittent connectivity. AWS IoT Greengrass minimizes the cost of transmitting data to the cloud by allowing you to author AWS Lambda functions that execute locally.
AWS IoT SiteWise is an AWS IoT service that makes it easy to collect, structure, and search data from industrial equipment at scale.
AWS IoT Things Graph is an integrated set of tools that enable developers to build IoT applications using devices and services that use different data representation standards and communication protocols.
AWS IoT 1-Click is a service that makes it easy for simple devices to trigger AWS Lambda functions that execute a specific action. Some examples of possible actions include calling technical support, reordering goods and services, or locking and unlocking doors and windows. Simple devices are cloud-connected, single-purpose devices such as Wi-Fi and mobile network connected buttons.
Amazon Connect is a contact center as a service (CCaS) solution that offers easy, self-service configuration and enables dynamic, personal, and natural customer engagement at any scale.
Amazon Simple Email Service (Amazon SES) is an email sending and receiving service that provides an easy, cost-effective way for you to send email.
Amazon Sumerian is a set of tools for creating high-quality virtual reality (VR) experiences on the web. With Sumerian, you can construct an interactive 3D scene without any programming experience, test it in the browser, and publish it as a website that is immediately available to users.
Amazon Silk is a next-generation web browser available only on Kindle Fire. Built on a split architecture that divides processing between the client and the Amazon cloud, Silk is designed to create a faster, more responsive mobile browsing experience.


Qs
->What is an AWS region?: A region is a geographical area that consists of different availability zones. Each region at least two Availability Zones
->What does an AWS Region consist of?: An independent collection of AWS computing resources in a defined geography
->Which statement best describes Availability Zones: Distinct locations from within an AWS region that are engineered to be isolated from failures.
->An AWS VPC is a component of which AWS service: Networking Service.
->What is a VPC: Virtual Private Cloud
->Which AWS service is specifically designed to run a developer's code on an infrastructure that is automatically provisioned to host that code?: Elastic Beanstalk
->Which AWS service allows you to run code without having to worry about provisioning any underlying resources (such as virtual machines, databases, etc.): lambda
->Amazon's highly scaleable DNS service is known as: Route53
->Which AWS compute service is specifically designed to assist you in processing large data sets?: Elastic Map Reduce
->What is the difference between Elastic Beanstalk & CloudFormation?: Elastic Beanstalk automatically handles the deployment from capacity provisioning load balancing auto-scaling to application health monitoring based on the code you upload to it where as CloudFormation is an automated provisioning engine designed to deploy entire cloud environments via a JSON script.
->Which AWS service is used a CDN to distribute content around the world?: cloud front
->Which of the following AWS solutions offers durable, available storage for flat files?: S3
->Which of the following AWS services would be the best choice for long term data archival?: Glacier
->Which AWS service offers the following database engines: SQL, MySQL, MariaDB, PostgreSQL, Aurora, and Oracle?: RDS (Relation DB Service)
->Which of the following AWS services is used primarily for data warehousing?: Redshift
->Which AWS service is used for collating large amounts of data streamed from multiple sources?: kinesis
->You need to add users to your AWS account and set password rotation policies for these new users. Which AWS service would best fit your requirements?: IAM (Identity Access Management)
->You need to supply auditors with logs detailing the individual users that provision specific resources on your AWS platform. Which service would best meet this need?: CloudTail
->You need a configuration management service that enables your system administrators to configure and operate your web applications using Chef. Which AWS service would best suit your needs?:OpsWorks
->Your digital media agency needs to convert their media files in to different formats to suit different devices. Which AWS service should you consider using to meet these needs?: Elastic Transcoder
->Which statement best describes IAM: IAM allows you to manage users groups roles and their corresponding level of access to the AWS Platform
->Which of the following is not a feature of IAM?: Allows you to setup biometric authentication so that no passwords are required
->Power User Access allows: Access to all AWS services except for management of groups and users within IAM
->What level of access does the "root" account have?: Administrator Access
->You are a solutions architect working for a large engineering company who are moving their existing legacy hardware to AWS. You have configured their first AWS account and you have set up IAM. Your company will be primarily based out of West Germany, however they will have a small subsidiary operating out of South Korea and you will need an AWS environment configured there as well. Which of the following statements is true: You will need to configure Users and Policy Documents only once as these are applied globally
->You have a client who is considering moving to AWS services and do not yet have an account. What is the first thing the company should do to set up an AWS Account?: Set up an account using their company email address.
->You are a security administrator working for a hotel chain. You have a new member of staff who has started as a systems administrator and they will need full access to the AWS console. You have created the user account and generated the access key id and the secret access key. You have moved this user into the group where the other administrators are and you have provided the new user with their secret access key and their access key id. However when they go to log in to the AWS console, they cannot sign in. What could be the cause of this?: You cannot log in to the AWS console using the Access Key ID and Secret Access Key instead you must generate a password for the user and supply the user with this password as well as the unique link to sign in to the AWS console.
->What is an additional way to secure IAM for both the root login and new users alike?: Implement multi-factor Authentication for all accounts.
->By default when you create a new user in the IAM console, what level of access do they have?: No access to all AWS services
->In what language are policy documents written in?: JSON
->S3 has what consistency model for PUTS of new objects: Read After Write Consistency
->What is AWS Storage Gateway?: It's an on-premise virtual appliance that can be used to cache S3 locally at a customers site.
->One of your users is trying to upload a 7.5GB file to S3 however they keep getting the following error message - "Your proposed upload exceeds the maximum allowed object size.". What is a possible solution for this? :Design your application to use the multi-part upload API for all objects
->What does RRS stand for when talking about S3: Reduced Redundancy Storage
->You have been asked by your company to create an S3 bucket with the name "acloudguru1234" in the EU West region. What would be the URL for this bucket?: https://s3-eu-west-1.amazonaws.com/acloudguru1234
->What is Amazon Glacier?: An AWS service designed for long term data archival.
->What does S3 stand for: Simple Storage Service
->You are a solutions architect who works with a large digital media company. The company has decided that they want to operate within the Japanese region and they need a bucket called "testbucket" set up immediately to test their web application on. You log in to the AWS console and try to create this bucket in the Japanese region however you are told that the bucket name is already taken. What should you do to resolve this?: Bucketnames are global not regional. This is a popular bucket name and is already taken. You should choose another bucket name.
->What is the availability/durability on RRS?: 99.99% / 99.99%
->What is the durability/availability on S3?:  99.999999999% / 99.99%
->What is the minimum file size that I can store on S3?: 1 byte
->The difference between S3 and EBS is that EBS is object based where as S3 is block based.: false (reverse is true)
->S3 has eventual consistency for which HTTP Methods?: overwrite PUTS and DELETES
->You work for a busy digital marketing company who currently store their data on premise. They are looking to migrate to AWS S3 and to store their data in buckets. Each bucket will be named after their individual customers, followed by a random series of letters and numbers. Once written to S3 the data is rarely changed, as it has already been sent to the end customer for them to use as they see fit. However on some occassions, customers may need certain files updated quickly, and this may be for work that has been done months or even years ago. You would need to be able to access this data immediately to make changes in that case, but you must also keep your storage costs extremely low. The data is not easily reproducible if lost. Which S3 storage class should you choose to minimise costs and to maximise retrieval times?: S3-IA
->You need to use an Object based storage solution to store your critical, non replaceable data in a cost effective way. This data will be frequently updated and will need some form of version control enabled on it. Which S3 storage solution should you use? : S3
->You work for a health insurance company who collects large amounts of documents regarding patients health records. This data will be used usually only once when assessing a customer and will then need to be securely stored for a period of 7 years. In some rare cases you may need to retrieve this data within 24 hours of a claim being lodged. Which storage solution would best suit this scenario? You need to keep your costs as low as possible. : Glacier
->You run a meme creation website that frequently generates meme images. The original images are stored in S3 and the meta data about the memes are stored in DynamoDB. You need to store the memes themselves in a low cost storage solution. If an object is lost, you have created a Lambda function that will automatically recreate this meme using the original file in S3 and the metadata in Dynamodb. Which storage solution should you consider to store this non-critical, easily reproducible data on in the most cost effective solution as possible?: S3 RRS
->You run a popular photo sharing website that is based off S3. You generate revenue from your website via paid for adverts, however you have discovered that other websites are linking directly to the images on your site, and not to the HTML pages that serve the content. This means that people are not seeing your adverts and every time a request is made to S3 to serve an image it is costing your business money. How could you resolve this issue?: Remove the ability for images to be served publicly to the site and then used signed URL's with expiry dates.
->EBS Snapshots are backed up to S3 in what manner?: Incrementally
->Do Amazon EBS volumes persist independently from the life of an Amazon EC2 instance, for example, if I terminated an EC2 instance, would that EBS volume remain? : Only if instructed to when created
->Can I delete a snapshot of an EBS Volume that is used as the root device of a registered AMI?: No
->A placement group can be deployed across multiple Availability Zones.:False
->While creating the snapshots using the command line tools, which command should I be using?:  ec2-create-snapshot
->Can you attach an EBS volume to more than one EC2 instance at the same time?
->A placement group is ideal for: EC2 instances that require high network throughput and low latency across a single availability zone.
->Using the console, I can add a role to an EC2 instance, after that instance has been created and powered up: False
->I can change the permissions to a role, even if that role is already assigned to an existing EC2 instance, and these changes will take effect immediately: True
->Does Route 53 support MX Records?: yes
->Route53 is named so because: The DNS Port is on Port 53 and Route53 is a DNS Service
->Route53 does not support zone apex records (or naked domain names): Incorrect
->Route53 is Amazon's DNS Service: true
->There is a limit to the number of domain names that you can manage using Route 53: True and False. There is a limit of 50 domain names however this limit can be raised by contacting AWS support
->What AWS DB platform is most suitable for OLTP?: RDS/DynamoDB
->When replicating data from your primary RDS instance to your secondary RDS instance, what is the charge?: No charge
->What AWS service is best suited for non relational databases?: DynamoDB
->When you add a rule to an RDS security group you do not need to specify a port number or protocol?: False
->If you are using Amazon RDS Provisioned IOPS storage with MySQL and Oracle database engines what is the maximum size RDS volume you can have by default?: 6TB
->What happens to the I/O operations while you take a database snapshot: IO operation are suspended for the duration of snapshot
->What AWS service is best used for Business Intelligence Tools/Data Warehousing?: Redshift
->In RDS when using multiple availability zones, can you use the secondary database as an independent read node?: No
->Amazon's Elasticache uses which two engines?: Redis & Memcached
->By default, the maximum provisioned IOPS capacity on an Oracle and MySQL RDS instance (using provisioned IOPS) is 30,000 IOPS: True
->Security groups act like a firewall at the instance level whereas ___ are an additional layer of security that act at the subnet level: Network ACLs
->How many VPC's am I allowed in each AWS Region by default: 10
->VPC stands for: Virtual Private Cloud
->How many internet gateways can I attach to my custom VPC: 1
->You have a VPC with both public and private subnets. You have 3 EC2 instances that have been deployed in to the public subnet and each has internet access. You deploy a 4th instance using the same AMI and this instance does not have internet access. What could be the cause of this?: The instance needs either an Elastic IP address/Public IP address assigned to it.
->SWF: Simple Work Flow
->SES: Simple Email Service
->What happens when you create a topic on Amazon SNS: An Amazon Resource Name is created
->What is the difference between SNS and SQS?: SNS is push notification service where as SQS is message system that requires worker nodes to poll the queue.
->What application service allows you to decouple your infrastructure using messaged based queues?: SQS
->What does a "domain" refer to in Amazon SWF: A collection of related workflows
->By default, EC2 instances pull SQS messages from an SQS queue on a FIFO (First In First out) basis.: False as order is not guranteed but all will be delivered is the gurantee
->Amazon's SQS service guarantees a message will be delivered at least once.: true
->Amazon SWF ensures that a task is assigned only once and is never duplicated: true, its master slave design pattern
->Amazon SWF restrict me to use specific programming languages: false
->Amazon SWF is designed to help users: Coordinate synchronous and asynchronous tasks
->In RDS, what is the maximum value I can set for my backup retention period?: 35 days
->Automated backups are enabled by default for a new DB Instance?: true
->Amazon RDS does not currently support increasing storage on a ____ Db instance: SQL Server
->In what circumstances would I choose provisoned IOPS in RDS over standard storage?: If you use production online transaction processing
->Amazon's S3 is: Object Based Storage
->In S3 with RRS the AVAILABILITY is: 99.99%
->Amazon's EBS volumes are: Block Based
->If I want to run a database/OS on an EC2 instance, which is the most recommended Amazon storage option: EBS
->In S3 the durability of my files is: 99.999999999%
->Can you access Amazon EBS Snapshots?: Yes through the AWS APIs/CLI & AWS Console
->A __________ is a document that provides a formal statement of one or more permissions.: Policy
->In a default VPC, all Amazon EC2 instances are assigned 2 IP addresses at launch, what are these?: Private IP Address & Public IP Address
->If an Amazon EBS volume is the root device of an instance, can I detach it without stopping the instance?: No
->If you want your application to check whether a request generated an error then you look for an ______ node in the response from the Amazon RDS API: Error Node
->EC2 instances can have credentials stored on them so that the instances can access other resources (such as S3 buckets) and AWS recommends that you do this instead of assigning roles.: False
->Can I move a reserved instance from one region to another?: No
->In S3 RRS the durability of my files is: 99.99%
->In RDS, changes to the backup window take effect: Immediately
->In RDS what is the maximum size for a Microsoft SQL Server DB Instance with SQL Server Express edition?: 10Gb per Database
->In S3 what does RRS stand for?: Reduced Redundancy Storage
->Can I "force" a failover for any RDS instance that has Multi-AZ configured?: Yes
->What does EBS stand for?: Elastic Block Storage
->You can conduct your own vulnerability scans within your own VPC without alerting AWS first?: False
->Reserved instances are available for multi-AZ deployments.: true
->Amazon's Glacier service is a Content Distribution Network which integrates with S3.: false
->MySQL installations default to port number: 3306
->If an Amazon EBS volume is an additional partition (ie not the root volume) , can I detach it without stopping the instance?: Yes
->Every user you create in the IAM systems starts with _____: No Permission
->You can RDP or SSH in to an RDS instance to see what is going on with the operating system.: No
->When creating a new security group, all in bound traffic is allowed by default.: false
->To save administration headaches, Amazon recommend that you leave all security groups in web facing subnets open on port 22 to 0.0.0.0/0 CIDR, that way you can connect where ever you are in the world.: Incorrect
->What are the four levels of AWS premium support?: Basic,Developer,Business,Enterprise
->As the AWS platform is PCI DSS 1.0 compliant, I can immediately deploy a website to it that can take and store credit card details. I do not need to get any kind of delta accredditaion from a QSA.: False
->To help you manage your Amazon EC2 instances you can assign your own metadata in the form of: Tags
->The service to allow Big Data Processing on the AWS platform is known as AWS "Elastic Big Data".: false
->Individual instances are provisioned in: Availibility Zones
->When using a custom VPC and placing an EC2 instance in to a public subnet, it will be automatically internet accessible (ie you do not need to apply an elastic IP address or ELB to the instance).: false
->What is the underlying Hypervisor for EC2?: Xen
->The AWS platform is certified PCI DSS 1.0 compliant: True
->The AWS platform consists of how many regions currently?: 14
->How many copies of my data does RDS - Aurora store by default?: 6
->Amazon's product debut conference is held in Las Vegas each year and is known as: Re-Invent
->Amazon's product debut conference is held in Las Vegas each year and is known as: false
->What is the maximum response time for a Business Level Premium Support Case?: 1hr
-> When I create a new security group, all outbound traffic is allowed by default.: true
->What types of RDS databases are currently available?: Oracle, SQL, MySQL, Postgres
->I can enable multifactor authentication by using: IAM
->When deploying databases on your own EC2 instances, it is recommended that you deploy these on magnetic storage rather than SSD storage as you get better performance.: false
->Auditing user access/API calls etc across the entire AWS estate can be achieved by using: cloudtail
->You are a solutions architect working for a company that specialises in ingesting large data feeds (using Kinesis) and then analysing these feeds using Elastic Map Reduce (EMR). The results are then stored on a custom MySQL database which is hosted on an EC2 instance which has 3 volumes, the root/boot volume, and then 2 additional volumes which are striped in to a RAID 1. Your company recently had an outage and lost some key data and have since decided that they will need to run nightly back ups. Your application is only used during office hours, so you can afford to have some down time in the middle of the night if required. You decide to take a snapshot of all three volumes every 24 hours. In what manner should you do this?: Stop the EC2 instance and take a snapshot of each EC2 instance independently. Once the snapshots are complete start the EC2 instance and ensure that all relevant volumes are remounted.
->What are the valid methodologies for encrypting data on S3?: Server Side Encryption (SSE)-S3, SSE-C, SSE-KMS or a client library such as Amazon S3 Encryption Client
->In Identity and Access Management, when you first create a new user, certain security credentials are automatically generated. Which of the below are valid security credentials?: Access Key Id and Secret Access Key
->Amazon Web Services offer 3 different levels of support, which of the below are valid support levels.: Enterprise, Business and Developer
->Amazon Web Services offer 3 different levels of support, which of the below are valid support levels.: Generate a password for each user created and give these passwords to your system administrators.
->Amazon S3 buckets in all Regions provide which of the following?: Read-after-write consistency for PUTS of new objects AND Eventually consistent for overwrite PUTS & DELETES
->What function of an AWS VPC is stateless?: Network ACLs (Access Control List)
->Which of the following services allows you root access (ie you can login using SSH)?: Elastic Map Reduce 
->When trying to grant an amazon account access to S3 using access control lists what method of identification should you use to identify that account with?: email address or canonical user id
->You are a solutions architect working for a large oil and gas company. Your company runs their production environment on AWS and has a custom VPC. The VPC contains 3 subnets, 1 of which is public and the other 2 are private. Inside the public subnet is a fleet of EC2 instances which are the result of an autoscaling group. All EC2 instances are in the same security group. Your company has created a new custom application which connects to mobile devices using a custom port. This application has been rolled out to production and you need to open this port globally to the internet. What steps should you take to do this, and how quickly will the change occur?: Open the port on the existing security group. Your EC2 instances will be able to communicate over this port immediately.
->Which of the following is not supported by AWS Import/Export?: Export to Amazon Glacier
->Which of the following is not a service of the security category of the AWS trusted advisor service?: vulnerability scans on existing VPCs
->You work for a market analysis firm who are designing a new environment. They will ingest large amounts of market data via Kinesis and then analyse this data using Elastic Map Reduce. The data is then imported in to a high performance NoSQL Cassandra database which will run on EC2 and then be accessed by traders from around the world. The database volume itself will sit on 2 EBS volumes that will be grouped into a RAID 0 volume. They are expecting very high demand during peak times, with an IOPS performance level of approximately 15,000. Which EBS volume should you recommend?: provisioned IOPS
->What are the different types of virtualization available on EC2?: Para-Virtual (PV) & Hardware Virtual Machine (HVM)
->Which of the following is not a valid configuration type for AWS Storage gateway.: Gateway-accessed volumes
->You have started a new role as a solutions architect for an architectural firm that designs large sky scrapers in the Middle East. Your company hosts large volumes of data and has about 250Tb of data on internal servers. They have decided to store this data on S3 due to the redundancy offered by it. The company currently has a telecoms line of 2Mbps connecting their head office to the internet. What method should they use to import this data on to S3 in the fastest manner possible.: AWS Import/Export
->You are designing a site for a new start up which generates cartoon images for people automatically. Customers will log on to the site, upload an image which is stored in S3. The application then passes a job to AWS SQS and a fleet of EC2 instances poll the queue to receive new processing jobs. These EC2 instances will then turn the picture in to a cartoon and will then need to store the processed job somewhere. Users will typically download the image once (immediately), and then never download the image again. What is the most commercially feasible method to store the processed images?: Store the images on S3 RRS and create a lifecycle policy to delete the image after 24 hours.
->You are hosting a website in Ireland called aloud.guru and you decide to have a static DR site available on S3 in the event that your primary site would go down. Your bucket name is also called “acloudguru”. What would be the S3 URL of the static website?: https://acloudguru.s3-website-eu-west-1.amazonaws.com
->Which of the following is NOT a valid SNS subscribers?: SWF
->You are appointed as your company's Chief Security Officer and you want to be able to tack all changes made to your AWS environment, by all users and at all times, in all regions. What AWS service should you use to achieve this?: Cloudtail
->you are building a system to distribute confidential training videos to employees. Using cloudfront, what ,ethod could be used to serve content that is stored in S3, but not publically accessible from S3 directly: create an Origin Access Identity (OAI) for cloudfront and grant access to the objects in your S3 bucket to that OAI
->A company is storing an access key (access key id & secret access key) in a text file on a custom AMI. The company uses the access key to access dynamo db tables from instances created from the AMI. the security team has mandated a more secure solution. which it can be be: create an IAM role with permissions to  access the table and launch instances with new role. This is the most secured way. Accessing key directly / indirectly is never secure and we should never choose that option. as the Role way of operating, we also get tokens associated which get changed frequently and hence is secure from accidental leakages of keys also
->a company is developing a highly available web application using stateless web servers. which services are suitable for storing session state data (select 2): Dynamo DB and Elastic Cache (storage gateway is used for hybrid cloud solutions, hence its not right fit)
->Comapnay salespeople upload their sales figures daily. A solutions Architect needs a durable storage solution for these documents that also protects users accidentally deleting important documents: Store data in an S3 bucket and enable versioning
->An application requires a highly available relational database with an inital storage capacity of 8TB. The database will grow by 8 GB per day. To support expected traffic, at least 8 read replicas will be required to handle database reads. which option will meet it: Amazon RDBMS servcie which Aurora from the options
Amazon RDS: Aurora, preview-parallel query, oracle, mysql, mariaDB, PostgreSQL, MS Sql Server
->A solution architect is designing a critical business application with a relationsal database that runs on an EC2 instance. it rquires a single EBS volume that can support upto 16000 IOPS. which EBS volume best supports this.: A provisioned IOPS SSD would support this which throughput is not guranteed, it has max 32000 IOPS. EBS General purpose SSD also has max 20000 IOPS but IOPS is not guranteed and throughput is, hence its not best option.
->A web application allows customers to upload orders to an S3 bucket. The resulting Amazon S3 events trigger a Lambda function that inserts a message to an SQS queue. A single EC2 instance needs messages from the queue, processes them and stores them in an dynamodb table partioned bu unique order id. next month traffic is expected to increase by a factor of 10 and a solution architect is reviewing the architecture for possible scaling problems. which component of below needs rearchitecting to be able to accomodate new traffic: lamda/sqs/ec2/dynamodb: EC2 instances need rearchitecting as its a single point of failure. if it goes down, all would go down. make this part of auto scaling group across min2 AZs. the responses will then be managed by AWS itself.
->An applications aves logs to an S3 bucket. A user wants to keep the logs for one month and then purge it: Confgure lifecycle events/rules to S3 bucket
->You have a high performance compute application and you need to minimize network latency between EC2 instances as much as possible. What can you do to achieve this:
->You have a high performance compute application and you need to minimize network latency between EC2 instances as much as possible. What can you do to achieve this: Create a placement group within an Availability Zone and place the EC2 instances within that placement group.
->True or False? Amazon S3 buckets in all Regions provide read-after-write consistency for PUTS of new objects and eventual consistency for overwrite PUTS and DELETES. : true
->Placement Groups can be created across 2 or more Availability Zones.: false
->You can add multiple volumes to an EC2 instance and then create your own RAID 5/RAID 10/RAID 0 configurations using those volumes.: true
->You are creating your own relational database on an EC2 instance and you need to maximise IOPS performance. What can you do to achieve this goal?: Add multiple additional volumes with provisioned IOPS and then create a RAID 0 stripe across those volumes
->Which of the services below do you get root access to?: EC2 and Elastic Map Reduce
->Using SAML (Security Assertion Markup Language 2.0) you can give your federated users single sign-on (SSO) access to the AWS Management Console.: true
->You can have 1 subnet stretched across multiple availability zones.: false
->When you create new subnets within a custom VPC, by default they can communicate with each other, across availability zones.: true
->It is possible to transfer a reserved instance from one Availability Zone to another.: true
->You have an EC2 instance which needs to find out both its private IP address and its public IP address. To do this you need to: Retrieve the instance Metadata from http://169.254.169.254/latest/meta-data/
->To retrieve instance metadata or userdata you will need to use the following IP Address:  http://169.254.169.254
->Amazon S3 buckets in all regions provide read-after-write consistency for PUTS of new objects.: true
->Amazon S3 provides: Unlimited Storage
->In order to enable encryption at rest using EC2 and Elastic Block Store you need to: Configure encryption when creating the EBS volume
->You can select a specific Availability Zone in which to place your DynamoDB Table: false
->When creating an RDS instance you can select which availability zone in which to deploy your instance.: true
->Amazon's Redshift uses which block size for its columnar storage?: 1024KB / 1MB
->You run a website which hosts videos and you have two types of members, premium fee paying members and free members. All videos uploaded by both your premium members and free members are processed by a fleet of EC2 instances which will poll SQS as videos are uploaded. However you need to ensure that your premium fee paying members videos have a higher priority than your free members. How do you design SQS?: : Create two SQS queues one for premium members and one for free members. Program your EC2 fleet to poll the premium queue first and if empty to then poll your free members SQS queue.
->You have uploaded a file to S3. What HTTP code would indicate that the upload was successful?: 200
->You are hosting a MySQL database on the root volume of an EC2 instance. The database is using a large amount of IOPs and you need to increase the IOPs available to it. What should you do?: Add 4 additional EBS SSD volumes and create a RAID 10 using these volumes.
->You have been asked to create VPC for your company. The VPC must support both Internet-facing web applications (ie they need to be publicly accessible) and internal private applications (i.e. they are not publicly accessible and can be accessed only over VPN). The internal private applications must be inside a private subnet. Both the internet-facing and private applications must be able to leverage at least three Availability Zones for high availability. At a minimum, how many subnets must you create within your VPC to achieve this?: 6 (2->prviate/public in each AZ)

Chapter1
1. Which of the following describes a physical location around the world where AWS clusters data centers? D: region
2. Each AWS region is compose d of two or more locations that offer organizations the ability to operate production systems that are more highly available, fault tolerant, and scalable than would be possible using a single data center. What are these locations called? A: Availability Zones
3. What is the deployment term for an environment that extends an existing on-premises infrastructure into the cloud to connect cloud resources to internal systems? B. Hybrid deployment
4. Which AWS Cloud service allows organizations to gain system-wide visibility into resource utilization, application performance, and operational health? C. Amazon CloudWatch
5. Which of the following AWS Cloud services is a fully managed NoSQL database service? B. Amazon DynamoDB
6. Your company experiences fluctuations in traffic patterns to their e-commerce website based on flash sales. What service can help your company dynamically match the required compute capacity to the spike in traffic during flash sales? A. Auto Scaling
7. Your company provides an online photo sharing service. The development team is looking for ways to deliver image files with the lowest latency to end users so the websitecontent is delivered with the best possible performance. What service can help speed updistribution of these image files to end users around the world? D. Amazon CloudFront
8. Your company runs an Amazon Elastic Compute Cloud (Amazon EC2) instanceperiodically to perform a batch processing job on a large and growing filesystem. At the end of the batch job, you shut down the Amazon EC2 instance to save money but need to persist the filesystem on the Amazon EC2 instance from the previous batch runs. What AWS Cloud service can you leverage to meet these requirements? A. Amazon Elastic Block Store (Amazon EBS)
9. What AWS Cloud service provides a logically isolated section of the AWS Cloud where organizations can launch AWS resources in a virtual network that they define? C. Amazon Virtual Private Cloud (Amazon VPC)
10. Your company provides a mobile voting application for a popular TV show, and 5 to 25 million viewers all vote in a 15-second timespan. What mechanism can you use to decouple the voting application from your back-end services that tally the votes? B. Amazon Simple Queue Service (Amazon SQS)

Chapter2
1.In what ways does Amazon Simple Storage Service (Amazon S3) object storage differ from block and file storage? (Choose 2 answers) D, E. Objects are stored in buckets, and objects contain both data and metadata.
2. Which of the following are not appropriates use cases for Amazon Simple Storage Service (Amazon S3)? (Choose 2 answers) B/D: Storing a file system mounted to an Amazon Elastic Compute Cloud (Amazon EC2) instance / Primary storage for a database
3. What are some of the key characteristics of Amazon Simple Storage Service (AmazonS3)? (Choose 3 answers)A/B/C: All objects have a URL/ Amazon S3 can store unlimited amounts of data./ Amazon S3 uses a REST (Representational State Transfer) Application Program Interface (API).
4. Which features can be used to restrict access to Amazon Simple Storage Service (AmazonS3) data? (Choose 3 answers) B/C/E: Create a pre-signed URL for an object. / Use an Amazon S3 Access Control List (ACL) on a bucket or object / Use an Amazon S3 bucket policy
5. which must be protected against inadvertent or intentional deletion. How can this data be protected? (Choose 2 answers) E/C: Enable MFA Delete on the bucket / Enable versioning on the bucket
6. Your company stores documents in Amazon Simple Storage Service (Amazon S3), but it wants to minimize cost. Most documents are used actively for only about a month, then much less frequently. However, all data needs to be available within minutes when requested. How can you meet these requirements?: C: Migrate the data to Amazon S3 Standard – Infrequent Access (IA) after 30 days.
7. How is data stored in Amazon Simple Storage Service (Amazon S3) for high durability?: B. Data is automatically replicated within a region.
8. Based on the following Amazon Simple Storage Service (Amazon S3) URL, which one of the following statements is correct? https://bucket1.abc.com.s3.amazonaws.com/folderx/myfile.doc: C. The object “folderx/myfile.doc” is stored in the bucket “bucket1.abc.com.”
9. To have a record of who accessed your Amazon Simple Storage Service (Amazon S3) data and from where, you should do what? C Enable server access logs on the bucket.
10. What are some reasons to enable cross-region replication on an Amazon Simple Storage Service (Amazon S3) bucket? (Choose 2 answers)  B / C. You have a set of users or customers who can access the second bucket with lower latency. / For compliance reasons, you need to store data in a location at least 300 miles away from the first region
11. Your company requires that all data sent to external storage be encrypted before being sent. Which Amazon Simple Storage Service (Amazon S3) encryption solution will meet this requirement? C. If data must be encrypted before being sent to Amazon S3, client-side encryption must be used.
12. You have a popular web application that accesses data stored in an Amazon Simple Storage Service (Amazon S3) bucket. You expect the access to be very read-intensive, with expected request rates of up to 500 GETs per second from many clients. How can you increase the performance and scalability of Amazon S3 in this case?: B. Amazon S3 scales automatically, but for request rates over 100 GETS per second, it helps to make sure there is some randomness in the key space. Replication and logging will not affect performance or scalability. Using sequential key names could have a negative effect on performance or scalability.
13. What is needed before you can enable cross-region replication on an Amazon Simple Storage Service (Amazon S3) bucket? (Choose 2 answers): A, D. You must enable versioning before you can enable cross-region replication, and Amazon S3 must have IAM permissions to perform the replication. Lifecycle rules migrate data from one storage class to another, not from one bucket to another. Static website hosting is not a prerequisite for replication.
14. Your company has 100TB of financial records that need to be stored for seven years by law. Experience has shown that any record more than one-year old is unlikely to be accessed. Which of the following storage plans meets these needs in the most cost efficient manner? B. Store the data on Amazon Simple Storage Service (Amazon S3) with lifecycle policies that change the storage class to Amazon Glacier after one year and delete the object after seven years.
15. Amazon Simple Storage Service (S3) bucket policies can restrict access to an Amazon S3 bucket and objects by which of the following? (Choose 3 answers) B, C, E. Amazon S3 bucket policies cannot specify a company name or a country or origin, but they can specify request IP range, AWS account, and a prefix for objects that can be accessed.
16. Amazon Simple Storage Service (Amazon S3) is an eventually consistent storage system. For what kinds of operations is it possible to get stale data as a result of eventual consistency? (Choose 2 answers): B/C GET or LIST after a DELETE / GET after overwrite PUT
17. What must be done to host a static website in an Amazon Simple Storage Service (Amazon S3) bucket? (Choose 3 answers): A, B, D. A, B, and D are required, and normally you also set a friendly CNAME to the bucket URL. Amazon S3 does not support FTP transfers, and HTTP does not need to be enabled.
18. You have valuable media files hosted on AWS and want them to be served only to authenticated users of your web application. You are concerned that your content could be stolen and distributed for free. How can you protect your content?: B. Pre-signed URLs allow you to grant time-limited permission to download objects from an Amazon Simple Storage Service (Amazon S3) bucket. Static web hosting generally requires world-read access to all content. AWS IAM policies do not know who the authenticated users of the web app are. Logging can help track content loss, but not prevent it.
19. Amazon Glacier is well-suited to data that is which of the following? (Choose 2 answers) A/C. Is infrequently or rarely accessed / Is available after a three- to five-hour restore period
20. Which statements about Amazon Glacier are true? (Choose 3 answers): C, D, E. Amazon Glacier stores data in archives, which are contained in vaults. Archives are identified by system-created archive IDs, not key names.


Chapter3
EC2-Classic Security Groups: Control outgoing instance traffic
VPC Security Groups: Control outgoing and incoming instance traffi
CIDR(Classless Inter-Domain Routing) block—An x.x.x.x/x style definition that defines a specific range of IP addresses

1. Your web application needs four instances to support steady traffic nearly all of the time. On the last day of each month, the traffic triples. What is a cost-effective way to handle this traffic pattern?:  Run four Reserved Instances constantly, then add eight On-Demand Instances on the last day of each month. C. Reserved Instances provide cost savings when you can commit to running instances full time, such as to handle the base traffic. On-Demand Instances provide the flexibility to handle traffic spikes, such as on the last day of the month.
2. Your order-processing application processes orders extracted from a queue with two Reserved Instances processing 10 orders/minute. If an order fails during processing, then it is returned to the queue without penalty. Due to a weekend sale, the queues have several hundred orders backed up. While the backup is not catastrophic, you would like to drain it so that customers get their confirmation emails faster. What is a cost-effective
way to drain the queue for orders?: B Deploy additional Spot Instances to assist in processing the orders
3. Which of the following must be specified when launching a new Amazon Elastic Compute Cloud (Amazon EC2) Windows instance? (Choose 2 answers): C/D: Amazon EC2 instance type / Amazon Machine Image (AMI)
4. You have purchased an m3.xlarge Linux Reserved instance in us-east-1a. In which ways can you modify this reservation? (Choose 2 answers): A/C: Change it into two m3.large instances / Move it to us-east-1b
5. Your instance is associated with two security groups. The first allows Remote Desktop Protocol (RDP) access over port 3389 from Classless Inter-Domain Routing (CIDR) block 72.14.0.0/16. The second allows HTTP access over port 80 from CIDR block: D RDP traffic over port 3389 from 72.14.0.0/16 and HTTP traffic over port 80 from 0.0.00/0
6. Which of the following are features of enhanced networking? (Choose 3 answers): A/B/E: More Packets Per Second (PPS) / Lower latency / Less jitter
7 . You are creating a High-Performance Computing (HPC) cluster and need very low latency and high bandwidth between instances. What combination of the following will allow this? (Choose 3 answers): A/B/D: Use an instance type with 10 Gbps network performance / Put the instances in a placement group. / Enable enhanced networking on the instances
8. Which Amazon Elastic Compute Cloud (Amazon EC2) feature ensures that your instances will not share a physical host with instances from any other AWS customer?: C: Dedicated Instances
9. Which of the following are true of instance stores? (Choose 2 answers): B/C: Data is lost when the instance stops. / Very high IOPS 
10. Which of the following are features of Amazon Elastic Block Store (Amazon EBS)? (Choose 2 answers): A/C: Data stored on Amazon EBS is automatically replicated within an Availability Zone. / Amazon EBS volumes can be encrypted transparently to workloads on the attached
instance
11. You need to take a snapshot of an Amazon Elastic Block Store (Amazon EBS) volume. How long will the volume be unavailable? B: The volume will be available immediately.
12. You are restoring an Amazon Elastic Block Store (Amazon EBS) volume from a snapshot. How long will it be before the data is available? B: The data will be available immediately.
13 You have a workload that requires 15,000 consistent IOPS for data that must be durable. What combination of the following steps do you need? (Choose 2 answers): A/C: Use an Amazon Elastic Block Store (Amazon EBS)-optimized instance. / Use a Provisioned IOPS SSD volume.
14. Which of the following can be accomplished through bootstrapping?:D: Bootstrapping runs the provided script, so anything you can accomplish in a script you can accomplish during bootstrapping.
15. How can you connect to a new Linux instance using SSH?: C: Using the private half of the instance’s key pair The public half of the key pair is stored on the instance, and the private half can then be used to connect via SSH.
16 VM Import/Export can import existing virtual machines as: (Choose 2 answers): B/C: EC2 instances and AMIs
17 Which of the following can be used to address an Amazon Elastic Compute Cloud (Amazon EC2) instance over the web? (Choose 2 answers): B/D: Public DNS name/ Elastic IP address
18 Using the correctly decrypted Administrator password and RDP, you cannot log in to a Windows instance you just launched. Which of the following is a possible reason?: A: There is no security group rule that allows RDP access over port 3389 from your IP
address.
19 You have a workload that requires 1 TB of durable block storage at 1,500 IOPS during normal use. Every night there is an Extract, Transform, Load (ETL) task that requires 3,000 IOPS for 15 minutes. What is the most appropriate volume type for this workload?: C: Use a general-purpose SSD volume.
20 How are you billed for elastic IP addresses?: B: Hourly when they are not associated with an instance

Chapter4
1. What is the minimum size subnet that you can have in an Amazon VPC?: C: /28
2. You are a solutions architect working for a large travel company that is migrating its existing server estate to AWS. You have recommended that they use a custom Amazon VPC, and they have agreed to proceed. They will need a public subnet for their web servers and a private subnet in which to place their databases. They also require that the web servers and database servers be highly available and that there be a minimum of two web servers and two database servers each. How many subnets should you have to maintain high availability?: C: 4
3. 3. Which of the following is an optional security control that can be applied at the subnet layer of a VPC?: A. Network ACL
4. What is the maximum size IP address range that you can have in an Amazon VPC? A: /16
5. You create a new subnet and then add a route to your route table that routes traffic out from that subnet to the Internet using an IGW. What type of subnet have you created?: D: A public subnet
6. What happens when you create a new Amazon VPC?: A: main route table is created by default
7. You create a new VPC in US-East-1 and provision three subnets inside this Amazon VPC. Which of the following statements is true? : C. All subnets will be able to communicate with each other by default.
8. How many IGWs can you attach to an Amazon VPC at any one time?: A: 1
9. What aspect of an Amazon VPC is stateful?: B: Security groups are stateful
10. You have created a custom Amazon VPC with both private and public subnets. You have created a NAT instance and deployed this instance to a public subnet. You have attached an EIP address and added your NAT to the route table. Unfortunately, instances in your private subnet still cannot access the Internet. What may be the cause of this?: C: You should disable source/destination checks on the NAT.
11. Which of the following will occur when an Amazon Elastic Block Store (Amazon EBS)- backed Amazon EC2 instance in an Amazon VPC with an associated EIP is stopped and started? (Choose 2 answers): B/E: All data on instance-store devices will be lost. / The underlying host for the instance is changed.
12. How many VPC Peering connections are required for four VPCs located within the same AWS region to be able to send traffic to each of the others?: D: 6 since it has to 1-1 and birectionall hence 6 connections
13. Which of the following AWS resources would you use in order for an EC2-VPC instance to resolve DNS names outside of AWS?: B. A DHCP option set
14. Which of the following is the Amazon side of an Amazon VPN connection?: D. A VPG is amazon side of vpn. A CGW is the customer side of a VPN connection, and an IGW connects a network to the Internet
15. What is the default limit for the number of Amazon VPCs that a customer may have in a region?: A: 5
16. You are responsible for your company’s AWS resources, and you notice a significant amount of traffic from an IP address in a foreign country in which your company does not have customers. Further investigation of the traffic indicates the source of the traffic is scanning for open ports on your EC2-VPC instances. Which one of the following resources can deny the traffic from reaching the instances?: B: Network ACLs
17. Which of the following is the security protocol supported by Amazon VPC?: D: IPSec is security Protocol
18. Which of the following Amazon VPC resources would you use in order for EC2-VPC instances to send traffic directly to Amazon S3?: D: an amazon VPC endpoint
19. What properties of an Amazon VPC must be specified at the time of creation? (Choose 2 answers): A/C: CIDR block and region
20. Which Amazon VPC feature allows you to create a dual-homed instance?  B. Attaching an ENI associated with a different subnet to an instance can make the instance dual-homed.

Chapter 5
The CLI command that follows will create a launch configuration with the following attributes:
Name: myLC
AMI: ami-0535d66cInstance type: m3.medium
Security groups: sg-f57cde9d
Instance key pair: myKeyPair
> aws autoscaling create-launch-configuration -–launch-configuration-name myLC -- image-id ami-0535d66c --instance-type m3.medium --security-groups sg-f57cde9d -- key-name myKeyPair
> aws autoscaling describe-account-limits
> aws autoscaling create-auto-scaling-group --auto–scaling-group-name myASG -- launch-configuration-name myLC --availability-zones us-east-1a, us-east-1c --min-size 1 --max-size 10 --desired-capacity 3 --load-balancer-names myELB
> aws autoscaling put-scaling-policy --auto-scaling-group-name myASG --policy-name
CPULoadScaleOut --scaling-adjustment 1 --adjustment-type ChangeInCapacity --
cooldown 30 > aws autoscaling put-scaling-policy --auto-scaling-group-name myASG -
-policy-name CPULoadScaleIn --scaling-adjustment -1 --adjustment-type
ChangeInCapacity --cooldown 600
> aws cloudwatch put-metric-alarm --alarm name capacityAdd --metric-name
CPUUtilization --namespace AWS/EC2 --statistic Average –-period 300 --threshold 75
--comparison-operator GreaterThanOrEqualToThreshold --dimensions
"Name=AutoScalingGroupName, Value=myASG" --evaluation-periods 1 --alarm-actions
arn:aws:autoscaling:us-east-1:123456789012:scalingPolicy:12345678-90ab-cdef-
1234567890ab:autoScalingGroupName/myASG:policyName/CPULoadScaleOut --unit Percent
> aws cloudwatch put-metric-alarm --alarm name capacityReduce --metric-name
CPUUtilization --namespace AWS/EC2 --statistic Average --period 1200 --threshold 40
--comparison-operator GreaterThanOrEqualToThreshold --dimensions
"Name=AutoScalingGroupName, Value=myASG" --evaluation-periods 1 --alarm-actions
arn:aws:autoscaling:us-east-1:123456789011:scalingPolicy:11345678-90ab-cdef-
1234567890ab:autoScalingGroupName/myASG:policyName/CPULoadScaleIn --unit Percent

1. Which of the following are required elements of an Auto Scaling group? (Choose 2
answers): A/D: Minimum size / Launch configuration
2. You have created an Elastic Load Balancing load balancer listening on port 80, and you
registered it with a single Amazon Elastic Compute Cloud (Amazon EC2) instance also
listening on port 80. A client makes a request to the load balancer with the correct
protocol and port for the load balancer. In this scenario, how many connections does the
balancer maintain?: B. 2
3. How long does Amazon CloudWatch keep metric data?: D. 2 weeks
4. Which of the following are the minimum required elements to create an Auto Scaling
launch configuration?: A. Launch configuration name, Amazon Machine Image (AMI), and instance type
5. You are responsible for the application logging solution for your company’s existing
applications running on multiple Amazon EC2 instances. Which of the following is the
best approach for aggregating the application logs within AWS? B. Amazon CloudWatch Logs Agent
6. Which of the following must be configured on an Elastic Load Balancing load balancer to
accept incoming traffic?: C. A listener
7. You create an Auto Scaling group in a new region that is configured with a minimum size
value of 10, a maximum size value of 100, and a desired capacity value of 50. However,
you notice that 30 of the Amazon Elastic Compute Cloud (Amazon EC2) instances within
the Auto Scaling group fail to launch. Which of the following is the cause of this
behavior? D. You have not raised your default Amazon EC2 capacity (20) for the new region.
8.You want to host multiple Hypertext Transfer Protocol Secure (HTTPS) websites on a
fleet of Amazon EC2 instances behind an Elastic Load Balancing load balancer with a
single X.509 certificate. How must you configure the Secure Sockets Layer (SSL)
certificate so that clients connecting to the load balancer are not presented with a
warning when they connect? A. Create one SSL certificate with a Subject Alternative Name (SAN) value for each
website name.
9. Your web application front end consists of multiple Amazon Compute Cloud (Amazon
EC2) instances behind an Elastic Load Balancing load balancer. You have configured the
load balancer to perform health checks on these Amazon EC2 instances. If an instance
fails to pass health checks, which statement will be true?: C. The load balancer stops sending traffic to the instance that failed its health check.
10. In the basic monitoring package for Amazon Elastic Compute Cloud (Amazon EC2), what
Amazon CloudWatch metrics are available?: D. Hypervisor visible metrics such as CPU utilization
11. A cell phone company is running dynamic-content television commercials for a contest.
They want their website to handle traffic spikes that come after a commercial airs. The
website is interactive, offering personalized content to each visitor based on location,
purchase history, and the current commercial airing. Which architecture will configure
Auto Scaling to scale out to respond to spikes of demand, while minimizing costs during
quiet periods?: C. Configure Auto Scaling to scale out as traffic increases. Configure the launch
configuration to start new instances from a preconfigured Amazon Machine Image
(AMI).
12. For an application running in the ap-northeast-1 region with three Availability Zones (ap-
northeast-1a, ap-northeast-1b, and ap-northeast-1c), which instance deployment provides
high availability for the application that normally requires nine running Amazon Elastic
Compute Cloud (Amazon EC2) instances but can run on a minimum of 65 percent
capacity while Auto Scaling launches replacement instances in the remaining Availability
Zones?: Deploy the application on three servers in ap-northeast-1a, three servers in ap-
northeast-1b, and three servers in ap-northeast-1c.
13. Which of the following are characteristics of the Auto Scaling service on AWS? (Choose 3 answers): B/E/F: Responds to changing conditions by adding or terminating Amazon Elastic ComputeCloud (Amazon EC2) instances / Launches instances from a specified Amazon Machine Image (AMI) / Enforces a minimum number of running Amazon EC2 instances
14. Why is the launch configuration referenced by the Auto Scaling group instead of being
part of the Auto Scaling group?: D all: A. It allows you to change the Amazon Elastic Compute Cloud (Amazon EC2) instance
type and Amazon Machine Image (AMI) without disrupting the Auto Scaling group.
B. It facilitates rolling out a patch to an existing set of instances managed by an Auto Scaling group.
C. It allows you to change security groups associated with the instances launched without having to make changes to the Auto Scaling group
15. An Auto Scaling group may use: (Choose 2 answers): A/C. On-Demand Instances / Spot Instances
16. Amazon CloudWatch supports which types of monitoring plans? (Choose 2 answers): A/F: Basic monitoring, which is free / Detailed monitoring, which has an additional cost
17. Elastic Load Balancing health checks may be: (Choose 3 answers): A/C/D: A ping / A connection attempt / A page request
18. When an Amazon Elastic Compute Cloud (Amazon EC2) instance registered with an Elastic Load Balancing load balancer using connection draining is deregistered or unhealthy, which of the following will happen? (Choose 2 answers): B/C: Keep the connections open to that instance, and attempt to complete in-flight requests. / Redirect the requests to a user-defined error page like “Oops this is embarrassing” or “Under Construction.”
19. Elastic Load Balancing supports which of the following types of load balancers? (Choose 3 answers): B/E/F: Internet-facing / Internal / Hypertext Transfer Protocol Secure (HTTPS) using Secure Sockets Layer (SSL)
20. Auto Scaling supports which of the following plans for Auto Scaling groups? (Choose 3 answers): B/D/E: Manual / Scheduled / Dynamic

Chapter 6
1. Which of the following methods will allow an application using an AWS SDK to be authenticated as a principal to access AWS Cloud services? (Choose 2 answers): B/C: Create an IAM user and store both parts of the access key for the user in the application’s configuration. / Run the application on an Amazon EC2 instance with an assigned IAM role
2. Which of the following are found in an IAM policy? (Choose 2 answers): A/C: Service Name / Action
3. Your AWS account administrator left your company today. The administrator had access to the root user and a personal IAM administrator account. With these accounts, he generated other IAM accounts and keys. Which of the following should you do today to protect your AWS infrastructure? (Choose 4 answers) A/B/C/E: Change the password and add MFA to the root user / Put an IP restriction on the root user / Rotate keys and change passwords for IAM accounts / Delete the administrator’s personal IAM account.
4. Which of the following actions can be authorized by IAM? (Choose 2 answers): B/D: Launching an Amazon Linux EC2 instance / Adding a message to an Amazon Simple Queue Service (Amazon SQS) queue
5. Which of the following are IAM security features? (Choose 2 answers): A/C: Password policies / MFA (Multi Factor Authentication)
6. Which of the following are benefits of using Amazon EC2 roles? (Choose 2 answers): B/C: Credentials do not need to be stored on the Amazon EC2 instance. / Key rotation is not necessary.
7. Which of the following are based on temporary security tokens? (Choose 2 answers): A/D: Amazon EC2 roles / Federation
8. Your security team is very concerned about the vulnerability of the IAM administrator user accounts (the accounts used to configure all IAM features and accounts). What steps can be taken to lock down these accounts? (Choose 3 answers): A/C/D: Add multi-factor authentication (MFA) to the accounts. / Implement a password policy on the AWS account. / Apply a source IP address condition to the policy that only grants permissions when the user is on the corporate network.
9. You want to grant the individuals on your network team the ability to fully manipulate Amazon EC2 instances. Which of the following accomplish this goal? (Choose 2 answers): B/C: Assign the managed policy, EC2FullAccess, to a group named NetworkTeam, and assign all the team members’ IAM user accounts to that group. / Create a new policy that grants EC2:* actions on all resources, and assign that policy to each individual’s IAM user account on the network team.
10. What is the format of an IAM policy?: C: JSON

Chapter 7
1. Which AWS database service is best suited for traditional Online Transaction Processing
(OLTP)?: B RDS
2. Which AWS database service is best suited for non-relational databases?: D.Dynamo DB
3. You are a solutions architect working for a media company that hosts its website on
AWS. Currently, there is a single Amazon Elastic Compute Cloud (Amazon EC2) Instance
on AWS with MySQL installed locally to that Amazon EC2 Instance. You have been asked
to make the company’s production environment more resilient and to increase
performance. You suggest that the company split out the MySQL database onto an
Amazon RDS Instance with Multi-AZ enabled. This addresses the company’s increased
resiliency requirements. Now you need to suggest how you can increase performance.
Ninety-nine percent of the company’s end users are magazine subscribers who will be
reading additional articles on the website, so only one percent of end users will need to
write data to the site. What should you suggest to increase performance?: C. Recommend that the company use read replicas, and distribute the traffic across
multiple read replicas.
4. Which AWS Cloud service is best suited for Online Analytics Processing (OLAP)?: A. Redshift
5. You have been using Amazon Relational Database Service (Amazon RDS) for the lastyear to run an important application with automated backups enabled. One of your team
members is performing routine maintenance and accidentally drops an important table,
causing an outage. How can you recover the missing data while minimizing the duration
of the outage?: B. Restore the database from a recent automated DB snapshot.
6. Which Amazon Relational Database Service (Amazon RDS) database engines support
Multi-AZ? A: All
7. Which Amazon Relational Database Service (Amazon RDS) database engines support
read replicas? B. MySQL, MariaDB, PostgreSQL, and Aurora
8. Your team is building an order processing system that will span multiple Availability
Zones. During testing, the team wanted to test how the application will react to a
database failover. How can you enable this type of test?: A. Force a Multi-AZ failover from one Availability Zone to another by rebooting the
primary instance using the Amazon RDS console.
9. You are a system administrator whose company has moved its production database to
AWS. Your company monitors its estate using Amazon CloudWatch, which sends alarms
using Amazon Simple Notification Service (Amazon SNS) to your mobile phone. One
night, you get an alert that your primary Amazon Relational Database Service (Amazon
RDS) Instance has gone down. You have Multi-AZ enabled on this instance. What should
you do to ensure the failover happens quickly?: D. No action is necessary. Your connection string points to the database endpoint, and
AWS automatically updates this endpoint to point to your secondary instance.
10. You are working for a small organization without a dedicated database administrator on
staff. You need to install Microsoft SQL Server Enterprise edition quickly to support an
accounting back office application on Amazon Relational Database Service (Amazon
RDS). What should you do?: A. Launch an Amazon RDS DB Instance, and select Microsoft SQL Server Enterprise
Edition under the Bring Your Own License (BYOL) model.
11. You are building the database tier for an enterprise application that gets occasional
activity throughout the day. Which storage type should you select as your default option?: B. General Purpose Solid State Drive (SSD)
12. You are designing an e-commerce web application that will scale to potentially hundreds
of thousands of concurrent users. Which database technology is best suited to hold the
session state for large numbers of concurrent users? : B. NoSQL database table using Amazon DynamoDB
13. Which of the following techniques can you use to help you meet Recovery Point
Objective (RPO) and Recovery Time Objective (RTO) requirements? (Choose 3 answers): A/C/D: DB snapshots / Read replica / Multi-AZ deployment
14. When using Amazon Relational Database Service (Amazon RDS) Multi-AZ, how can you
offload read requests from the primary? (Choose 2 answers): C/D. Add a read replica DB instance, and configure the client’s application logic to use a
read-replica. / Create a caching environment using ElastiCache to cache frequently used data.
Update the application logic to read/write from the cache.
15. You are building a large order processing system and are responsible for securing the
database. Which actions will you take to protect the data? (Choose 3 answers): A/B/C: Adjust AWS Identity and Access Management (IAM) permissions for administrators. / Configure security groups and network Access Control Lists (ACLs) to limit network
access. / Configure database users, and grant permissions to database objects
16. Your team manages a popular website running Amazon Relational Database Service
(Amazon RDS) MySQL back end. The Marketing department has just informed you
about an upcoming television commercial that will drive thousands of new visitors to the
website. How can you prepare your database to handle the load? (Choose 3 answers): A/B/C: Vertically scale the DB Instance by selecting a more powerful instance class. / Create read replicas to offload read requests and update your application. / Upgrade the storage from Magnetic volumes to General Purpose Solid State Drive
(SSD) volumes.
17. You are building a photo management application that maintains metadata on millions
of images in an Amazon DynamoDB table. When a photo is retrieved, you want to display
the metadata next to the image. Which Amazon DynamoDB operation will you use to
retrieve the metadata attributes from the table?: C. Query operation
18. You are creating an Amazon DynamoDB table that will contain messages for a social chat
application. This table will have the following attributes: Username (String), Timestamp
(Number), Message (String). Which attribute should you use as the partition key? Thesort key?: A. Username, Timestamp
19. Which of the following statements about Amazon DynamoDB tables are true? (Choose 2
answers): B/D: Local secondary indexes can only be created when the table is being created. / You can only have one local secondary index.
20. Which of the following workloads are a good fit for running on Amazon Redshift?
(Choose 2 answers) : B/C: Reporting database supporting back-office analytics / Data warehouse used to aggregate multiple disparate data sources

Chapter 8
1. Which of the following is not a supported Amazon Simple Notification Service (Amazon SNS) protocol?: D. Amazon DynamoDB(its a db,not protocolo)
2. When you create a new Amazon Simple Notification Service (Amazon SNS) topic, which of the following is created automatically?: A.ARN (Amazon Resource Name)
3. Which of the following are features of Amazon Simple Notification Service (Amazon SNS)? (Choose 3 answers): A/C/D: Publisher / Subscriber / Topic
4. What is the default time for an Amazon Simple Queue Service (Amazon SQS) visibility timeout?: A: 30 seconds
5. What is the longest time available for an Amazon Simple Queue Service (Amazon SQS) visibility timeout?: D: 12 hours
6. Which of the following options are valid properties of an Amazon Simple Queue Service(Amazon SQS) message? (Choose 2 answers): B, D. The valid properties of an SQS message are Message ID and Body. Each message
7. You are a solutions architect who is working for a mobile application company that wants to use Amazon Simple Workflow Service (Amazon SWF) for their new takeout ordering application. They will have multiple workflows that will need to interact. What should you advise them to do in structuring the design of their Amazon SWF environment?: B: Use a single domain containing multiple workflows. In this manner, the workflows will be able to interact.
8. In Amazon Simple Workflow Service (Amazon SWF), which of the following are actors? (Choose 3 answers): A/B/C: Activity Workers / Workflow Starter / Deciders
9. You are designing a new application, and you need to ensure that the components of your application are not tightly coupled. You are trying to decide between the different AWS Cloud services to use to achieve this goal. Your requirements are that messages between your application components may not be delivered more than once, tasks must be completed in either a synchronous or asynchronous fashion, and there must be some form of application logic that decides what do when tasks have been completed. What application service should you use?: B. Amazon Simple Workflow Service (Amazon SWF)
10. How does Amazon Simple Queue Service (Amazon SQS) deliver messages?: D. Amazon SQS doesn’t guarantee delivery of your messages in any particular order.
11. Of the following options, what is an efficient way to fanout a single Amazon Simple Notification Service (Amazon SNS) message to multiple Amazon Simple Queue Servic (Amazon SQS) queues?: A. Create an Amazon SNS topic using Amazon SNS. Then create and subscribe multiple Amazon SQS queues sent to the Amazon SNS topic.
12. Your application polls an Amazon Simple Queue Service (Amazon SQS) queue frequently and returns immediately, often with empty ReceiveMessageResponses. What is one thing that can be done to reduce Amazon SQS costs?: D. Use long polling by supplying a WaitTimeSeconds of greater than 0 seconds when calling ReceiveMessage.
13. What is the longest time available for an Amazon Simple Queue Service (Amazon SQS) long polling timeout?: B. 20 seconds.
14. What is the longest configurable message retention period for Amazon Simple Queue Service (Amazon SQS)?: D. 14 days
15. What is the default message retention period for Amazon Simple Queue Service (Amazon SQS)?: B. 4 days
16. Amazon Simple Notification Service (Amazon SNS) is a push notification service that lets you send individual or multiple messages to large numbers of recipients. What types of clients are supported?: D. Publisher and subscriber client types
17. In Amazon Simple Workflow Service (Amazon SWF), a decider is responsible for what?: Defining work coordination logic by specifying work sequencing, timing, and failure conditions
18. Can an Amazon Simple Notification Service (Amazon SNS) topic be recreated with a previously used topic name?: C. Yes. The topic name should typically be available after 30–60 seconds after the previous topic with the same name has been deleted.
19. What should you do in order to grant a different AWS account permission to your Amazon Simple Queue Service (Amazon SQS) queue?: C. Create an Amazon SQS policy that grants the other account access
20. Can an Amazon Simple Notification Service (Amazon SNS) message be deleted after being published to a topic?: C. No. After a message has been successfully published to a topic, it cannot be recalled.

Chapter 9
1. Which type of record is commonly used to route traffic to an IPv6 address?: C. AAA record
2. Where do you register a domain name?: B. With a domain registrar
3. You have an application that for legal reasons must be hosted in the United States when U.S. citizens access it. The application must be hosted in the European Union when citizens of the EU access it. For all other citizens of the world, the application must be hosted in Sydney. Which routing policy should you choose in order to achieve this?: C. Geolocation Routing
4. Which type of DNS record should you use to resolve an IP address to a domain name?: D. A PTR record
5. You host a web application across multiple AWS regions in the world, and you need to configure your DNS so that your end users will get the fastest network performance possible. Which routing policy should you apply?: B. Latency Based
6. Which DNS record should you use to configure the transmission of email to your intended mail server?: C. MX (Mail Exchange Record)
7. Which DNS records are commonly used to stop email spoofing and spam?: B. SPF records
8. You are rolling out A and B test versions of a web application to see which version results in the most sales. You need 10 percent of your traffic to go to version A, 10 percent to go to version B, and the rest to go to your current production version. Which routing policy should you choose to achieve this?: B. Weighted routing
9. Which DNS record must all zones have by default?: D. SOA Records
10. Your company has its primary production site in Western Europe and its DR site in the Asia Pacific. You need to configure DNS so that if your primary site becomes unavailable, you can fail DNS over to the secondary site. Which DNS routing policy would best achieve this?: D. failover rounting
11. Which type of DNS record should you use to resolve a domain name to another domain name?: B. CName
12. Which is a function that Amazon Route 53 does not perform?: C. Load balancing
13. Which DNS record can be used to store human-readable information about a server, network, and other accounting data with a host?: A. Txt record
14. Which resource record set would not be allowed for the hosted zone example.com?: C. www.example.ca
15. Which port number is used to serve requests by DNS?: B. 53
16. Which protocol is primarily used by DNS to serve requests?: D. User Datagram Protocol (UDP)
17. Which protocol is used by DNS when response data size exceeds 512 bytes?: A. TCP (Transmission)
18. What are the different hosted zones that can be created in Amazon Route 53?: B. 1 & 3: Public hosted zone / Private Hosted Zone
19. Amazon Route 53 cannot route queries to which AWS resource?: D. AWS OpsWorks
20. When configuring Amazon Route 53 as your DNS service for an existing domain, which is the first step that needs to be performed?: D. Transfer domain registration from current registrar to Amazon Route 53

Chapter 10
1. Which of the following objects are good candidates to store in a cache? (Choose 3 answers): A/B/C: Session State / Shoping cart / product Catalogue
2. Which of the following cache engines are supported by Amazon ElastiCache? (Choose 2 answers): B/C. Memcached / Redis
3. How many nodes can you add to an Amazon ElastiCache cluster running Memcached?: C. 20 nodes per cluster
4. How many nodes can you add to an Amazon ElastiCache cluster running Redis?: A. 1 (single node)
5. An application currently uses Memcached to cache frequently used database queries. Which steps are required to migrate the application to use Amazon ElastiCache with minimal changes? (Choose 2 answers): B/C: Update the configuration file with the endpoint for the Amazon ElastiCache cluster / Configure a security group to allow access from the application servers.
6. How can you back up data stored in Amazon ElastiCache running Redis? (Choose 2 answers): B/C. Configure automatic snapshots to back up the cache environment every night / Create a snapshot manually.
7. How can you secure an Amazon ElastiCache cluster? (Choose 3 answers): B/C/D.Restrict Application Programming Interface (API) actions using AWS Identity and Access Management (IAM) policies.  / Restrict network access using security groups / Restrict network access using a network Access Control List (ACL) 8. You are working on a mobile gaming application and are building the leaderboard feature to track the top scores across millions of users. Which AWS services are best suited for this use case?: C. Amazon ElastiCache using Redis:
9. You have built a large web application that uses Amazon ElastiCache using Memcached to store frequent query results. You plan to expand both the web fleet and the cache fleet multiple times over the next year to accommodate increased user traffic. How do you minimize the amount of changes required when a scaling event occurs?: A. Configure AutoDiscovery on the client side
10. Which cache engines does Amazon ElastiCache support? (Choose 2 answers): A/B. Memcached / Redis

Chapter 11
1. What origin servers are supported by Amazon CloudFront? (Choose 3 answers): B/C/E. An Amazon Simple Storage Service (Amazon S3) bucket / An HTTP server running on Amazon Elastic Compute Cloud (Amazon EC2) / An HTTP server running on-premises
2. Which of the following are good use cases for Amazon CloudFront? (Choose 2 answers): A/C. A popular software download site that supports users around the world, with dynamic content that changes rapidly / A heavily used video and music streaming service that requires content to be delivered only to paid subscribers
3. You have a web application that contains both static content in an Amazon Simple Storage Service (Amazon S3) bucket—primarily images and CSS files—and also dynamic content currently served by a PHP web app running on Amazon Elastic Compute Cloud (Amazon EC2). What features of Amazon CloudFront can be used to support this application with a single Amazon CloudFront distribution?: C/E: Multiple origings & multiple cache behavirous
4. You are building a media-sharing web application that serves video files to end users on both PCs and mobile devices. The media files are stored as objects in an Amazon Simple Storage Service (Amazon S3) bucket, but are to be delivered through Amazon CloudFront. What is the simplest way to ensure that only Amazon CloudFront has access to the objects in the Amazon S3 bucket?: B. Use an Amazon CloudFront Origin Access Identifier (OAI). Signed URLs, signed cookies, and IAM bucket policies can help to protect content served through Amazon
5. Your company data center is completely full, but the sales group has determined a need to store 200TB of product video. The videos were created over the last several years, with the most recent being accessed by sales the most often. The data must be accessed locally, but there is no space in the data center to install local storage devices to store this data. What AWS cloud service will meet sales’ requirements?: C. AWS Storage Gateway Gateway-Cached volumes
6. Your company wants to extend their existing Microsoft Active Directory capability into an Amazon Virtual Private Cloud (Amazon VPC) without establishing a trust relationship with the existing on-premises Active Directory. Which of the following is the best approach to achieve this goal?: B. Create and connect an AWS Directory Service Simple AD
8. Which of the following are AWS Key Management Service (AWS KMS) keys that will never exit AWS unencrypted?: C. AWS KMS CMKs are the fundamental resources that AWS KMS manages. CMKs can never leave AWS KMS unencrypted, but data keys can.
9. Which cryptographic method is used by AWS Key Management Service (AWS KMS) to encrypt data?: D. AWS KMS uses envelope encryption to protect data. AWS KMS creates a data key, encrypts it under a Customer Master Key (CMK), and returns plaintext and encrypted versions of the data key to you. You use the plaintext key to encrypt data and store the encrypted key alongside the encrypted data. You can retrieve a plaintext data key only if you have the encrypted data key and you have permission to use the corresponding master key.
10. Which AWS service records Application Program Interface (API) calls made on your account and delivers log files to your Amazon Simple Storage Service (Amazon S3) bucket?: A. AWS CloudTrail
11. You are trying to decrypt ciphertext with AWS KMS and the decryption operation is failing. Which of the following are possible causes? (Choose 2 answers): B/C. The plaintext was encrypted along with an encryption context, and you are not providing the identical encryption context when calling the Decrypt API / The ciphertext you are trying to decrypt is not valid.
12. Your company has 30 years of financial records that take up 15TB of on-premises
storage. It is regulated that you maintain these records, but in the year you have worked for the company no one has ever requested any of this data. Given that the company data center is already filling the bandwidth of its Internet connection, what is an alternative way to store the data on the most appropriate cloud storage?: B. AWS Import/Export to Amazon Glacier
13. Your company collects information from the point of sale registers at all of its franchise
locations. Each month these processes collect 200TB of information stored in Amazon Simple Storage Service (Amazon S3). Analytics jobs taking 24 hours are performed to gather knowledge from this data. Which of the following will allow you to perform these analytics in a cost-effective way?: C. Run a transient Amazon EMR cluster, and run the MapReduce jobs against the data directly in Amazon S3
14. Which service allows you to process nearly limitless streams of data in flight?: D. Kinesis
15. What combination of services enable you to copy daily 50TB of data to Amazon storage, process the data in Hadoop, and store the results in a large data warehouse?: C. Amazon Simple Storage Service (Amazon S3), Amazon Data Pipeline, Amazon EMR, and Amazon Redshift
16. Your company has 50,000 weather stations around the country that send updates every 2 seconds. What service will enable you to ingest this stream of data and store it to Amazon Simple Storage Service (Amazon S3) for future processing?: B. Amazon Kinesis Firehose
17. Your organization uses Chef heavily for its deployment automation. What AWS cloud service provides integration with Chef recipes to start new application server instances, configure application server software, and deploy applications?: C. AWS OpsWorks
18. A firm is moving its testing platform to AWS to provide developers with instant access to clean test and development environments. The primary requirement for the firm is to make environments easily reproducible and fungible. What service will help the firm meet their requirements?: A. AWS CloudFormation
19. Your company’s IT management team is looking for an online tool to provide recommendations to save money, improve system availability and performance, and to help close security gaps. What can help the management team?: B. AWS Trusted Advisor
20. Your company works with data that requires frequent audits of your AWS environment to ensure compliance with internal policies and best practices. In order to perform these audits, you need access to historical configurations of your resources to evaluate relevant= configuration changes. Which service will provide the necessary information for your audits?: A. AWS Config
21. All of the website deployments are currently done by your company’s development team. With a surge in website popularity, the company is looking for ways to be more agile with deployments. What AWS cloud service can help the developers focus more on writing code instead of spending time managing and configuring servers, databases, load balancers, firewalls, and networks?: D. AWS Elastic Beanstalk is the fastest and simplest way to get an application up and running on AWS

Chapter 12
1. Which is an operational process performed by AWS for data security?: B. Decommissioning of storage devices using industry-standard practices
2. You have launched a Windows Amazon Elastic Compute Cloud (Amazon EC2) instance and specified an Amazon EC2 key pair for the instance at launch. Which of the following accurately describes how to log in to the instance?: B. Use the Amazon EC2 key pair to decrypt the administrator password and then securely connect to the instance via Remote Desktop Protocol (RDP) as the administrator.
3. A Database security group controls network access to a database instance that is inside a Virtual Private Cloud (VPC) and by default allows access from?: C. No access is provided by default, and any access must be explicitly added with a rule to the DB security group.
4. Which encryption algorithm is used by Amazon Simple Storage Service (Amazon S3) to encrypt data at rest with Service-Side Encryption (SSE)?: A. Advanced Encryption Standard (AES)-256
5. How many access keys may an AWS Identity and Access Management (IAM) user have active at one time?: C. 2
6. Which of the following is the name of the security model employed by AWS with its customers?: B. The shared responsibility model
7. Which of the following describes the scheme used by an Amazon Redshift cluster leveraging AWS Key Management Service (AWS KMS) to encrypt data-at-rest?: D. Amazon Redshift uses a four-tier, key-based architecture for encryption.
8. Which of the following Elastic Load Balancing options ensure that the load balancer determines which cipher is used for a Secure Sockets Layer (SSL) connection?: D. Server Order Preference
9. Which technology does Amazon WorkSpaces use to provide data security?: C. Amazon WorkSpaces uses PCoIP , which provides an interactive video stream without transmitting actual data
10. As a Solutions Architect, how should you architect systems on AWS?: C. You should architect your AWS usage to take advantage of multiple regions and Availability Zones (Fail tolerant)
11. Which security scheme is used by the AWS Multi-Factor Authentication (AWS MFA) token?: A. A virtual MFA device uses a software application that generates six-digit authentication codes that are compatible with the TOTP standard, as described in RFC 6238
12. DynamoDB tables may contain sensitive data that needs to be protected. Which of the following is a way for you to protect DynamoDB table content? (Choose 2 answers): B/D: DynamoDB can store data encrypted with a client-side encryption library solution before storing the data in DynamoDB / DynamoDB can be used with the AWS Key Management Service to encrypt the data before storing the data in DynamoDB
13. You have launched an Amazon Linux Elastic Compute Cloud (Amazon EC2) instance into EC2-Classic, and the instance has successfully passed the System Status Check and Instance Status Check. You attempt to securely connect to the instance via Secure Shell (SSH) and receive the response, “WARNING: UNPROTECTED PRIVATE KEY FILE,” after which the login fails. Which of the following is the cause of the failed login?: B. The permissions for the private key are too insecure for the key to be trusted.
14. Which of the following public identity providers are supported by Amazon Cognito Identity?: D. Amazon Cognito Identity supports public identity providers—Amazon, Facebook, and Google—as well as unauthenticated identities.
15. Which feature of AWS is designed to permit calls to the platform from an Amazon Elastic Compute Cloud (Amazon EC2) instance without needing access keys placed on the instance?: A. AWS Identity and Access Management (IAM) instance profile
16. Which of the following Amazon Virtual Private Cloud (Amazon VPC) elements acts as a stateless firewall?: B. Network Access Control List (ACL)
17. Which of the following is the most recent version of the AWS digital signature calculation process?: D. Signature Version 4
18. Which of the following is the name of the feature within Amazon Virtual Private Cloud (Amazon VPC) that allows you to launch Amazon Elastic Compute Cloud (Amazon EC2) instances on hardware dedicated to a single customer?: B. Dedicated instances
19. Which of the following describes how Amazon Elastic MapReduce (Amazon EMR) protects access to the cluster?: C. The master node is launched into a security group that allows Secure Shell (SSH) and service access, while the slave nodes are launched into a separate security group that only permits communication with the master node.
20. To help prevent data loss due to the failure of any single hardware component, Amazon Elastic Block Storage (Amazon EBS) automatically replicates EBS volume data to which of the following?: A. Amazon EBS replicates EBS volume data within the same Availability Zone in a region.

Chapter 14
architecture best practices
  think parallel
  loosely coupled
  auditing, logging
  offload security responsibility to aws
  shared responsibility
  single responsible code
  microservices architecture
  design for failure
  implement elasticity
  leverage different storage options
  layered design
  dont fear constraints
1. When designing a loosely coupled system, which AWS services provide an intermediate durable storage layer between components? (Choose 2 answers): B/E: Amazon Kinesis / SQS
2. Which of the following options will help increase the availability of a web server farm? (Choose 2 answers): B/C. Launch the web server instances across multiple Availability Zones / Auto scailing
3. Which of the following AWS Cloud services are designed according to the Multi-AZ principle? (Choose 2 answers): A/E. DynamoDB / S3
4. Your e-commerce site was designed to be stateless and currently runs on a fleet of Amazon Elastic Compute Cloud (Amazon EC2) instances. In an effort to control cost and increase availability, you have a requirement to scale the fleet based on CPU and network utilization to match the demand curve for your site. What services do you need to meet this requirement? (Choose 2 answers): A/D. Amazon CloudWatch / Auto scailing
5. Your compliance department has mandated a new requirement that all data on AmazonElastic Block Storage (Amazon EBS) volumes must be encrypted. Which of the following steps would you follow for your existing Amazon EBS volumes to comply with the new requirement? (Choose 3 answers): B/D/E. Create a new Amazon EBS volume with encryption enabled / Attach an Amazon EBS volume with encryption enabled to the instance that hosts the data, then migrate the data to the encryption-enabled Amazon EBS volume / Copy the data from the unencrypted Amazon EBS volume to the Amazon EBS volume with encryption enabled
6. When building a Distributed Denial of Service (DDoS)-resilient architecture, how does Amazon Virtual Private Cloud (Amazon VPC) help minimize the attack surface area? (Choose 3 answers): A/C/D. Reduces the number of necessary Internet entry points / Obfuscates necessary Internet entry points to the level that untrusted end users cannot access them / Adds non-critical Internet entry points to the architecture
7. Your e-commerce application provides daily and ad hoc reporting to various business units on customer purchases. This is resulting in an extremely high level of read traffic to your MySQL Amazon Relational Database Service (Amazon RDS) instance. What can you do to scale up read traffic without impacting your database’s performance?: C. Amazon RDS read replicas are the ans
8. Your website is hosted on a fleet of web servers that are load balanced across multiple Availability Zones using an Elastic Load Balancer (ELB). What type of record set in Amazon Route 53 can be used to point myawesomeapp.com to your website?: A. An alias resource record set
9. You need a secure way to distribute your AWS credentials to an application running on Amazon Elastic Compute Cloud (Amazon EC2) instances in order to accesssupplementary AWS Cloud services. What approach provides your application access to use short-term credentials for signing requests while protecting those credentials from other users?: D. Provision the Amazon EC2 instances with an instance profile that has the appropriate privileges.
10. You are running a suite of microservices on AWS Lambda that provide the business logic and access to data stored in Amazon DynamoDB for your task management system. You need to create well-defined RESTful Application Program Interfaces (APIs) for these microservices that will scale with traffic to support a new mobile application. What AWS Cloud service can you use to create the necessary RESTful APIs?: B. Amazon API Gateway is fully managed
11. Your WordPress website is hosted on a fleet of Amazon Elastic Compute Cloud (Amazon EC2) instances that leverage Auto Scaling to provide high availability. To ensure that the content of the WordPress site is sustained through scale up and scale down events, you need a common file system that is shared between more than one Amazon EC2 instance. Which AWS Cloud service can meet this requirement?: C. Amazon EFS is a file storage service
12. You are changing your application to move session state information off the individual Amazon Elastic Compute Cloud (Amazon EC2) instances to take advantage of the elasticity and cost benefits provided by Auto Scaling. Which of the following AWS Cloud services is best suited as an alternative for storing session state information?: A. Dynamo DB
13. A media sharing application is producing a very high volume of data in a very short period of time. Your back-end services are unable to manage the large volume of transactions. What option provides a way to manage the flow of transactions to yourback-end services?: B. SQS
14. Which of the following are best practices for managing AWS Identity and Access Management (IAM) user access keys? (Choose 3 answers): B/C/E. Use different access keys for different applications / Rotate access keys periodically / Configure Multi-Factor Authentication (MFA) for your most sensitive operations
15. You need to implement a service to scan Application Program Interface (API) calls and related events’ history to your AWS account. This service will detect things like unused permissions, overuse of privileged accounts, and anomalous logins. Which of the following AWS Cloud services can be leveraged to implement this service? (Choose 3 answers): A/B/E: CloudTrail, S3, lambda
16. Government regulations require that your company maintain all correspondence for a period of seven years for compliance reasons. What is the best storage mechanism to keep this data secure in a cost-effective manner?: B. Glacier
17. Your company provides media content via the Internet to customers through a paid subscription model. You leverage Amazon CloudFront to distribute content to your customers with low latency. What approach can you use to serve this private content securely to your paid subscribers?: A. signed urls
18. Your company provides transcoding services for amateur producers to format their short films to a variety of video formats. Which service provides the best option for storing the videos?: B. S3
19. A week before Cyber Monday last year, your corporate data center experienced a failed air conditioning unit that caused flooding into the server racks. The resulting outage cost your company significant revenue. Your CIO mandated a move to the cloud, but he is still concerned about catastrophic failures in a data center. What can you do to alleviate his concerns?: A. Distributed data architecture to multiple AZs
20. Your Amazon Virtual Private Cloud (Amazon VPC) includes multiple private subnets. The instances in these private subnets must access third-party payment Application Program Interfaces (APIs) over the Internet. Which option will provide highly available Internet access to the instances in the private subnets?: Create a Network Address Translation (NAT) gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone




</pre>